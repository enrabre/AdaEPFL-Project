{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import math\n",
    "from collections import Counter\n",
    "import csv\n",
    "import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm_pandas\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def now():\n",
    "    return str(datetime.now().time())[:8]\n",
    "def pr(strToPrint):\n",
    "    print(now() + ' '+ strToPrint)\n",
    "\n",
    "from IPython.display import Audio\n",
    "sound_file = './sound/beep2.wav'\n",
    "# Used by other groups: pd.read_csv(data_path, sep=\"\\t\",encoding='utf-8',  escapechar='\\\\', quoting=csv.QUOTE_NONE, header=None, na_values='N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_header = ['id', 'userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId',\n",
    "                  'inReplyTo', 'source', 'truncated', 'placeLatitude', 'placeLongitude', 'sourceName', 'sourceUrl',\n",
    "                 'userName', 'screenName', 'followersCount', 'friendsCount', 'statusesCount',\n",
    "                 'userLocation']\n",
    "\n",
    "# filename = os.path.join('data','sample.tsv')\n",
    "filename = os.path.join('data','twex.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr('Starting to read file...')\n",
    "df = pd.read_csv(filename, sep='\\t', encoding='utf-8', escapechar='\\\\', names=columns_header,\n",
    "                      quoting=csv.QUOTE_NONE, na_values='N', header=None)\n",
    "\n",
    "pr('File is loaded!')\n",
    "Audio(url=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tw = df.get_chunk()\n",
    "# tw = df.head(1000000)\n",
    "# tw = df2.copy()\n",
    "tw = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a sample of the data. You can make your own sample data with the to_pickle command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_filename = os.path.join('data','head_100k_pickle.pkl')\n",
    "pickle_filename2 = os.path.join('data','sample_100k_pickle.pkl')\n",
    "pickle_filename3 = os.path.join('data','head_1M_pickle.pkl')\n",
    "pickle_filename4 = os.path.join('data','sample_1M_pickle.pkl')\n",
    "# tw.to_pickle(pickle_filename3)\n",
    "tw = pd.read_pickle(pickle_filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of \"numbers\" in the id section of each row to see if our data is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_number_type(s):\n",
    "    try:\n",
    "        float(s)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_nb = 0\n",
    "index_list = []\n",
    "def count_nn_nb(row):\n",
    "#     print(row)\n",
    "    if not is_number_type(row.id):\n",
    "        global non_nb\n",
    "        non_nb += 1\n",
    "        global index_list\n",
    "        index_list.append(row.name)\n",
    "pr('Starting count...')    \n",
    "tw.apply(count_nn_nb, axis=1)\n",
    "pr('non nb: {} / {}'.format(non_nb, len(tw)))\n",
    "index_list[:10]\n",
    "# is_number(tw.iloc[60].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw.loc[index_list]\n",
    "# tw.loc[index_list[0]-1:index_list[0]+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Problem of  <i>\\n</i>   in some of the texts</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the texts have the char \\n and it makes a new line. This should fix the problem. However, it does not work for the whole dataset and it appears finally that this happens very rarely! So we decided to drop this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_nan(field):\n",
    "    try:\n",
    "        isNan = np.isnan(field)\n",
    "    except TypeError:\n",
    "        return False\n",
    "    else:\n",
    "        return isNan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def row_is_bad(row, nextRow):\n",
    "    for columnName in columns_header[4:len(columns_header)]:\n",
    "        if not is_nan(row[columnName]):\n",
    "            return False\n",
    "    for columnName in columns_header[17:len(columns_header)]:\n",
    "        if not is_nan(nextRow[columnName]):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_bad_rows(row, nextRow):\n",
    "    new_row = {}\n",
    "    for columnName in columns_header[0:3]:\n",
    "        new_row[columnName] = row[columnName]\n",
    "    new_row['text'] = ''.join([row.text, nextRow.id])\n",
    "    for i, columnName in enumerate(columns_header[4:len(columns_header)]):\n",
    "        new_row[columnName] = nextRow[columns_header[i+1]]\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows_to_drop_list = []\n",
    "rows_to_add = []\n",
    "def merge_bad_lines(row, df):\n",
    "    # Get next row\n",
    "    index = df.index.get_loc(row.name)\n",
    "    if index < len(df) - 1:\n",
    "        next_row = df.iloc[index + 1]\n",
    "        if row_is_bad(row, next_row):\n",
    "            global rows_to_drop_list, rows_to_add            \n",
    "            rows_to_add.append(merge_bad_rows(row, next_row))\n",
    "            rows_to_drop_list.append(row.name)\n",
    "            rows_to_drop_list.append(next_row.name)\n",
    "            ## add next row to drop list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def changeBadRows(tw):\n",
    "    global rows_to_drop_list, rows_to_add            \n",
    "    rows_to_drop_list = []\n",
    "    rows_to_add = []\n",
    "    pr('Starting finding bad rows...')\n",
    "    tw.apply(merge_bad_lines, args=(tw,), axis=1)\n",
    "    pr('Finished finding bad rows!')\n",
    "    print('Bad rows: ', rows_to_drop_list)\n",
    "    print('Number of rows to add: ', len(rows_to_add))\n",
    "\n",
    "    pr('Starting to drop bad rows...')\n",
    "    tw_drop_rows = tw.drop(rows_to_drop_list)\n",
    "\n",
    "    pr('Making new df...')\n",
    "    addedRowDf = pd.DataFrame(rows_to_add, index=rows_to_drop_list[0:len(rows_to_drop_list):2])\n",
    "\n",
    "    res = pd.concat([addedRowDf, tw_drop_rows])\n",
    "    resGoodOrder = res.reindex_axis(tw.columns, axis=1)\n",
    "    return resGoodOrder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm to change some bad rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tw2 = changeBadRows(tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veryfing the changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# newTw.loc[index_list[0]-2:index_list[0]+2]\n",
    "# tw2.loc[index_list[2]-2:index_list[2]+2]\n",
    "# tw2.loc[index_list[2]-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data cleaning - observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of rows that have location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat = 0\n",
    "long = 0\n",
    "def count_location_data(row):\n",
    "    if not is_nan(row.latitude):\n",
    "        global lat\n",
    "        lat += 1 \n",
    "    if not is_nan(row.longitude):\n",
    "        global long\n",
    "        long += 1\n",
    "\n",
    "def execute_count_location_data(tw):\n",
    "    global lat, long\n",
    "    lat = 0\n",
    "    long = 0\n",
    "    pr('Starting computing longitude and latitude data.')\n",
    "    tw.apply(count_location_data, axis=1)\n",
    "    pr('Data with longitude: {:.2f}% / with latitude: {:.2f}%'.format(long/len(tw)*100, lat/len(tw)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "execute_count_location_data(tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the data that has a null text, and other parameters when the text is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txtNull = tw[pd.isnull(tw['text'])]\n",
    "txtNull.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txtNull.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('There are {} / {} data with a NaN text'.format(len(txtNull), len(tw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txtAndCreateNull = txtNull.loc[pd.isnull(txtNull['createdAt']) | pd.isnull(txtNull['text'])]\n",
    "print('There are {} / {} data with a NaN text OR a NaN createdAt field.'.format(len(txtAndCreateNull), len(tw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txtNull.loc[pd.notnull(txtNull['userId'])].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that 11.641 out of a sample of 100.000 have a lot of nan values. It looks like that for 10.056 of them, all the values are nan except the id value where there is text. This could be linked to bad reading. Should be explored more. However, if we only have the text, it is not that usefull in case of event detection, so we should remove them.\n",
    "Out of all of these, we have 679 with all the columns filled except the text. These should be removed too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the data in time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop every tweet that does not have a text and a createdAt date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw3 = tw.dropna(axis=0, how='any', subset=['text', 'createdAt'])\n",
    "print('The data have been reduced from {} tweets to {} tweets.'.format(len(tw), len(tw3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Having good date format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly in a date for a specific data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"Lets remove the following anomaly in created date: \")\n",
    "# print(df.loc[[8853944], 'createdAt'])\n",
    "# df.drop(8853944, inplace=True)\n",
    "# print(df.loc[[8853944], 'createdAt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dates usually have the lenght of 19 chars. We have a few exceptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr('Exceptions:')\n",
    "dateExceptions = tw3[tw3['createdAt'].str.len() != 19]\n",
    "pr('We have {} exception in our data. They look like that: '.format(len(dateExceptions)))\n",
    "dateExceptions.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove these exceptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr('Removing bad dates...')\n",
    "tw4 = tw3[tw3['createdAt'].str.len() == 19]\n",
    "pr('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find if other dates are not fit to be converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr('Starting to examine dates...')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "datetime_serie = tw4['createdAt'].convert_objects(convert_dates='coerce')\n",
    "dateNotConvertible = datetime_serie[pd.isnull(datetime_serie)]\n",
    "warnings.filterwarnings('default')\n",
    "pr('There are {} dates that cannot be transformed.'.format(len(dateNotConvertible)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to convert the date to datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr('Starting copy...') # (to avoid transformation problems)\n",
    "tw5 = tw4.copy()\n",
    "pr('Converting to datetime...')\n",
    "tw5['createdAt'] = pd.to_datetime(tw4['createdAt'])\n",
    "pr('Setting up new indices...')\n",
    "tw5.index = tw5['createdAt']\n",
    "pr('Deleting old \"createdAt\" column...')\n",
    "del tw5['createdAt']\n",
    "pr('Done!')\n",
    "tw5.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's take a look at the time series possibilities to analyze a bit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr(\"Let's copy our data in case we change the previous part...\")\n",
    "ts1 = tw5.copy()\n",
    "pr(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ts1['id'].resample('A').count().plot(marker='o', color='r')\n",
    "fig.suptitle('Tweets by year', fontsize=18)\n",
    "plt.xlabel('Year of creation', fontsize=14)\n",
    "plt.ylabel('Number of tweets', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ts1['id'].resample('M').count().plot(marker='.')\n",
    "fig.suptitle('Tweets by month', fontsize=18)\n",
    "plt.xlabel('Year of creation', fontsize=14)\n",
    "plt.ylabel('Number of tweets (monthly)', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets by month - comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years = [2011, 2012, 2013, 2014, 2015, 2016] \n",
    "twYearly = [ts1[str(year)] for year in years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i, y in enumerate(twYearly):\n",
    "    shift = (len(twYearly) - 1 - i) * 12\n",
    "    shitStr = str(shift) + 'M'\n",
    "    y['id'].resample('M', loffset=shitStr).count().plot(label=str(years[i]), marker='.')\n",
    "\n",
    "fig.suptitle('Tweets by month - comparison', fontsize=18)\n",
    "plt.xlabel('Creation month', fontsize=14)\n",
    "plt.ylabel('Number of tweets', fontsize=14)\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most tweeted days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The days where people tweeted the most:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mostTweetedDays = ts1.resample('D').count().sort_values('text', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts1['2012-10-30'].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtag timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect events, we will make a timeseries with each hashtags. We will try to detect anomalies in these time series. We will make one dictionary just with the timeseries, and another with the associated tweets."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[{\n",
    "    id: hashtag_name\n",
    "    tweetsTimeSeries: dataframe time serie\n",
    "}]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hashtag Time Series dictionary:\n",
    "hts = { hashtag : [timeseries] }\n",
    "\n",
    "Hashtag tweets dictionary:\n",
    "tw_h = { hashtag : [tweetList] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateDictList(dictionary, key, val):\n",
    "    '''\n",
    "    Updates a dictionary containing a list\n",
    "    '''\n",
    "    if key in dictionary:\n",
    "        dictionary[key].append(val)\n",
    "    else:\n",
    "        dictionary[key] = [val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hts = {}\n",
    "tw_h = {}\n",
    "hashtag_count = 0\n",
    "\n",
    "def makeHashtagDict(row, hts, tw_h):\n",
    "    hashStart = False\n",
    "    hashtagCharList = []\n",
    "    text = row.text if row.text else \"\"\n",
    "    for letter in text:\n",
    "        if hashStart:\n",
    "            #End of hashtag?\n",
    "            if not letter.isalnum():\n",
    "                if len(hashtagCharList) > 0:\n",
    "                    hashtag = (''.join(hashtagCharList)).lower()\n",
    "                    updateDictList(hts, hashtag, row.name)\n",
    "                    updateDictList(tw_h, hashtag, row)\n",
    "                hashStart = False\n",
    "                hashtagCharList = []\n",
    "            else:\n",
    "                hashtagCharList.append(letter)\n",
    "        # Start of hashtag\n",
    "        if letter == '#':\n",
    "            hashStart = True\n",
    "            global hashtag_count\n",
    "            hashtag_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def executehashtagDictCreation(df):\n",
    "    global hts, tw_h, hashtag_count\n",
    "    hts = {}\n",
    "    tw_h = {}\n",
    "    hashtag_count = 0\n",
    "    \n",
    "    pr('Starting to create hashtag dictionaries...')\n",
    "    df.apply(makeHashtagDict, args=(hts, tw_h,), axis=1)\n",
    "    pr('Done!')\n",
    "    print('In total, {} hashtags were detected. There are {} different ones.'.format(hashtag_count,len(hts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "executehashtagDictCreation(ts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What needs to be done!\n",
    "The dict contains only table => it needs to contain a time serie and not a table\n",
    "The command for 1 sample could be... :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hashtag_name = 'xxxxxxxxxxxx'\n",
    "lts = pd.Series([1 for i in hts[hashtag_name]], index=hts[hashtag_name])\n",
    "Which would allow to run => lts.resample('20Min').count().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> It needs to be done for the whole dict!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>HOWEVER</B> : Careful with conflict of dates!! (because it will put a \"1\" inthere and I don't know how it is dealt with) Which will happen and should be dealt with!\n",
    "\n",
    "<b>Also</b>: I don't understand why there are so many conflicts of dates for some hashtags.\n",
    "Is it the data that is bad, the data that is badly imported or badly dealt with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of hashtags\n",
    "Let's create a dictionary that contains the counts of hashtags. And a table of it that is sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ht_count = {}\n",
    "for key, value in hts.items():\n",
    "    ht_count[key] = len(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ht_count_sorted = sorted(ht_count, key=ht_count.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "top = 15\n",
    "print('Top {} hashtags: '.format(top))\n",
    "while(i < top):\n",
    "    ht = ht_count_sorted[i]\n",
    "    i += 1\n",
    "    print('{}. {} : {}'.format(i, ht, ht_count[ht]))\n",
    "    [print('     ',date) for date in hts[ht][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "nb_to_see = 10\n",
    "for key, val in hts.items():\n",
    "    if i < nb_to_see:\n",
    "        i += 1\n",
    "        print('{} : {}'.format(key, ht_count[key]))\n",
    "        [print('    ',dd) for dd in val[:5]]\n",
    "        print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ht_name = 'christmasrocks'\n",
    "lts = pd.Series([1 for i in hts[ht_name]], index=hts[ht_name])\n",
    "lts.resample('20Min').count().plot()\n",
    "# lts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ts1['2016-07-18']['id'].resample('10min').count().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ADA]",
   "language": "python",
   "name": "Python [ADA]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "018a951e1c1f419584d039221d8070d6": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "16fafb8d18c54570996a516fb52cdca2": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "209c2a3aa71a4eaf893cbc08ce5d1c43": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "3ec64bbb68f24135b60d21ec52b32688": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "56b26c8308b142d8939540c16fa7a2bb": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "845902c34f5d4c049a983ad9467092f5": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "a690e7ee5ead4f5899822307bdfc0708": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "bf8804a919ac4fad8a6fa38b98996be4": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "c17f90881394457084fee4a356651006": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "e3f4234260bc474ebf96320919a965fd": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "fa50e17d00744cf2aefca32b6b1b5cfa": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
