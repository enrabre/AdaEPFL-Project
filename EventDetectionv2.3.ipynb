{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event detection in Twitter dataset\n",
    "The goal of this project is to detect past events in Switzerland, using a dataset of tweets. This dataset contain 28 million tweets coming for most of them from Switzerland. This notebook will get you through our project, explaining our methodology. It is splitted in 5 parts:\n",
    "\n",
    "\n",
    "1. **Extracting hashtags and data cleaning**<br>\n",
    "We split the tweets\n",
    "2. **Data structure manipulation**<br>\n",
    "\n",
    "3. **Improving quality of detected event**<br>\n",
    "\n",
    "4. **Exporting data**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our code more readable, we exported some functions to the file _\"helpFunctions.py\"_.<br>\n",
    "Each time we will use one of these functions, we will explicitely write: **# EXTERNAL FUNCTION: name_of_the_function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import math\n",
    "from collections import Counter\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "import copy\n",
    "import json\n",
    "from datetime import timedelta\n",
    "from operator import itemgetter\n",
    "from helpFunctions import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of check point (a print plus the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pr(strToPrint):\n",
    "    '''\n",
    "    Print the current date and time, concatenated to the string passed in argument.   \n",
    "    :param strToPrint: Regular string to print\n",
    "    '''\n",
    "    print( str(datetime.now().time())[:8] + ' '+ strToPrint)\n",
    "    \n",
    "def strNb(nb):\n",
    "    '''\n",
    "    Transform a high number in a string, with '.' for each thousand\n",
    "    :param nb: A high number to print\n",
    "    '''\n",
    "    return '{0:,}'.format(nb).replace(',', '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a sample of the dataset (useful for testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_filename = os.path.join('data','head_100k_pickle.pkl')\n",
    "tw = pd.read_pickle(pickle_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing columns headers and file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_header = ['id', 'userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId',\n",
    "                  'inReplyTo', 'source', 'truncated', 'placeLatitude', 'placeLongitude', 'sourceName', 'sourceUrl',\n",
    "                 'userName', 'screenName', 'followersCount', 'friendsCount', 'statusesCount',\n",
    "                 'userLocation']\n",
    "filename = os.path.join('data', 'twex.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:05:32 Starting to read file... (3 min)\n",
      "20:05:32 File is loaded.\n"
     ]
    }
   ],
   "source": [
    "pr('Starting to read file... (3 min)')\n",
    "# tw = pd.read_csv(filename, sep='\\t', encoding='utf-8', escapechar='\\\\', names=columns_header,\n",
    "#                       quoting=csv.QUOTE_NONE, na_values='N', header=None)\n",
    "pr('File is loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 100.000 tweets.\n"
     ]
    }
   ],
   "source": [
    "print('The dataset contains {} tweets.'.format(strNb(len(tw))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>placeLatitude</th>\n",
       "      <th>placeLongitude</th>\n",
       "      <th>sourceName</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>userName</th>\n",
       "      <th>screenName</th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>userLocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9514097914</td>\n",
       "      <td>1.7341e+07</td>\n",
       "      <td>2010-02-23 05:55:51</td>\n",
       "      <td>Guuuuten Morgen! :-)</td>\n",
       "      <td>7.43926</td>\n",
       "      <td>46.9489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TwitBird</td>\n",
       "      <td>http://www.nibirutech.com</td>\n",
       "      <td>Tilman Jentzsch</td>\n",
       "      <td>blickwechsel</td>\n",
       "      <td>586</td>\n",
       "      <td>508.0</td>\n",
       "      <td>9016.0</td>\n",
       "      <td>Bern, Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9514846412</td>\n",
       "      <td>7.19828e+06</td>\n",
       "      <td>2010-02-23 06:22:40</td>\n",
       "      <td>Still the best coffee in town — at La Stanza h...</td>\n",
       "      <td>8.53781</td>\n",
       "      <td>47.3678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gowalla</td>\n",
       "      <td>http://gowalla.com/</td>\n",
       "      <td>Nico Luchsinger</td>\n",
       "      <td>halbluchs</td>\n",
       "      <td>1820</td>\n",
       "      <td>703.0</td>\n",
       "      <td>4687.0</td>\n",
       "      <td>Zurich, Switzerland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       userId            createdAt  \\\n",
       "0  9514097914   1.7341e+07  2010-02-23 05:55:51   \n",
       "1  9514846412  7.19828e+06  2010-02-23 06:22:40   \n",
       "\n",
       "                                                text  longitude latitude  \\\n",
       "0                               Guuuuten Morgen! :-)    7.43926  46.9489   \n",
       "1  Still the best coffee in town — at La Stanza h...    8.53781  47.3678   \n",
       "\n",
       "  placeId  inReplyTo source truncated placeLatitude placeLongitude sourceName  \\\n",
       "0     NaN        NaN    197       NaN           NaN            NaN   TwitBird   \n",
       "1     NaN        NaN    550       NaN           NaN            NaN    Gowalla   \n",
       "\n",
       "                   sourceUrl         userName    screenName followersCount  \\\n",
       "0  http://www.nibirutech.com  Tilman Jentzsch  blickwechsel            586   \n",
       "1        http://gowalla.com/  Nico Luchsinger     halbluchs           1820   \n",
       "\n",
       "   friendsCount  statusesCount         userLocation  \n",
       "0         508.0         9016.0    Bern, Switzerland  \n",
       "1         703.0         4687.0  Zurich, Switzerland  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('First rows of dataset:')\n",
    "tw.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Extracting hashtags and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Extracting hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect event, we have decided to only concentrate on hashtags. Therefore, we are going extract hashtags and only keep tweets that contain hashtags (as the other ones will contains no useful information for us). We start with this operation before the data cleaning because it is the one that reduces our data size the most, and therefore all further operations will execute quicker.<br>\n",
    "To do so, we are going to examine the _text_ field of each tweet, extract its hashtags and put them in a new column (in the form of a list of hashtags per tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:05:32 Extracting hashtags... (2 min)\n",
      "20:05:33 We have extracted 19.719 rows with hashtags out of the 100.000 rows of our initial dataframe.\n"
     ]
    }
   ],
   "source": [
    "# EXTERNAL FUNCTION: extract_hashtags(text)\n",
    "\n",
    "pr('Extracting hashtags... (2 min)')\n",
    "\n",
    "tw['hashtag'] = tw.text.apply(lambda x: extract_hashtags(str(x))) # Getting list of hashtag into new column\n",
    "twh = tw.ix[tw.hashtag.apply(lambda x: len(x) != 0)] # droping the rows (tweets) that contains no hashtags.\n",
    "\n",
    "pr('We have extracted {} rows with hashtags out of the {} rows of our initial dataframe.'.format(strNb(len(twh)),strNb(len(tw))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of tweets (with only text and hashtag column):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Magic spells run off after midnight, I guess s...</td>\n",
       "      <td>[fb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Limitas of public transportation! No taxi, rai...</td>\n",
       "      <td>[yam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>So, Feierabend. Jetzt #24 und später #VfB. — a...</td>\n",
       "      <td>[24, vfb]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text    hashtag\n",
       "8   Magic spells run off after midnight, I guess s...       [fb]\n",
       "10  Limitas of public transportation! No taxi, rai...      [yam]\n",
       "15  So, Feierabend. Jetzt #24 und später #VfB. — a...  [24, vfb]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Examples of tweets (with only text and hashtag column):')\n",
    "twh[['text', 'hashtag']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now clean the remaining dataset. The creation date is important for our event detection, and in order to provide visualization to the other team, we needed to give location information to all tweets. Therefore we will drop the rows which contain NA values at _creationAt_, _latitude_ or _longitude_ position.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:05:33 Droping rows with NA values (location and creation date).\n",
      "20:05:33 The data have been reduced from 19.719 tweets to 15.043 tweets.\n"
     ]
    }
   ],
   "source": [
    "pr(\"Droping rows with NA values (location and creation date).\")\n",
    "tw1 = twh.dropna(axis=0, how='any', subset=['createdAt'])\n",
    "tw1.dropna(subset=['longitude'], inplace=True)\n",
    "tw1.dropna(subset=['latitude'], inplace=True)\n",
    "pr('The data have been reduced from {} tweets to {} tweets.'.format(strNb(len(twh)), strNb(len(tw1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latitude and longitude values should be in _float_ format to analyze them correctly later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw1.latitude = tw1.latitude.apply(float)\n",
    "tw1.longitude = tw1.longitude.apply(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some date in our initial data could not be transformed to pandas datetime. Let's check if everything is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:05:33 Starting to examine dates...\n",
      "20:05:33 There are 0 dates that cannot be transformed.\n"
     ]
    }
   ],
   "source": [
    "pr('Starting to examine dates...')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "datetime_serie = tw1['createdAt'].convert_objects(convert_dates='coerce')\n",
    "dateNotConvertible = datetime_serie[pd.isnull(datetime_serie)]\n",
    "warnings.filterwarnings('default')\n",
    "pr('There are {} dates that cannot be transformed.'.format(len(dateNotConvertible)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks good! Now, we will transform our data in a time series. To do so, the best way is to put the creation date of each row as the index of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:05:33 Converting to datetime...\n",
      "20:05:33 Setting up new indices...\n",
      "20:05:33 Deleting old \"createdAt\" column...\n",
      "20:05:33 Done. The updated dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>placeLatitude</th>\n",
       "      <th>placeLongitude</th>\n",
       "      <th>sourceName</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>userName</th>\n",
       "      <th>screenName</th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>userLocation</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>createdAt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-02-23 09:59:41</th>\n",
       "      <td>9519737890</td>\n",
       "      <td>1.46579e+07</td>\n",
       "      <td>Magic spells run off after midnight, I guess s...</td>\n",
       "      <td>6.1387</td>\n",
       "      <td>46.175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>http://twitter.com/#!/download/iphone</td>\n",
       "      <td>Javier Belmonte</td>\n",
       "      <td>vichango</td>\n",
       "      <td>167</td>\n",
       "      <td>277.0</td>\n",
       "      <td>2885.0</td>\n",
       "      <td>Geneva, Switzerland</td>\n",
       "      <td>[fb]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id       userId  \\\n",
       "createdAt                                      \n",
       "2010-02-23 09:59:41  9519737890  1.46579e+07   \n",
       "\n",
       "                                                                  text  \\\n",
       "createdAt                                                                \n",
       "2010-02-23 09:59:41  Magic spells run off after midnight, I guess s...   \n",
       "\n",
       "                     longitude  latitude placeId  inReplyTo source truncated  \\\n",
       "createdAt                                                                      \n",
       "2010-02-23 09:59:41     6.1387    46.175     NaN        NaN      1       NaN   \n",
       "\n",
       "                    placeLatitude placeLongitude          sourceName  \\\n",
       "createdAt                                                              \n",
       "2010-02-23 09:59:41           NaN            NaN  Twitter for iPhone   \n",
       "\n",
       "                                                 sourceUrl         userName  \\\n",
       "createdAt                                                                     \n",
       "2010-02-23 09:59:41  http://twitter.com/#!/download/iphone  Javier Belmonte   \n",
       "\n",
       "                    screenName followersCount  friendsCount  statusesCount  \\\n",
       "createdAt                                                                    \n",
       "2010-02-23 09:59:41   vichango            167         277.0         2885.0   \n",
       "\n",
       "                            userLocation hashtag  \n",
       "createdAt                                         \n",
       "2010-02-23 09:59:41  Geneva, Switzerland    [fb]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr('Converting to datetime...')\n",
    "tw5 = tw1.copy()\n",
    "tw5['createdAt'] = pd.to_datetime(tw1['createdAt'])\n",
    "pr('Setting up new indices...')\n",
    "tw5.index = tw5['createdAt']\n",
    "pr('Deleting old \"createdAt\" column...')\n",
    "del tw5['createdAt']\n",
    "pr('Done. The updated dataframe:')\n",
    "tw5.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data structure manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have the data in the right format to process them. Therefore we will apply several operations to the dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - One hashtag = one row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Some people put more than one hashtag per tweet. A tweet that contain more than one hashtag will be analyzed the same way as if it contained only one hashtag. So, to process the data in a easier way, we will make a dataframe with **one row per hashtag**. Therefore, we will duplicate the row that contains more than one hashtag.\n",
    "\n",
    "This will be done by going through the dataframe, and making in parallel a list of rows (with 1 hashtag per row) that needs to be added to the old dataframe. These rows will be stored in a dictionary and will be appended afterwards to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:05:33 Multiplying the hashtag rows... (around 10 min)\n",
      "20:05:35 Finished! 6812 rows will be added to the dataframe!\n"
     ]
    }
   ],
   "source": [
    "# EXTERNAL FUNCTIONS: reset_added_hashtag_rows_list(), multiplyHashtagRows(row), get_added_hashtag_rows_list()\n",
    "\n",
    "pr('Multiplying the hashtag rows... (around 10 min)')\n",
    "\n",
    "# Initial reset of dictionary of rows to start the processing\n",
    "reset_added_hashtag_rows_list()\n",
    "tw5_1 = tw5.copy()\n",
    "\n",
    "# The fct multiplyHashtagRows (applied to each row) will return the first hashtag and add all the other hashtags to the dictionary.\n",
    "tw5_1['hashtag'] = tw5.apply(multiplyHashtagRows, args=[tw5.columns,], axis=1)\n",
    "\n",
    "# This will return the dictionary with the added rows.\n",
    "addedHashtagsRowsList = get_added_hashtag_rows_list()\n",
    "\n",
    "pr('Finished! {} rows will be added to the dataframe!'.format(len(addedHashtagsRowsList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a new dataframe with the additionnal rows and merge it with the old one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:05:35 Starting to make the new dataframe with additionnal rows and append it to the original dataframe..\n",
      "20:05:35 Done. Original dataframe size was: 15.043 - New dataframe size is: 21.855\n"
     ]
    }
   ],
   "source": [
    "pr('Starting to make the new dataframe with additionnal rows and append it to the original dataframe..')\n",
    "\n",
    "addedHashtagsDf = pd.DataFrame(addedHashtagsRowsList)\n",
    "addedHashtagsDf.set_index(['createdAt'], inplace=True)\n",
    "\n",
    "tw6 = tw5_1.append(addedHashtagsDf)\n",
    "\n",
    "pr('Done. Original dataframe size was: {} - New dataframe size is: {}'.format(strNb(len(tw5_1)),strNb(len(tw6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of tweets (with only text and hashtag column):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>createdAt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-02-23 09:59:41</th>\n",
       "      <td>Magic spells run off after midnight, I guess s...</td>\n",
       "      <td>fb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-23 11:28:27</th>\n",
       "      <td>Limitas of public transportation! No taxi, rai...</td>\n",
       "      <td>yam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-23 17:47:11</th>\n",
       "      <td>So, Feierabend. Jetzt #24 und später #VfB. — a...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text hashtag\n",
       "createdAt                                                                     \n",
       "2010-02-23 09:59:41  Magic spells run off after midnight, I guess s...      fb\n",
       "2010-02-23 11:28:27  Limitas of public transportation! No taxi, rai...     yam\n",
       "2010-02-23 17:47:11  So, Feierabend. Jetzt #24 und später #VfB. — a...      24"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Examples of tweets (with only text and hashtag column):')\n",
    "tw6[['text', 'hashtag']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Gouping per hashtag and merging per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are grouping each tweets by hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:05:35 Grouping by hastag.\n",
      "20:05:35 Done\n"
     ]
    }
   ],
   "source": [
    "pr('Grouping by hastag.')\n",
    "tw6['numberOfTweets'] = 1 ## We make a column number of tweets that will be useful later \n",
    "gp = tw6.groupby('hashtag')\n",
    "pr('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will merge all the tweets with the same hashtag that happened during a particular day.\n",
    "\n",
    "For the column containing the tweet text, we will use a join with a special delimiter to recognize the different tweet text. We made the choice to take the median of the longitude and latitude. This was done (instead of the mean) because we noticed in the data some tweets that were not localized in Switzerland at all, but very far from it. This should avoid having a value strongly biased by extremas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delimiter = '_$$$_'\n",
    "str_join = lambda x: delimiter.join(x)\n",
    "\n",
    "def aggDate(df):\n",
    "    '''\n",
    "    Function that applies to a dataframe will group each row by day and aggregate all its content.\n",
    "    '''\n",
    "    temp = df.groupby(df.index.map(lambda x: x.date))\n",
    "    groupedDf = temp.agg({  'text' : str_join, ## Merged text\n",
    "                            'longitude' : np.median, ## Median of the longitude\n",
    "                            'latitude' : np.median, ## Median of the latitude\n",
    "                            'hashtag' : lambda x: x.iloc[0], ## The name of the hashtag\n",
    "                            'numberOfTweets' : 'count', ## Number of tweets during the day\n",
    "                            'userId' : pd.Series.nunique}) ## Number of unique users\n",
    "    # rename userId column to a more representative name\n",
    "    return groupedDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better manipulate the data, we create a dictionary, with each key representing a hashtag. The dictionary value corresponding to the hashtag will be a dataframe that contains tweets grouped by day.<br>\n",
    "The form will be: {**'hashtag'** : _dataframe containing all tweets of corresponding hashtag grouped per day_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:05:35 Putting hashtags in dictionary... (around 50 min)\n",
      "20:05:40 10%\n",
      "20:05:44 20%\n",
      "20:05:49 30%\n",
      "20:05:54 40%\n",
      "20:05:58 50%\n",
      "20:06:03 60%\n",
      "20:06:08 70%\n",
      "20:06:12 80%\n",
      "20:06:17 90%\n",
      "20:06:22 100%\n",
      "20:06:22 Finished operations! Dictionary with 9410 different hashtags.\n"
     ]
    }
   ],
   "source": [
    "pr('Putting hashtags in dictionary... (around 50 min)')\n",
    "dictionary = {}\n",
    "\n",
    "# Printing variables\n",
    "count = 0\n",
    "lengp = len(gp)\n",
    "printingValue = int(lengp / 10)\n",
    "\n",
    "for hashtag, df in gp:\n",
    "    # Grouping per date\n",
    "    dictionary[hashtag] = aggDate(df)\n",
    "    \n",
    "    # Printing information\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/lengp*100))\n",
    "        \n",
    "pr('Finished operations! Dictionary with {} different hashtags.'.format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of the dictionary entry \"horror\":\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>longitude</th>\n",
       "      <th>numberOfTweets</th>\n",
       "      <th>latitude</th>\n",
       "      <th>text</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-08-09</th>\n",
       "      <td>horror</td>\n",
       "      <td>9.35622</td>\n",
       "      <td>1</td>\n",
       "      <td>47.4195</td>\n",
       "      <td>El rey de la montaña - Unsichtbare Gefahr #fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-11-03</th>\n",
       "      <td>horror</td>\n",
       "      <td>8.53639</td>\n",
       "      <td>1</td>\n",
       "      <td>47.3662</td>\n",
       "      <td>Ich glaube GfK hat das erste Mal eine PP Präse...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           hashtag  longitude  numberOfTweets  latitude  \\\n",
       "2010-08-09  horror    9.35622               1   47.4195   \n",
       "2011-11-03  horror    8.53639               1   47.3662   \n",
       "\n",
       "                                                         text  userId  \n",
       "2010-08-09  El rey de la montaña - Unsichtbare Gefahr #fil...       1  \n",
       "2011-11-03  Ich glaube GfK hat das erste Mal eine PP Präse...       1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Example of the dictionary entry \"{}\":'.format(list(dictionary.keys())[5]))\n",
    "dictionary[list(dictionary.keys())[5]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Event detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Parameters to define events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that define events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Parameters of an event:\n",
    "MIN_TOT_NB_TWEETS = 20 ## The hashtag must have happened at least this number of times in all tweets to be considered.\n",
    "MIN_NB_DAYS_WITH_HASHTAGS = 3 ## The hashtags must appear at least this number of different days to be considered.\n",
    "MIN_NB_TWEETS_DURING_EVENT = 7 ## To be considered an event, the hashtag must happen at least this nb of times during the day.\n",
    "THRESHOLD_ANOMALY_FACTOR = 2.5 ## The occurence of a hashtag during a single day must be above the mean by this FACTOR\n",
    "                             ## multiplied by the std to be considered as an event.\n",
    "MAX_DURATION_OF_EVENT = timedelta(days=30) ## The maximum number of days we consider an event can happen\n",
    "MIN_DURATION_BEFORE_NEW_EVENT = timedelta(days=304) ## (= 10 months) The min time that should pass before an event can happen\n",
    "                                                    ## again and still be considered as event (ie. Christmas is an event\n",
    "                                                    ## each year)\n",
    "MIN_NUMBER_DIFFERENT_USER = 2 # To state that an event occured, a minimum number of different users should have tweeted about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to detect recurrent events that should be removed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Events detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method that will be applied to each row that will return true if the single day for a defined hashtag should be considered as an event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isEvent(row, threshold):\n",
    "    minNbTweet = max(threshold, MIN_NB_TWEETS_DURING_EVENT)\n",
    "    return row.numberOfTweets >= minNbTweet and row.userId >= MIN_NUMBER_DIFFERENT_USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main method to detect event according to all the above parameters.<br>\n",
    "The external function _isSpecificEventListIllegal()_ will examine a time series of potential events for a defined hashtag, and detect recurrent events. If the events are too close to each other, they will be cathegorized as recurrent events and be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:12:25 Starting to compute 9410 different hashtags to detect event. (4 min)\n",
      "20:12:25 10%\n",
      "20:12:25 20%\n",
      "20:12:25 30%\n",
      "20:12:25 40%\n",
      "20:12:25 50%\n",
      "20:12:25 60%\n",
      "20:12:25 70%\n",
      "20:12:25 80%\n",
      "20:12:25 90%\n",
      "20:12:25 100%\n",
      "20:12:25 Done. 7 events were detected.\n"
     ]
    }
   ],
   "source": [
    "# EXTERNAL FUNCTION: isSpecificEventListIllegal(detectedEventDateList, max_event_duration, min_duration_before_new_event),\n",
    "\n",
    "pr('Starting to compute {} different hashtags to detect event. (4 min)'.format(len(dictionary)))\n",
    "\n",
    "# Printing variables\n",
    "nbOfEventDetected = 0\n",
    "count = 0\n",
    "printingValue = int(len(dictionary) / 10)\n",
    "\n",
    "# Going through all items of dictionary\n",
    "for [h,df] in dictionary.items():\n",
    "    \n",
    "    # Printing information\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "        \n",
    "    # Making the different tests corresponding to the above parameters to detect event\n",
    "    if len(df) > MIN_NB_DAYS_WITH_HASHTAGS and df['numberOfTweets'].sum() >= MIN_TOT_NB_TWEETS:\n",
    "        \n",
    "        threshold = df['numberOfTweets'].mean() + THRESHOLD_ANOMALY_FACTOR * df['numberOfTweets'].std()\n",
    "        df['event'] = df.apply(isEvent, args=[threshold,], axis=1)          \n",
    "\n",
    "        # Remove recurrent events:\n",
    "        detectedEventDf = df[df['event']]\n",
    "        if len(detectedEventDf) > 2 and isSpecificEventListIllegal(detectedEventDf.index, MAX_DURATION_OF_EVENT, MIN_DURATION_BEFORE_NEW_EVENT):\n",
    "            df['event'] = False\n",
    "                \n",
    "        # Printing counts updated\n",
    "        nbOfEventDetected += len(df[df['event']])\n",
    "            \n",
    "pr('Done. {} events were detected.'.format(strNb(nbOfEventDetected)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pr('Starting to compute {} dict items to detect event. (4 min)'.format(len(dictionary)))\n",
    "# nbOfEventDetected = 0\n",
    "# count = 0\n",
    "# printingValue = int(len(dictionary) / 10)\n",
    "# for [h,df] in dictionary.items():\n",
    "#     count += 1\n",
    "#     if count % printingValue == 0:\n",
    "#         pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "#     df['event'] = False\n",
    "#     if len(df) > MIN_NB_DAYS_WITH_HASHTAGS:\n",
    "#         if df['numberOfTweets'].sum() > MIN_TOT_NB_TWEETS:\n",
    "#             threshold = df['numberOfTweets'].mean() + THRESHOLD_ANOMALY_FACTOR * df['numberOfTweets'].std()\n",
    "#             df['event'] = df.numberOfTweets.apply(lambda x: x >= threshold and x >= MIN_NB_TWEETS_DURING_EVENT)\n",
    "            \n",
    "#             ## Remove recurrent events:\n",
    "#             detectedEventDf = df[df['event']]\n",
    "#             if len(detectedEventDf) > 2 and isSpecificEventListIllegal(detectedEventDf.index):\n",
    "#                 df['event'] = False\n",
    "#             nbOfEventDetected += len(df[df['event']])\n",
    "# pr('Finished! Number of events detected = {}'.format(nbOfEventDetected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging close events and grouping into single event dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have a function that is going to merge events that are considered as too \"close\" to each other to be considered individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def mergeCloseEvents(rowsList):\n",
    "#     '''\n",
    "#     Take a list of dictionary, where each dictionary is a \"row\" of the event df, which contained detected events.\n",
    "#     It will process the list to detect event that are close and merge them together.\n",
    "#     Return : the processed list of event.\n",
    "#     '''\n",
    "    \n",
    "#     def areCloseEvents(event1, event2):\n",
    "#         '''\n",
    "#         Return true is 2 events dates are defined as \"close\"\n",
    "#         '''\n",
    "#         return abs(event1['date'] - event2['date']) < MAX_DURATION_OF_EVENT\n",
    "        \n",
    "#     def mergeCloseEventsSublist(closeEventList):\n",
    "#         '''\n",
    "#         This will be applied to each close event sublist. It will merge all events into one unique event.\n",
    "#         The event will consist of the total number of tweets, with the concatenation of the tweet texts and the mean\n",
    "#         of longitude/latitude. A meanDate will be defined as a ponderated mean between all dates.\n",
    "#         The final date will be the one that is in the closeEventList and is closest to this mean date.\n",
    "#         We did this to keep the meaning of the date if it had some, and not have some meaningless \"mean-date\".\n",
    "#         '''\n",
    "#         latitude = 0\n",
    "#         longitude = 0\n",
    "#         numberOfTweets = 0\n",
    "#         text = \"\"\n",
    "#         originalDate = closeEventList[0]['date']\n",
    "#         dateDiff = timedelta(days=0)\n",
    "#         first = True        \n",
    "#         for tweet in closeEventList:\n",
    "#             longitude += tweet['longitude']\n",
    "#             latitude += tweet['latitude'] \n",
    "#             numberOfTweets += tweet['numberOfTweets']\n",
    "#             if first:\n",
    "#                 text = tweet['text']\n",
    "#                 first = False\n",
    "#             else:\n",
    "#                 text += delimiter + tweet['text']\n",
    "#                 dateDiff = dateDiff + (tweet['date'] - originalDate) * tweet['numberOfTweets']\n",
    "\n",
    "#         ## It is multiplied by 2 then soustracted to round correctly to the nearest day\n",
    "#         meanDate = originalDate + 2* dateDiff / numberOfTweets - dateDiff / numberOfTweets        \n",
    "#         latitude = latitude / len(closeEventList)\n",
    "#         longitude = longitude / len(closeEventList)\n",
    "        \n",
    "#         ## We are going to detect the event the closest to the mean date\n",
    "#         minSelectedDate = closeEventList[0]['date']\n",
    "#         minDistance = abs(closeEventList[0]['date'] - meanDate)\n",
    "#         for tweet in closeEventList:\n",
    "#             if abs(tweet['date'] - meanDate) < minDistance:\n",
    "#                 minSelectedDate = tweet['date']    \n",
    "        \n",
    "#         return {'date': minSelectedDate, 'hashtag': closeEventList[0]['hashtag'], 'text': text,\n",
    "#                     'longitude': longitude, 'latitude':latitude, 'numberOfTweets': numberOfTweets, }\n",
    "    \n",
    "#     ############ -----  MAIN METHOD  ----- ############\n",
    "    \n",
    "#     ## If the list is big enough, go through the list and form an export list and merge elements that needs to.\n",
    "#     if len(rowsList) < 2:\n",
    "#         return rowsList\n",
    "#     else:\n",
    "#         firstLastPosOfItemsToMerge = []\n",
    "#         sortedRowsList = sorted(rowsList, key=itemgetter('date')) \n",
    "#         exportedEventList = []\n",
    "#         ## This goes through the *sorted* list and add the pair of indices (first indice and last indice) where events \n",
    "#         ## that should be merged appear.\n",
    "#         lastEventWasClose = False\n",
    "#         firstItem = -1\n",
    "#         for i in range(0, len(sortedRowsList)-1):\n",
    "#             if areCloseEvents(sortedRowsList[i], sortedRowsList[i+1]):\n",
    "#                 if not lastEventWasClose: # So it is the first pairs of the sublist of close events in the whole list\n",
    "#                     firstItem = i\n",
    "#                     lastEventWasClose = True\n",
    "#             else:\n",
    "#                 if lastEventWasClose: # So the list has just ended.\n",
    "#                     exportedEventList.append(mergeCloseEventsSublist(sortedRowsList[firstItem:i+1]))\n",
    "#                     lastEventWasClose = False\n",
    "#                 else: # The element is by itself, let's append it\n",
    "#                     exportedEventList.append(sortedRowsList[i])  \n",
    "#         if lastEventWasClose: # If there were events to merge till the last elem of list\n",
    "#             exportedEventList.append(mergeCloseEventsSublist(sortedRowsList[firstItem:len(sortedRowsList)]))\n",
    "#         else:\n",
    "#             exportedEventList.append(sortedRowsList[len(sortedRowsList)-1])\n",
    "    \n",
    "#     return exportedEventList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be applied to each dataframe. If a row is detected as an event, it will be added to the locaRowsList which will be used to make a general dataframe of all the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "localRowsList = []\n",
    "def applyToMakeEventDf(row):\n",
    "    if row.event:\n",
    "        rowToAdd = {'date': row.name, 'hashtag': row.hashtag, 'text': row.text,\n",
    "                    'longitude': row.longitude, 'latitude':row.latitude, 'numberOfTweets': row.numberOfTweets, }\n",
    "        global localRowsList\n",
    "        localRowsList.append(rowToAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eventRowsList = []\n",
    "localRowsList = []\n",
    "count = 0\n",
    "printingValue = int(len(dictionary) / 10)\n",
    "\n",
    "pr('Starting to make event df with {} dataframes. (around 6 min)'.format(len(dictionary)))\n",
    "for h, df in dictionary.items():\n",
    "    global localRowsList\n",
    "    localRowsList = []\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "        \n",
    "    df.apply(applyToMakeEventDf, axis=1)\n",
    "    mergedList = mergeCloseEvents(localRowsList) # merging close events\n",
    "    eventRowsList += mergedList\n",
    "\n",
    "pr('Making new dataframe.')\n",
    "new_events = pd.DataFrame(eventRowsList)\n",
    "new_events.set_index(['date'], inplace=True)\n",
    "pr('Finished! Dataframe with {} rows'.format(len(new_events)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Events dataframe:')\n",
    "new_events.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Linked dataframe of all days:')\n",
    "dictionary[new_events.iloc[0].hashtag].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we worked with another team, we needed a way to communicate them our detection. We used a JSON with all the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_number_of_events = len(new_events)\n",
    "print('There are {} events.'.format(total_number_of_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_df = new_events.copy()\n",
    "e_df['date'] = e_df.index\n",
    "e_df.index = [i for i in range (len(e_df))]\n",
    "e_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to generate the right datetimes for the jsons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# epoch_dt = datetime(1970, 1, 1)\n",
    "# def to_utc(date):\n",
    "#     d_dt = datetime.combine(date, datetime.min.time())\n",
    "#     return int((d_dt - epoch_dt).total_seconds()*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def convert_to_unix_time(record):\n",
    "#     datetime_index = pd.DatetimeIndex([datetime(record['year'], record['month'], 1)])\n",
    "#     unix_time_index = datetime_index.astype(np.int64) // 10**6\n",
    "#     return unix_time_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr('Converting dates...')\n",
    "e_df['year'] = e_df['date'].apply(lambda x: x.year)\n",
    "e_df['month'] = e_df['date'].apply(lambda x: x.month)\n",
    "e_df['utc_date'] = e_df['date'].apply(lambda x: to_utc(x))\n",
    "e_df['unix_time'] = e_df.apply(convert_to_unix_time, axis=1)\n",
    "pr('Done.')\n",
    "e_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation of a JSON is easier from a dictionary than from a dataframe. Also, the other team we worked with asked us to group events by months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grouping by months\n",
    "e_gb_month = e_df.groupby(e_df.unix_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generation of the dictionary for the final JSON\n",
    "pr('Making event list...')\n",
    "months = []\n",
    "for month, df in e_gb_month:\n",
    "    days = []\n",
    "    for i in range (len(df)):\n",
    "        ht = df.iloc[i]['hashtag']\n",
    "        lat = df.iloc[i]['latitude']\n",
    "        lon = df.iloc[i]['longitude']\n",
    "        t_num = df.iloc[i]['numberOfTweets']\n",
    "        tweets = df.iloc[i]['text'].split(delimiter)\n",
    "        date = df.iloc[i]['utc_date']\n",
    "        \n",
    "        data_unit = { 'name': ht\n",
    "                    , 'latitude' : lat\n",
    "                    , 'longitude' : lon\n",
    "                    , 'tweets' : tweets\n",
    "                    , 'number_of_tweets' : str(t_num)\n",
    "                    , 'date' : int(date)}\n",
    "        days.append(data_unit)\n",
    "    \n",
    "    curr_month = {'date': int(month), 'data' : days}\n",
    "    months.append(curr_month)\n",
    "\n",
    "final_events = {'events' : months}\n",
    "pr('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the final JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exportFilename = 'export_twitter_events_' + datetime.now().strftime(\"%Y-%m-%d_%Hh%Mmin%S\") + \\\n",
    "'_' + str(total_number_of_events)+ '_events.json'\n",
    "exportPath =  os.path.join('data', exportFilename)\n",
    "\n",
    "pr('Exporting to json...')\n",
    "with open(exportPath, 'w') as f:\n",
    "     json.dump(final_events, f)\n",
    "pr('Export done. File \"{}\" has been created.'.format(exportFilename))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ADA-kernel]",
   "language": "python",
   "name": "conda-env-ADA-kernel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
