{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event detection on a Twitter dataset\n",
    "The goal of this project is to detect past events in Switzerland, using a dataset of tweets. This dataset contains 28 million tweets coming for most of them from Switzerland. This notebook will get you through our project, explaining our methodology. It is splitted in 5 parts:\n",
    "\n",
    "\n",
    "1. **Extracting hashtags and data cleaning**<br>\n",
    "The data are cleaned. The hashtags in all the tweets are found and extracted.\n",
    "2. **Data structure manipulation**<br>\n",
    "The tweet are grouped per hashtag, and discretized per day.\n",
    "3. **Event detection**<br>\n",
    "Events are detected. Improvement is done by removing recurrent event and merging close events.\n",
    "4. **Exporting the data**<br>\n",
    "The data is exported in the JSON format to allow another group to provide a visualization with it.\n",
    "5. **Conclusion**<br>\n",
    "A small conclusion on the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To make our code more readable, we exported some functions to the file _\"utils.py\"_. Each time we will use one of these functions, we will explicitely write: *# EXTERNAL FUNCTION: name_of_the_function()*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import json\n",
    "import csv\n",
    "from datetime import timedelta\n",
    "from utils import * # File where some functions were exported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful printing methods that will be used through the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pr(strToPrint):\n",
    "    '''\n",
    "    Print the current date and time, concatenated to the string passed in argument.   \n",
    "    :param strToPrint: Regular string to print\n",
    "    '''\n",
    "    print(str(datetime.now().time())[:8] + ' '+ strToPrint)\n",
    "    \n",
    "def strNb(nb):\n",
    "    '''\n",
    "    Transform a high number in a string, with '.' for each thousand\n",
    "    :param nb: A high number to print\n",
    "    '''\n",
    "    return '{0:,}'.format(nb).replace(',', '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a sample of the dataset (useful for testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle_filename = os.path.join('data','head_100k_pickle.pkl')\n",
    "# tw = pd.read_pickle(pickle_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing columns headers and file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_header = ['id', 'userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId',\n",
    "                  'inReplyTo', 'source', 'truncated', 'placeLatitude', 'placeLongitude', 'sourceName', 'sourceUrl',\n",
    "                 'userName', 'screenName', 'followersCount', 'friendsCount', 'statusesCount',\n",
    "                 'userLocation']\n",
    "filename = os.path.join('data', 'twex.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:13:50 Starting to read file... (3 min)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\Anaconda3\\envs\\ADA-kernel\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (0,1,2,3,5,6,8,9,10,11,12,13,14,15,16,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:16:21 File is loaded.\n"
     ]
    }
   ],
   "source": [
    "pr('Starting to read file... (3 min)')\n",
    "tw = pd.read_csv(filename, sep='\\t', encoding='utf-8', escapechar='\\\\', names=columns_header,\n",
    "                      quoting=csv.QUOTE_NONE, na_values='N', header=None)\n",
    "pr('File is loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 27.632.392 tweets.\n"
     ]
    }
   ],
   "source": [
    "print('The dataset contains {} tweets.'.format(strNb(len(tw))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>placeLatitude</th>\n",
       "      <th>placeLongitude</th>\n",
       "      <th>sourceName</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>userName</th>\n",
       "      <th>screenName</th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>userLocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9514097914</td>\n",
       "      <td>1.7341e+07</td>\n",
       "      <td>2010-02-23 05:55:51</td>\n",
       "      <td>Guuuuten Morgen! :-)</td>\n",
       "      <td>7.43926</td>\n",
       "      <td>46.9489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TwitBird</td>\n",
       "      <td>http://www.nibirutech.com</td>\n",
       "      <td>Tilman Jentzsch</td>\n",
       "      <td>blickwechsel</td>\n",
       "      <td>586</td>\n",
       "      <td>508.0</td>\n",
       "      <td>9016.0</td>\n",
       "      <td>Bern, Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9514846412</td>\n",
       "      <td>7.19828e+06</td>\n",
       "      <td>2010-02-23 06:22:40</td>\n",
       "      <td>Still the best coffee in town — at La Stanza h...</td>\n",
       "      <td>8.53781</td>\n",
       "      <td>47.3678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gowalla</td>\n",
       "      <td>http://gowalla.com/</td>\n",
       "      <td>Nico Luchsinger</td>\n",
       "      <td>halbluchs</td>\n",
       "      <td>1820</td>\n",
       "      <td>703.0</td>\n",
       "      <td>4687.0</td>\n",
       "      <td>Zurich, Switzerland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       userId            createdAt  \\\n",
       "0  9514097914   1.7341e+07  2010-02-23 05:55:51   \n",
       "1  9514846412  7.19828e+06  2010-02-23 06:22:40   \n",
       "\n",
       "                                                text  longitude latitude  \\\n",
       "0                               Guuuuten Morgen! :-)    7.43926  46.9489   \n",
       "1  Still the best coffee in town — at La Stanza h...    8.53781  47.3678   \n",
       "\n",
       "  placeId  inReplyTo source truncated placeLatitude placeLongitude sourceName  \\\n",
       "0     NaN        NaN    197       NaN           NaN            NaN   TwitBird   \n",
       "1     NaN        NaN    550       NaN           NaN            NaN    Gowalla   \n",
       "\n",
       "                   sourceUrl         userName    screenName followersCount  \\\n",
       "0  http://www.nibirutech.com  Tilman Jentzsch  blickwechsel            586   \n",
       "1        http://gowalla.com/  Nico Luchsinger     halbluchs           1820   \n",
       "\n",
       "   friendsCount  statusesCount         userLocation  \n",
       "0         508.0         9016.0    Bern, Switzerland  \n",
       "1         703.0         4687.0  Zurich, Switzerland  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('First rows of dataset:')\n",
    "tw.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Extracting hashtags and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Extracting hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect event, we have decided to only concentrate on hashtags. Therefore, we are going extract hashtags and only keep tweets that contain hashtags (as the other ones will contains no useful information for us). We start with this operation before the data cleaning because it is the one that reduces our data size the most, and therefore all further operations will execute quicker.<br>\n",
    "To do so, we are going to examine the _text_ field of each tweet, extract its hashtags and put them in a new column (in the form of a list of hashtags per tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:16:21 Extracting hashtags... (2 min)\n",
      "01:18:11 We have extracted 3.875.280 rows with hashtags out of the 27.632.392 rows of our initial dataframe.\n"
     ]
    }
   ],
   "source": [
    "# EXTERNAL FUNCTION: extract_hashtags(text)\n",
    "\n",
    "pr('Extracting hashtags... (2 min)')\n",
    "\n",
    "tw['hashtag'] = tw.text.apply(lambda x: extract_hashtags(str(x))) # Getting list of hashtag into new column\n",
    "twh = tw.ix[tw.hashtag.apply(lambda x: len(x) != 0)] # droping the rows (tweets) that contains no hashtags.\n",
    "\n",
    "pr('We have extracted {} rows with hashtags out of the {} rows of our initial dataframe.'.format(strNb(len(twh)),strNb(len(tw))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of tweets (with only text and hashtag column):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Magic spells run off after midnight, I guess s...</td>\n",
       "      <td>[fb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Limitas of public transportation! No taxi, rai...</td>\n",
       "      <td>[yam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>So, Feierabend. Jetzt #24 und später #VfB. — a...</td>\n",
       "      <td>[24, vfb]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text    hashtag\n",
       "8   Magic spells run off after midnight, I guess s...       [fb]\n",
       "10  Limitas of public transportation! No taxi, rai...      [yam]\n",
       "15  So, Feierabend. Jetzt #24 und später #VfB. — a...  [24, vfb]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Examples of tweets (with only text and hashtag column):')\n",
    "twh[['text', 'hashtag']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now clean the remaining dataset. The creation date is important for our event detection, and in order to provide visualization to the other team, we needed to give location information to all tweets. Therefore we will drop the rows which contain NA values at _creationAt_, _latitude_ or _longitude_ position.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:18:11 Droping rows with NA values (location and creation date).\n",
      "01:18:16 The data have been reduced from 3.875.280 tweets to 2.107.021 tweets.\n"
     ]
    }
   ],
   "source": [
    "pr(\"Droping rows with NA values (location and creation date).\")\n",
    "tw1 = twh.dropna(axis=0, how='any', subset=['createdAt'])\n",
    "tw1.dropna(subset=['longitude'], inplace=True)\n",
    "tw1.dropna(subset=['latitude'], inplace=True)\n",
    "pr('The data have been reduced from {} tweets to {} tweets.'.format(strNb(len(twh)), strNb(len(tw1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latitude and longitude values should be in _float_ format to analyze them correctly later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw1.latitude = tw1.latitude.apply(float)\n",
    "tw1.longitude = tw1.longitude.apply(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some date in our initial data could not be transformed to pandas datetime. Let's check if everything is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:18:18 Starting to examine dates...\n",
      "01:18:19 There are 0 dates that cannot be transformed.\n"
     ]
    }
   ],
   "source": [
    "pr('Starting to examine dates...')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "datetime_serie = tw1['createdAt'].convert_objects(convert_dates='coerce')\n",
    "dateNotConvertible = datetime_serie[pd.isnull(datetime_serie)]\n",
    "warnings.filterwarnings('default')\n",
    "pr('There are {} dates that cannot be transformed.'.format(len(dateNotConvertible)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks good! Now, we will transform our data in a time series. To do so, the best way is to put the creation date of each row as the index of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:18:19 Converting to datetime...\n",
      "01:18:21 Setting up new indices...\n",
      "01:18:21 Deleting old \"createdAt\" column...\n",
      "01:18:21 Done. The updated dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>placeLatitude</th>\n",
       "      <th>placeLongitude</th>\n",
       "      <th>sourceName</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>userName</th>\n",
       "      <th>screenName</th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>userLocation</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>createdAt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-02-23 09:59:41</th>\n",
       "      <td>9519737890</td>\n",
       "      <td>1.46579e+07</td>\n",
       "      <td>Magic spells run off after midnight, I guess s...</td>\n",
       "      <td>6.1387</td>\n",
       "      <td>46.175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>http://twitter.com/#!/download/iphone</td>\n",
       "      <td>Javier Belmonte</td>\n",
       "      <td>vichango</td>\n",
       "      <td>167</td>\n",
       "      <td>277.0</td>\n",
       "      <td>2885.0</td>\n",
       "      <td>Geneva, Switzerland</td>\n",
       "      <td>[fb]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id       userId  \\\n",
       "createdAt                                      \n",
       "2010-02-23 09:59:41  9519737890  1.46579e+07   \n",
       "\n",
       "                                                                  text  \\\n",
       "createdAt                                                                \n",
       "2010-02-23 09:59:41  Magic spells run off after midnight, I guess s...   \n",
       "\n",
       "                     longitude  latitude placeId  inReplyTo source truncated  \\\n",
       "createdAt                                                                      \n",
       "2010-02-23 09:59:41     6.1387    46.175     NaN        NaN      1       NaN   \n",
       "\n",
       "                    placeLatitude placeLongitude          sourceName  \\\n",
       "createdAt                                                              \n",
       "2010-02-23 09:59:41           NaN            NaN  Twitter for iPhone   \n",
       "\n",
       "                                                 sourceUrl         userName  \\\n",
       "createdAt                                                                     \n",
       "2010-02-23 09:59:41  http://twitter.com/#!/download/iphone  Javier Belmonte   \n",
       "\n",
       "                    screenName followersCount  friendsCount  statusesCount  \\\n",
       "createdAt                                                                    \n",
       "2010-02-23 09:59:41   vichango            167         277.0         2885.0   \n",
       "\n",
       "                            userLocation hashtag  \n",
       "createdAt                                         \n",
       "2010-02-23 09:59:41  Geneva, Switzerland    [fb]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr('Converting to datetime...')\n",
    "tw5 = tw1.copy()\n",
    "tw5['createdAt'] = pd.to_datetime(tw1['createdAt'])\n",
    "pr('Setting up new indices...')\n",
    "tw5.index = tw5['createdAt']\n",
    "pr('Deleting old \"createdAt\" column...')\n",
    "del tw5['createdAt']\n",
    "pr('Done. The updated dataframe:')\n",
    "tw5.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data structure manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have the data in the right format to process them. Therefore we will apply several operations to the dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - One hashtag = one row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Some people put more than one hashtag per tweet. A tweet that contain more than one hashtag will be analyzed the same way as if it contained only one hashtag. So, to process the data in a easier way, we will make a dataframe with **one row per hashtag**. Therefore, we will duplicate the row that contains more than one hashtag.\n",
    "\n",
    "This will be done by going through the dataframe, and making in parallel a list of rows (with 1 hashtag per row) that needs to be added to the old dataframe. These rows will be stored in a dictionary and will be appended afterwards to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:18:21 Multiplying the hashtag rows... (around 10 min)\n",
      "01:23:52 Finished! 2.265.316 rows will be added to the dataframe!\n"
     ]
    }
   ],
   "source": [
    "# EXTERNAL FUNCTIONS: reset_added_hashtag_rows_list(), multiplyHashtagRows(row), get_added_hashtag_rows_list()\n",
    "\n",
    "pr('Multiplying the hashtag rows... (around 10 min)')\n",
    "\n",
    "# Initial reset of dictionary of rows to start the processing\n",
    "reset_added_hashtag_rows_list()\n",
    "tw5_1 = tw5.copy()\n",
    "\n",
    "# The fct multiplyHashtagRows (applied to each row) will return the first hashtag and add all the other hashtags to the dictionary.\n",
    "tw5_1['hashtag'] = tw5.apply(multiplyHashtagRows, args=[tw5.columns,], axis=1)\n",
    "\n",
    "# This will return the dictionary with the added rows.\n",
    "addedHashtagsRowsList = get_added_hashtag_rows_list()\n",
    "\n",
    "pr('Finished! {} rows will be added to the dataframe!'.format(strNb(len(addedHashtagsRowsList))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a new dataframe with the additionnal rows and merge it with the old one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:23:52 Making the new dataframe of additionnal rows and appending it to the original dataframe..\n",
      "01:24:04 Done: Original dataframe size was: 2.107.021 - New dataframe size is: 4.372.337\n"
     ]
    }
   ],
   "source": [
    "pr('Making the new dataframe of additionnal rows and appending it to the original dataframe..')\n",
    "\n",
    "addedHashtagsDf = pd.DataFrame(addedHashtagsRowsList)\n",
    "addedHashtagsDf.set_index(['createdAt'], inplace=True)\n",
    "\n",
    "tw6 = tw5_1.append(addedHashtagsDf)\n",
    "\n",
    "pr('Done: Original dataframe size was: {} - New dataframe size is: {}'.format(strNb(len(tw5_1)),strNb(len(tw6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of tweets (with only text and hashtag column):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>createdAt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-02-23 09:59:41</th>\n",
       "      <td>Magic spells run off after midnight, I guess s...</td>\n",
       "      <td>fb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-23 11:28:27</th>\n",
       "      <td>Limitas of public transportation! No taxi, rai...</td>\n",
       "      <td>yam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-23 17:47:11</th>\n",
       "      <td>So, Feierabend. Jetzt #24 und später #VfB. — a...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text hashtag\n",
       "createdAt                                                                     \n",
       "2010-02-23 09:59:41  Magic spells run off after midnight, I guess s...      fb\n",
       "2010-02-23 11:28:27  Limitas of public transportation! No taxi, rai...     yam\n",
       "2010-02-23 17:47:11  So, Feierabend. Jetzt #24 und später #VfB. — a...      24"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Examples of tweets (with only text and hashtag column):')\n",
    "tw6[['text', 'hashtag']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Gouping per hashtag and merging per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are grouping each tweets by hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:24:07 Grouping by hastag.\n",
      "01:24:07 Done\n"
     ]
    }
   ],
   "source": [
    "pr('Grouping by hastag.')\n",
    "tw6['numberOfTweets'] = 1 ## We make a column number of tweets that will be useful later \n",
    "gp = tw6.groupby('hashtag')\n",
    "pr('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will merge all the tweets with the same hashtag that happened during a particular day.\n",
    "\n",
    "For the column containing the tweet text, we will use a join with a special delimiter to recognize the different tweet text. We made the choice to take the median of the longitude and latitude. This was done (instead of the mean) because we noticed in the data some tweets that were not localized in Switzerland at all, but very far from it. This should avoid having a value strongly biased by extremas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delimiter = '_$$$_'\n",
    "str_join = lambda x: delimiter.join(x)\n",
    "\n",
    "def aggDate(df):\n",
    "    '''\n",
    "    Function that applies to a dataframe will group each row by day and aggregate all its content.\n",
    "    '''\n",
    "    temp = df.groupby(df.index.map(lambda x: x.date))\n",
    "    groupedDf = temp.agg({  'text' : str_join, ## Merged text\n",
    "                            'longitude' : np.median, ## Median of the longitude\n",
    "                            'latitude' : np.median, ## Median of the latitude\n",
    "                            'hashtag' : lambda x: x.iloc[0], ## The name of the hashtag\n",
    "                            'numberOfTweets' : 'count', ## Number of tweets during the day\n",
    "                            'userId' : pd.Series.nunique}) ## Number of unique users\n",
    "    # rename userId column to a more representative name\n",
    "    return groupedDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better manipulate the data, we create a dictionary, with each key representing a hashtag. The dictionary value corresponding to the hashtag will be a dataframe that contains tweets grouped by day.<br>\n",
    "The form will be: {**'hashtag'** : _dataframe containing all tweets of corresponding hashtag grouped per day_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:24:07 Putting hashtags in dictionary... (around 50 min)\n",
      "01:30:07 10%\n",
      "01:35:42 20%\n",
      "01:41:11 30%\n",
      "01:46:47 40%\n",
      "01:52:22 50%\n",
      "01:57:55 60%\n",
      "02:03:21 70%\n",
      "02:08:51 80%\n",
      "02:14:20 90%\n",
      "02:19:45 100%\n",
      "02:19:45 Finished operations! Dictionary with 607601 different hashtags.\n"
     ]
    }
   ],
   "source": [
    "pr('Putting hashtags in dictionary... (around 50 min)')\n",
    "dictionary = {}\n",
    "\n",
    "# Printing variables\n",
    "count = 0\n",
    "lengp = len(gp)\n",
    "printingValue = int(lengp / 10)\n",
    "\n",
    "for hashtag, df in gp:\n",
    "    # Grouping per date\n",
    "    dictionary[hashtag] = aggDate(df)\n",
    "    \n",
    "    # Printing information\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/lengp*100))\n",
    "        \n",
    "pr('Finished operations! Dictionary with {} different hashtags.'.format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of the dictionary entry \"1reedition\":\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numberOfTweets</th>\n",
       "      <th>longitude</th>\n",
       "      <th>userId</th>\n",
       "      <th>latitude</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-05-02</th>\n",
       "      <td>2</td>\n",
       "      <td>7.25455</td>\n",
       "      <td>1</td>\n",
       "      <td>46.0904</td>\n",
       "      <td>1reedition</td>\n",
       "      <td>Départ de la Patrouilles des jeunes! #pdg2014 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            numberOfTweets  longitude  userId  latitude     hashtag  \\\n",
       "2014-05-02               2    7.25455       1   46.0904  1reedition   \n",
       "\n",
       "                                                         text  \n",
       "2014-05-02  Départ de la Patrouilles des jeunes! #pdg2014 ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Example of the dictionary entry \"{}\":'.format(list(dictionary.keys())[5]))\n",
    "dictionary[list(dictionary.keys())[5]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Event detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will detect the different events using the hashtags. We will start by creating some parameters that will define what is an event. The parameters final value was found with a lot of testing in order to find the best combination. In the second part we will execute the code that will go though the tweets to detect events. It will also remove recurrent events. In the third part, \"close\" events will be merged together, and the events will all be grouped in a single dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Parameters to define events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that define events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Parameters of an event:\n",
    "MIN_TOT_NB_TWEETS = 20 ## The hashtag must have happened at least this number of times in all tweets to be considered.\n",
    "MIN_NB_DAYS_WITH_HASHTAGS = 3 ## The hashtags must appear at least this number of different days to be considered.\n",
    "MIN_NB_TWEETS_DURING_EVENT = 7 ## To be considered an event, the hashtag must happen at least this nb of times during the day.\n",
    "THRESHOLD_ANOMALY_FACTOR = 2.5 ## The occurence of a hashtag during a single day must be above the mean by this FACTOR\n",
    "                             ## multiplied by the std to be considered as an event.\n",
    "MAX_DURATION_OF_EVENT = timedelta(days=30) ## The maximum number of days we consider an event can happen\n",
    "MIN_DURATION_BEFORE_NEW_EVENT = timedelta(days=304) ## (= 10 months) The min time that should pass before an event can happen\n",
    "                                                    ## again and still be considered as event (ie. Christmas is an event\n",
    "                                                    ## each year)\n",
    "MIN_NUMBER_DIFFERENT_USER = 2 # To state that an event occured, a minimum number of different users should have tweeted about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to detect recurrent events that should be removed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Events detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method that will be applied to each row that will return true if the single day for a defined hashtag should be considered as an event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isEvent(row, threshold):\n",
    "    minNbTweet = max(threshold, MIN_NB_TWEETS_DURING_EVENT)\n",
    "    return row.numberOfTweets >= minNbTweet and row.userId >= MIN_NUMBER_DIFFERENT_USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main method to detect event according to all the above parameters.<br>\n",
    "The external function _isSpecificEventListIllegal()_ will examine a time series of potential events for a defined hashtag, and detect recurrent events. If the events are too close to each other, they will be cathegorized as recurrent events and be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:19:46 Starting to compute 607601 different hashtags to detect event. (4 min)\n",
      "02:20:12 10%\n",
      "02:20:47 20%\n",
      "02:21:13 30%\n",
      "02:21:39 40%\n",
      "02:22:04 50%\n",
      "02:22:30 60%\n",
      "02:22:55 70%\n",
      "02:23:21 80%\n",
      "02:23:47 90%\n",
      "02:24:12 100%\n",
      "02:24:12 Done. 4.138 events were detected.\n"
     ]
    }
   ],
   "source": [
    "# EXTERNAL FUNCTION: isSpecificEventListIllegal(detectedEventDateList, max_event_duration, min_duration_before_new_event),\n",
    "\n",
    "pr('Starting to compute {} different hashtags to detect event. (4 min)'.format(len(dictionary)))\n",
    "\n",
    "# Printing variables\n",
    "nbOfEventDetected = 0\n",
    "count = 0\n",
    "printingValue = int(len(dictionary) / 10)\n",
    "\n",
    "# Going through all items of dictionary\n",
    "for [h,df] in dictionary.items():\n",
    "    \n",
    "    # Printing information\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "    \n",
    "    # Initializing event column\n",
    "    df['event'] = False\n",
    "    \n",
    "    # Making the different tests corresponding to the above parameters to detect event\n",
    "    if len(df) > MIN_NB_DAYS_WITH_HASHTAGS and df['numberOfTweets'].sum() >= MIN_TOT_NB_TWEETS:\n",
    "        \n",
    "        threshold = df['numberOfTweets'].mean() + THRESHOLD_ANOMALY_FACTOR * df['numberOfTweets'].std()\n",
    "        df['event'] = df.apply(isEvent, args=[threshold,], axis=1)          \n",
    "\n",
    "        # Remove recurrent events:\n",
    "        detectedEventDf = df[df['event']]\n",
    "        if len(detectedEventDf) > 2 and isSpecificEventListIllegal(detectedEventDf.index, MAX_DURATION_OF_EVENT, MIN_DURATION_BEFORE_NEW_EVENT):\n",
    "            df['event'] = False\n",
    "                \n",
    "        # Printing counts updated\n",
    "        nbOfEventDetected += len(df[df['event']])\n",
    "            \n",
    "pr('Done. {} events were detected.'.format(strNb(nbOfEventDetected)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Merging close events and grouping into single event dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to group all the detected event into a single dataframe.<br>\n",
    "In this process, we are going to examine each event series  in order to merge events that are considered as too \"close\" to each other to be considered individually. For example, we could have an event that happened on the 24th of January, but there might also be a peak just before and just after it (for example on the 18th of January and 25th of January, because people talked about it before and afterwards). These days should be merged together. The strategy applied is described in details in the external file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eventsWithSpecificHashtagRowList = []\n",
    "\n",
    "def applyToMakeEventDf(row):\n",
    "    '''\n",
    "    This function will be applied to each row of each dataframe of hashtags.\n",
    "    If a row is detected as an event, it will be added to the locaRowsList which will\n",
    "    be used to make a dataframe of all the events.\n",
    "    '''\n",
    "    if row.event:\n",
    "        rowToAdd = {'date': row.name, 'hashtag': row.hashtag, 'text': row.text,\n",
    "                    'longitude': row.longitude, 'latitude':row.latitude, 'numberOfTweets': row.numberOfTweets, }\n",
    "        global eventsWithSpecificHashtagRowList\n",
    "        eventsWithSpecificHashtagRowList.append(rowToAdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main method that will put all events in a single dataframe and merge the events that need to be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:24:12 Starting to make event df with 607601 dataframes. (around 6 min)\n",
      "02:24:44 10%\n",
      "02:25:16 20%\n",
      "02:25:48 30%\n",
      "02:26:20 40%\n",
      "02:26:52 50%\n",
      "02:27:23 60%\n",
      "02:27:55 70%\n",
      "02:28:27 80%\n",
      "02:28:59 90%\n",
      "02:29:31 100%\n",
      "02:29:31 Making new dataframe.\n",
      "02:29:31 Done. Dataframe with 3.156 events.\n"
     ]
    }
   ],
   "source": [
    "# EXTERNAL FUNCTION: mergeCloseEvents(rowsList, max_event_duration, delimiter)\n",
    "\n",
    "allEventsRowsList = []\n",
    "eventsWithSpecificHashtagRowList = []\n",
    "\n",
    "# Print values\n",
    "count = 0\n",
    "printingValue = int(len(dictionary) / 10)\n",
    "\n",
    "pr('Starting to make event df with {} dataframes. (around 6 min)'.format(len(dictionary)))\n",
    "for h, df in dictionary.items():\n",
    "    # Print information\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "    \n",
    "    # Initializing the event list of rows\n",
    "    global eventsWithSpecificHashtagRowList\n",
    "    eventsWithSpecificHashtagRowList = []\n",
    "\n",
    "    # Detecting events for a hashtag\n",
    "    df.apply(applyToMakeEventDf, axis=1) \n",
    "    \n",
    "    # Merging close events if needed\n",
    "    mergedList = mergeCloseEvents(eventsWithSpecificHashtagRowList, MAX_DURATION_OF_EVENT, delimiter)\n",
    "    \n",
    "    # Adding events to the all events list\n",
    "    allEventsRowsList += mergedList\n",
    "\n",
    "pr('Making new dataframe.')\n",
    "\n",
    "new_events = pd.DataFrame(allEventsRowsList)\n",
    "new_events.set_index(['date'], inplace=True)\n",
    "\n",
    "pr('Done. Dataframe with {} events.'.format(strNb(len(new_events))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So every detected events are in a dataframe. We can observe the start of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>numberOfTweets</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-03-05</th>\n",
       "      <td>laferrari</td>\n",
       "      <td>46.233550</td>\n",
       "      <td>6.118950</td>\n",
       "      <td>8</td>\n",
       "      <td>#LaFerrari new name of Maranello's car #SIAG h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-03</th>\n",
       "      <td>lrts</td>\n",
       "      <td>46.288000</td>\n",
       "      <td>6.166440</td>\n",
       "      <td>7</td>\n",
       "      <td>Ça m'manque #LRTs_$$$_Elle a tout dit ! #LRTS_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-06-16</th>\n",
       "      <td>lrts</td>\n",
       "      <td>46.219900</td>\n",
       "      <td>6.146200</td>\n",
       "      <td>7</td>\n",
       "      <td>MDRRRRRRR :( #lrts_$$$_Merci Aurevoir. #lrts_$...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-13</th>\n",
       "      <td>lrts</td>\n",
       "      <td>46.609700</td>\n",
       "      <td>6.148460</td>\n",
       "      <td>7</td>\n",
       "      <td>J'ai toujours trouvé que c'était Zayn qui avai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-15</th>\n",
       "      <td>colorado</td>\n",
       "      <td>45.953875</td>\n",
       "      <td>9.176060</td>\n",
       "      <td>20</td>\n",
       "      <td>Guardando #colorado perché c'è bisogno di ride...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-27</th>\n",
       "      <td>colorado</td>\n",
       "      <td>45.788800</td>\n",
       "      <td>9.068445</td>\n",
       "      <td>8</td>\n",
       "      <td>#Colorado #riderenoncostaniente guardando Colo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-23</th>\n",
       "      <td>mun25000</td>\n",
       "      <td>47.235700</td>\n",
       "      <td>6.025860</td>\n",
       "      <td>17</td>\n",
       "      <td>#Mun25000 [Info] Le taux de participation offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-06-25</th>\n",
       "      <td>سويسرا</td>\n",
       "      <td>47.115900</td>\n",
       "      <td>9.256620</td>\n",
       "      <td>26</td>\n",
       "      <td>#سويسرا #ورد @ lugano http://t.co/d6It7t6P3a_$...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-13</th>\n",
       "      <td>سويسرا</td>\n",
       "      <td>46.679794</td>\n",
       "      <td>7.917126</td>\n",
       "      <td>247</td>\n",
       "      <td>#سويسرا\\n#جبال_الالب\\n#المسافرون_العرب\\n#انترل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-26</th>\n",
       "      <td>سويسرا</td>\n",
       "      <td>46.686750</td>\n",
       "      <td>7.885123</td>\n",
       "      <td>41</td>\n",
       "      <td>#سويسرا #ص #اوربا#انترلاكن #ليتربون #صباحية_سو...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              hashtag   latitude  longitude  numberOfTweets  \\\n",
       "date                                                          \n",
       "2013-03-05  laferrari  46.233550   6.118950               8   \n",
       "2013-04-03       lrts  46.288000   6.166440               7   \n",
       "2013-06-16       lrts  46.219900   6.146200               7   \n",
       "2014-07-13       lrts  46.609700   6.148460               7   \n",
       "2012-10-15   colorado  45.953875   9.176060              20   \n",
       "2015-02-27   colorado  45.788800   9.068445               8   \n",
       "2014-03-23   mun25000  47.235700   6.025860              17   \n",
       "2013-06-25     سويسرا  47.115900   9.256620              26   \n",
       "2014-08-13     سويسرا  46.679794   7.917126             247   \n",
       "2015-08-26     سويسرا  46.686750   7.885123              41   \n",
       "\n",
       "                                                         text  \n",
       "date                                                           \n",
       "2013-03-05  #LaFerrari new name of Maranello's car #SIAG h...  \n",
       "2013-04-03  Ça m'manque #LRTs_$$$_Elle a tout dit ! #LRTS_...  \n",
       "2013-06-16  MDRRRRRRR :( #lrts_$$$_Merci Aurevoir. #lrts_$...  \n",
       "2014-07-13  J'ai toujours trouvé que c'était Zayn qui avai...  \n",
       "2012-10-15  Guardando #colorado perché c'è bisogno di ride...  \n",
       "2015-02-27  #Colorado #riderenoncostaniente guardando Colo...  \n",
       "2014-03-23  #Mun25000 [Info] Le taux de participation offi...  \n",
       "2013-06-25  #سويسرا #ورد @ lugano http://t.co/d6It7t6P3a_$...  \n",
       "2014-08-13  #سويسرا\\n#جبال_الالب\\n#المسافرون_العرب\\n#انترل...  \n",
       "2015-08-26  #سويسرا #ص #اوربا#انترلاكن #ليتربون #صباحية_سو...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Events dataframe:')\n",
    "new_events.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of the original tweets for the word \"laferrari\" that was detected as an event:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numberOfTweets</th>\n",
       "      <th>longitude</th>\n",
       "      <th>userId</th>\n",
       "      <th>latitude</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>text</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-03-05</th>\n",
       "      <td>8</td>\n",
       "      <td>6.118950</td>\n",
       "      <td>7</td>\n",
       "      <td>46.23355</td>\n",
       "      <td>laferrari</td>\n",
       "      <td>#LaFerrari new name of Maranello's car #SIAG h...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-06</th>\n",
       "      <td>4</td>\n",
       "      <td>6.114180</td>\n",
       "      <td>2</td>\n",
       "      <td>46.23400</td>\n",
       "      <td>laferrari</td>\n",
       "      <td>#Ferrari #LaFerrari bel nome! #genevamotorshow...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-07</th>\n",
       "      <td>1</td>\n",
       "      <td>6.108770</td>\n",
       "      <td>1</td>\n",
       "      <td>46.23160</td>\n",
       "      <td>laferrari</td>\n",
       "      <td>#GenevaMotorShow, in this pic you can almost m...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10</th>\n",
       "      <td>2</td>\n",
       "      <td>6.116535</td>\n",
       "      <td>2</td>\n",
       "      <td>46.23410</td>\n",
       "      <td>laferrari</td>\n",
       "      <td>Scarsa #LaFerrari #FerrariGeneva2013 #Siag @ 2...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-11</th>\n",
       "      <td>1</td>\n",
       "      <td>6.118610</td>\n",
       "      <td>1</td>\n",
       "      <td>46.23420</td>\n",
       "      <td>laferrari</td>\n",
       "      <td>#salongeneve#ferrari#laferrari#italia @ Geneva...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-12</th>\n",
       "      <td>1</td>\n",
       "      <td>6.119670</td>\n",
       "      <td>1</td>\n",
       "      <td>46.23370</td>\n",
       "      <td>laferrari</td>\n",
       "      <td>A Genève!! Avui toca Saló internacional de l'A...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-13</th>\n",
       "      <td>2</td>\n",
       "      <td>6.117995</td>\n",
       "      <td>2</td>\n",
       "      <td>46.23155</td>\n",
       "      <td>laferrari</td>\n",
       "      <td>@Tuittolo @kiara969 questa si che è meglio di ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-15</th>\n",
       "      <td>1</td>\n",
       "      <td>6.137330</td>\n",
       "      <td>1</td>\n",
       "      <td>46.20300</td>\n",
       "      <td>laferrari</td>\n",
       "      <td>Soddisfazioni #Motorshow #Geneva\\n#LaFerrari @...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-16</th>\n",
       "      <td>1</td>\n",
       "      <td>6.118580</td>\n",
       "      <td>1</td>\n",
       "      <td>46.23450</td>\n",
       "      <td>laferrari</td>\n",
       "      <td>La più bella #Ferrari dopo la #250 ! #bellimai...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-18</th>\n",
       "      <td>1</td>\n",
       "      <td>6.093810</td>\n",
       "      <td>1</td>\n",
       "      <td>46.22040</td>\n",
       "      <td>laferrari</td>\n",
       "      <td>The #LaFerrari F70 http://t.co/FeuVizUF3Y #Fer...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            numberOfTweets  longitude  userId  latitude    hashtag  \\\n",
       "2013-03-05               8   6.118950       7  46.23355  laferrari   \n",
       "2013-03-06               4   6.114180       2  46.23400  laferrari   \n",
       "2013-03-07               1   6.108770       1  46.23160  laferrari   \n",
       "2013-03-10               2   6.116535       2  46.23410  laferrari   \n",
       "2013-03-11               1   6.118610       1  46.23420  laferrari   \n",
       "2013-03-12               1   6.119670       1  46.23370  laferrari   \n",
       "2013-03-13               2   6.117995       2  46.23155  laferrari   \n",
       "2013-03-15               1   6.137330       1  46.20300  laferrari   \n",
       "2013-03-16               1   6.118580       1  46.23450  laferrari   \n",
       "2013-03-18               1   6.093810       1  46.22040  laferrari   \n",
       "\n",
       "                                                         text  event  \n",
       "2013-03-05  #LaFerrari new name of Maranello's car #SIAG h...   True  \n",
       "2013-03-06  #Ferrari #LaFerrari bel nome! #genevamotorshow...  False  \n",
       "2013-03-07  #GenevaMotorShow, in this pic you can almost m...  False  \n",
       "2013-03-10  Scarsa #LaFerrari #FerrariGeneva2013 #Siag @ 2...  False  \n",
       "2013-03-11  #salongeneve#ferrari#laferrari#italia @ Geneva...  False  \n",
       "2013-03-12  A Genève!! Avui toca Saló internacional de l'A...  False  \n",
       "2013-03-13  @Tuittolo @kiara969 questa si che è meglio di ...  False  \n",
       "2013-03-15  Soddisfazioni #Motorshow #Geneva\\n#LaFerrari @...  False  \n",
       "2013-03-16  La più bella #Ferrari dopo la #250 ! #bellimai...  False  \n",
       "2013-03-18  The #LaFerrari F70 http://t.co/FeuVizUF3Y #Fer...  False  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Example of the original tweets for the word \"{}\" that was detected as an event:'.format(new_events.iloc[0].hashtag))\n",
    "dictionary[new_events.iloc[0].hashtag].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3156 events to export.\n"
     ]
    }
   ],
   "source": [
    "total_number_of_events = len(new_events)\n",
    "print('There are {} events to export.'.format(total_number_of_events))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this project, we worked with another team which was providing a visualisation for the event we detected. Therefore, we decided together to export all these information in a JSON file that we could give to them. The JSON format was the following:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    "    \"events\": // This contains all the events\n",
    "        [\n",
    "            {  // This object contains all the events that happened in the month \"date\"\n",
    "             \"date\": 1456790400000, // The month that group every event from the month\n",
    "             \"data\": [   // This list contains all the events of the month just above\n",
    "                   {     // This object is one specific example event\n",
    "                     \"name\": \"example\",\n",
    "                     \"date\": 1458342000000,  // This date is the real, precise date of the event\n",
    "                     \"latitude\": 46.8095953,\n",
    "                     \"longitude\": 7.1032042,\n",
    "                     \"tweets\": [\"Great sudden example #example\", \"#example #somethingelse\"],\n",
    "                     \"number_of_tweets\": 2\n",
    "                   },\n",
    "                   {\n",
    "                           // Another event of the same month\n",
    "                   }\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "             \"date\": // Another month...\n",
    "             \"data\": [ ... ] // List of events that happened this other month\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are first going to create a column with the month and the unix time for all the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:29:31 Converting dates.\n",
      "02:29:32 Done.\n"
     ]
    }
   ],
   "source": [
    "# EXTERNAL FUNCTIONS: convert_to_unix_time(record), to_utc(date)\n",
    "\n",
    "pr('Converting dates.')\n",
    "e_df = new_events.copy()\n",
    "e_df['date'] = e_df.index\n",
    "e_df.index = [i for i in range (len(e_df))]\n",
    "e_df['year'] = e_df['date'].apply(lambda x: x.year)\n",
    "e_df['month'] = e_df['date'].apply(lambda x: x.month)\n",
    "e_df['utc_date'] = e_df['date'].apply(lambda x: to_utc(x))\n",
    "e_df['unix_time'] = e_df.apply(convert_to_unix_time, axis=1)\n",
    "pr('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dictionary with the same structure as the json that will be exported. This will allow to make the json generation very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grouping events by months\n",
    "e_gb_month = e_df.groupby(e_df.unix_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:29:32 Generation of the dictionary for the final JSON...\n",
      "02:29:35 Dictionary has been generated.\n"
     ]
    }
   ],
   "source": [
    "pr('Generation of the dictionary for the final JSON...')\n",
    "\n",
    "months = []\n",
    "for month, df in e_gb_month:\n",
    "    days = []\n",
    "    for i in range (len(df)):\n",
    "        ht = df.iloc[i]['hashtag']\n",
    "        lat = df.iloc[i]['latitude']\n",
    "        lon = df.iloc[i]['longitude']\n",
    "        t_num = df.iloc[i]['numberOfTweets']\n",
    "        tweets = df.iloc[i]['text'].split(delimiter) # We split the tweet text\n",
    "        date = df.iloc[i]['utc_date']\n",
    "        \n",
    "        data_unit = { 'name': ht\n",
    "                    , 'latitude' : lat\n",
    "                    , 'longitude' : lon\n",
    "                    , 'tweets' : tweets\n",
    "                    , 'number_of_tweets' : str(t_num)\n",
    "                    , 'date' : int(date)}\n",
    "        days.append(data_unit)\n",
    "    \n",
    "    curr_month = {'date': int(month), 'data' : days}\n",
    "    months.append(curr_month)\n",
    "\n",
    "final_events = {'events' : months}\n",
    "pr('Dictionary has been generated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the final JSON from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:29:35 Exporting to json...\n",
      "02:29:36 Export done. File \"export_twitter_events_2017-02-05_02h29min35_3156_events.json\" has been created.\n"
     ]
    }
   ],
   "source": [
    "# Filename with the date\n",
    "exportFilename = 'export_twitter_events_' + datetime.now().strftime(\"%Y-%m-%d_%Hh%Mmin%S\") + \\\n",
    "'_' + str(total_number_of_events)+ '_events.json'\n",
    "exportPath =  os.path.join('export', exportFilename)\n",
    "\n",
    "pr('Exporting to json...')\n",
    "with open(exportPath, 'w') as f:\n",
    "     json.dump(final_events, f)\n",
    "pr('Export done. File \"{}\" has been created.'.format(exportFilename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were very satisfied with the number of event detected. German, Italian and French tweets were correctly located into the different part of Switzerland. Big events like sport games and local events like the white dinner in Basel were correctly detected. The model was, as well, able to detect smaller events, like conferences at the EPFL.\n",
    "\n",
    "The visualization provided by the other team was great and allowed to understand a lot better our results.\n",
    "\n",
    "Overall, this was a very interesting project to work on with an amazing dataset and we are happy to had had this opportunity."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ADA-kernel]",
   "language": "python",
   "name": "conda-env-ADA-kernel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
