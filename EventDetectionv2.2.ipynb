{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event detection in Twitter dataset\n",
    "The goal of this project is to detect past events in Switzerland, using a dataset of tweets. This dataset contain 28 million tweets coming for most of them from Switzerland. This notebook will get you through our project, explaining our methodology. It is splitted in 5 parts:\n",
    "\n",
    "1. **Data cleaning and hashtags extraction**<br>\n",
    "We split the tweets\n",
    "2. **Event detection**<br>\n",
    "\n",
    "3. **Improving quality of detected event**<br>\n",
    "\n",
    "4. **Exporting data**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import math\n",
    "from collections import Counter\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "import copy\n",
    "import json\n",
    "from datetime import timedelta\n",
    "from operator import itemgetter\n",
    "from helpFunctions import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of check point (a print plus the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def now():\n",
    "    return str(datetime.now().time())[:8]\n",
    "def pr(strToPrint):\n",
    "    print(now() + ' '+ strToPrint)\n",
    "    \n",
    "def strNb(nb):\n",
    "    return '{0:,}'.format(nb).replace(',', '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_filename = os.path.join('data','head_100k_pickle.pkl')\n",
    "tw = pd.read_pickle(pickle_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:44:10 Starting to read file... (3 min)\n",
      "19:44:10 File is loaded!\n"
     ]
    }
   ],
   "source": [
    "columns_header = ['id', 'userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId',\n",
    "                  'inReplyTo', 'source', 'truncated', 'placeLatitude', 'placeLongitude', 'sourceName', 'sourceUrl',\n",
    "                 'userName', 'screenName', 'followersCount', 'friendsCount', 'statusesCount',\n",
    "                 'userLocation']\n",
    "\n",
    "filename = os.path.join('data', 'sample.tsv')#'twex.tsv')\n",
    "pr('Starting to read file... (3 min)')\n",
    "# tw = pd.read_csv(filename, sep='\\t', encoding='utf-8', escapechar='\\\\', names=columns_header,\n",
    "#                       quoting=csv.QUOTE_NONE, na_values='N', header=None)\n",
    "\n",
    "pr('File is loaded!')\n",
    "# Audio(url=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 1.000.000 tweets.\n"
     ]
    }
   ],
   "source": [
    "print('The dataset contains {} tweets.'.format(strNb(len(tw))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>placeLatitude</th>\n",
       "      <th>placeLongitude</th>\n",
       "      <th>sourceName</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>userName</th>\n",
       "      <th>screenName</th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>userLocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9514097914</td>\n",
       "      <td>17341045.0</td>\n",
       "      <td>2010-02-23 05:55:51</td>\n",
       "      <td>Guuuuten Morgen! :-)</td>\n",
       "      <td>7.43926</td>\n",
       "      <td>46.9489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>197.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TwitBird</td>\n",
       "      <td>http://www.nibirutech.com</td>\n",
       "      <td>Tilman Jentzsch</td>\n",
       "      <td>blickwechsel</td>\n",
       "      <td>586.0</td>\n",
       "      <td>508.0</td>\n",
       "      <td>9016.0</td>\n",
       "      <td>Bern, Switzerland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      userId            createdAt                  text  \\\n",
       "0  9514097914  17341045.0  2010-02-23 05:55:51  Guuuuten Morgen! :-)   \n",
       "1         NaN         NaN                  NaN              TwitBird   \n",
       "\n",
       "                   longitude         latitude       placeId  inReplyTo  \\\n",
       "0                    7.43926          46.9489           NaN        NaN   \n",
       "1  http://www.nibirutech.com  Tilman Jentzsch  blickwechsel      586.0   \n",
       "\n",
       "   source  truncated      placeLatitude  placeLongitude  sourceName  \\\n",
       "0   197.0        NaN                NaN             NaN         NaN   \n",
       "1   508.0     9016.0  Bern, Switzerland             NaN         NaN   \n",
       "\n",
       "   sourceUrl  userName  screenName  followersCount  friendsCount  \\\n",
       "0        NaN       NaN         NaN             NaN           NaN   \n",
       "1        NaN       NaN         NaN             NaN           NaN   \n",
       "\n",
       "   statusesCount  userLocation  \n",
       "0            NaN           NaN  \n",
       "1            NaN           NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('First rows of dataset:')\n",
    "tw.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data cleaning and extracting hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def extract_hashtags(text):\n",
    "#     ht_list = re.findall(r\"#(\\w+)\", text)\n",
    "#     non_empty_hts = list(filter((lambda ht: ht != []), ht_list))\n",
    "#     lowerCharList = [ht.lower() for ht in non_empty_hts]\n",
    "#     return lowerCharList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to extract all the hashtags of the \"text\" cell in each tweet and put them in a new column (in the form of a list of hashtags per tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:44:10 Extracting hashtags... (2 min)\n",
      "19:44:18 We have found 79.200 rows with hashtags.\n"
     ]
    }
   ],
   "source": [
    "pr('Extracting hashtags... (2 min)')\n",
    "tw['hashtag'] = tw.text.apply(lambda x: extract_hashtags(str(x))) # Getting list of hashtag into new column\n",
    "twh = tw.ix[tw.hashtag.apply(lambda x: len(x) != 0)] # droping the rows (tweets) that contains no hashtags.\n",
    "pr('We have found {} rows with hashtags.'.format(strNb(len(twh))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of tweets (with only text and hashtag column):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Magic spells run off after midnight, I guess s...</td>\n",
       "      <td>[fb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Limitas of public transportation! No taxi, rai...</td>\n",
       "      <td>[yam]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text hashtag\n",
       "16  Magic spells run off after midnight, I guess s...    [fb]\n",
       "20  Limitas of public transportation! No taxi, rai...   [yam]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Examples of tweets (with only text and hashtag column):')\n",
    "twh[['text', 'hashtag']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data and making date index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop tweet which not contains values for text or createdAt as this is mandatory information to privide to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data have been reduced from 79200 tweets to 79200 tweets.\n"
     ]
    }
   ],
   "source": [
    "tw1 = twh.dropna(axis=0, how='any', subset=['text', 'createdAt'])\n",
    "print('The data have been reduced from {} tweets to {} tweets.'.format(len(twh), len(tw1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:44:18 Removing bad dates...\n",
      "19:44:18 Finished.\n"
     ]
    }
   ],
   "source": [
    "pr('Removing bad dates...')\n",
    "twhCleanDate = tw1[tw1['createdAt'].str.len() == 19]\n",
    "pr('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:44:18 Starting to examine dates...\n",
      "19:44:18 There are 0 dates that cannot be transformed.\n"
     ]
    }
   ],
   "source": [
    "pr('Starting to examine dates...')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "datetime_serie = twhCleanDate['createdAt'].convert_objects(convert_dates='coerce')\n",
    "dateNotConvertible = datetime_serie[pd.isnull(datetime_serie)]\n",
    "warnings.filterwarnings('default')\n",
    "pr('There are {} dates that cannot be transformed.'.format(len(dateNotConvertible)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:44:18 Starting copy...\n",
      "19:44:18 Converting to datetime...\n",
      "19:44:18 Setting up new indices...\n",
      "19:44:18 Deleting old \"createdAt\" column...\n",
      "19:44:18 Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>placeLatitude</th>\n",
       "      <th>placeLongitude</th>\n",
       "      <th>sourceName</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>userName</th>\n",
       "      <th>screenName</th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>userLocation</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>createdAt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-02-23 09:59:41</th>\n",
       "      <td>9519737890</td>\n",
       "      <td>14657884.0</td>\n",
       "      <td>Magic spells run off after midnight, I guess s...</td>\n",
       "      <td>6.1387</td>\n",
       "      <td>46.175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-23 11:28:27</th>\n",
       "      <td>9521789689</td>\n",
       "      <td>9962022.0</td>\n",
       "      <td>Limitas of public transportation! No taxi, rai...</td>\n",
       "      <td>6.33641</td>\n",
       "      <td>46.4631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[yam]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id      userId  \\\n",
       "createdAt                                     \n",
       "2010-02-23 09:59:41  9519737890  14657884.0   \n",
       "2010-02-23 11:28:27  9521789689   9962022.0   \n",
       "\n",
       "                                                                  text  \\\n",
       "createdAt                                                                \n",
       "2010-02-23 09:59:41  Magic spells run off after midnight, I guess s...   \n",
       "2010-02-23 11:28:27  Limitas of public transportation! No taxi, rai...   \n",
       "\n",
       "                    longitude latitude placeId  inReplyTo  source  truncated  \\\n",
       "createdAt                                                                      \n",
       "2010-02-23 09:59:41    6.1387   46.175     NaN        NaN     1.0        NaN   \n",
       "2010-02-23 11:28:27   6.33641  46.4631     NaN        NaN   550.0        NaN   \n",
       "\n",
       "                    placeLatitude  placeLongitude  sourceName  sourceUrl  \\\n",
       "createdAt                                                                  \n",
       "2010-02-23 09:59:41           NaN             NaN         NaN        NaN   \n",
       "2010-02-23 11:28:27           NaN             NaN         NaN        NaN   \n",
       "\n",
       "                     userName  screenName  followersCount  friendsCount  \\\n",
       "createdAt                                                                 \n",
       "2010-02-23 09:59:41       NaN         NaN             NaN           NaN   \n",
       "2010-02-23 11:28:27       NaN         NaN             NaN           NaN   \n",
       "\n",
       "                     statusesCount  userLocation hashtag  \n",
       "createdAt                                                 \n",
       "2010-02-23 09:59:41            NaN           NaN    [fb]  \n",
       "2010-02-23 11:28:27            NaN           NaN   [yam]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr('Starting copy...') # (to avoid transformation problems)\n",
    "tw5 = twhCleanDate.copy()\n",
    "pr('Converting to datetime...')\n",
    "tw5['createdAt'] = pd.to_datetime(twhCleanDate['createdAt'])\n",
    "pr('Setting up new indices...')\n",
    "tw5.index = tw5['createdAt']\n",
    "pr('Deleting old \"createdAt\" column...')\n",
    "del tw5['createdAt']\n",
    "pr('Done!')\n",
    "tw5.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createdAt\n",
       "2010-02-23 09:59:41               [fb]\n",
       "2010-02-23 11:28:27              [yam]\n",
       "2010-02-23 17:47:11          [24, vfb]\n",
       "2010-02-23 18:19:03    [iphoneography]\n",
       "2010-02-23 18:31:46     [partnermonth]\n",
       "2010-02-24 06:09:23      [insider, fb]\n",
       "Name: hashtag, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw5['hashtag'][:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put one hashtag per row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will make a dataframe with one row = one hashtag. This will be done by going through the dataframe, and making in parallel a list of rows (with 1 hashtag per row) that needs to be added to the old dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addedHashtagsRowsList = []\n",
    "def multiplyHashtagRows(row, columns):\n",
    "    '''\n",
    "    Examine each row. If there are multiple hashtags, it will return the first one.\n",
    "    (so the first one will replace the list of hashtags in the df). Then for all the next ones,\n",
    "    it will make a copy of the row in the addedHashtagsRowsList, (in a dictionary format).\n",
    "    So this dictionary can in the end be transformed in a DF and added to the original DF.\n",
    "    (The speed is increased a lot by doing it this way!)\n",
    "    '''\n",
    "    htList = row.hashtag\n",
    "    if len(htList) > 1:\n",
    "        ## Making the dictionary\n",
    "        addedHashtag = {}\n",
    "        addedHashtag['createdAt'] = row.name #the df index\n",
    "        for col in columns:\n",
    "            addedHashtag[col] = row[col]\n",
    "        ## Copying the dict for each hashtag\n",
    "        i = 1\n",
    "        while i < len(htList) :\n",
    "            deepCopy = copy.deepcopy(addedHashtag)\n",
    "            deepCopy['hashtag'] = htList[i]\n",
    "            addedHashtagsRowsList.append(deepCopy)\n",
    "            i+=1\n",
    "    return htList[0] # return the first hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:44:19 Multiplying the hashtag rows... (around 10 min)\n",
      "19:44:32 Finished! 40780 rows will be added to the dataframe!\n"
     ]
    }
   ],
   "source": [
    "addedHashtagsRowsList = []\n",
    "tw5_1 = tw5.copy()\n",
    "pr('Multiplying the hashtag rows... (around 10 min)')\n",
    "tw5_1['hashtag'] = tw5.apply(multiplyHashtagRows, args=[tw5.columns,], axis=1)\n",
    "pr('Finished! {} rows will be added to the dataframe!'.format(len(addedHashtagsRowsList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:44:32 Starting to make the new dataframe with additionnal rows..\n",
      "19:44:32 Starting to append the two df... Old df size = 79162\n",
      "19:44:32 Done! New df size = 119942\n"
     ]
    }
   ],
   "source": [
    "pr('Starting to make the new dataframe with additionnal rows..')\n",
    "addedHashtagsDf = pd.DataFrame(addedHashtagsRowsList)\n",
    "addedHashtagsDf.set_index(['createdAt'], inplace=True)\n",
    "pr('Starting to append the two df... Old df size = {}'.format(len(tw5_1)))\n",
    "tw6 = tw5_1.append(addedHashtagsDf)\n",
    "pr('Done! New df size = {}'.format(len(tw6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example hahshtag:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>placeLatitude</th>\n",
       "      <th>placeLongitude</th>\n",
       "      <th>screenName</th>\n",
       "      <th>source</th>\n",
       "      <th>sourceName</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>userId</th>\n",
       "      <th>userLocation</th>\n",
       "      <th>userName</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>createdAt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-07-24 16:03:13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vfb</td>\n",
       "      <td>95162125584039936</td>\n",
       "      <td>9.501397e+16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e401fb8eb4e7595a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@mikstweed Wie lange ist es her, dass der #vfb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>921241.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-23 17:47:11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vfb</td>\n",
       "      <td>9535390586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.6463</td>\n",
       "      <td>9.1657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So, Feierabend. Jetzt #24 und später #VfB. — a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>921241.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-01 17:25:56</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vfb</td>\n",
       "      <td>10021795398684672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.9499</td>\n",
       "      <td>7.47071</td>\n",
       "      <td>e38a1a641d02f8db</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So, und jetzt Flachmann suchen und dann ab ins...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120433.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     followersCount  friendsCount hashtag                 id  \\\n",
       "createdAt                                                                      \n",
       "2011-07-24 16:03:13             NaN           NaN     vfb  95162125584039936   \n",
       "2010-02-23 17:47:11             NaN           NaN     vfb         9535390586   \n",
       "2010-12-01 17:25:56             NaN           NaN     vfb  10021795398684672   \n",
       "\n",
       "                        inReplyTo latitude longitude           placeId  \\\n",
       "createdAt                                                                \n",
       "2011-07-24 16:03:13  9.501397e+16      NaN       NaN  e401fb8eb4e7595a   \n",
       "2010-02-23 17:47:11           NaN  47.6463    9.1657               NaN   \n",
       "2010-12-01 17:25:56           NaN  46.9499   7.47071  e38a1a641d02f8db   \n",
       "\n",
       "                    placeLatitude  placeLongitude  screenName  source  \\\n",
       "createdAt                                                               \n",
       "2011-07-24 16:03:13           NaN             NaN         NaN    14.0   \n",
       "2010-02-23 17:47:11           NaN             NaN         NaN   550.0   \n",
       "2010-12-01 17:25:56           NaN             NaN         NaN     1.0   \n",
       "\n",
       "                     sourceName  sourceUrl  statusesCount  \\\n",
       "createdAt                                                   \n",
       "2011-07-24 16:03:13         NaN        NaN            NaN   \n",
       "2010-02-23 17:47:11         NaN        NaN            NaN   \n",
       "2010-12-01 17:25:56         NaN        NaN            NaN   \n",
       "\n",
       "                                                                  text  \\\n",
       "createdAt                                                                \n",
       "2011-07-24 16:03:13  @mikstweed Wie lange ist es her, dass der #vfb...   \n",
       "2010-02-23 17:47:11  So, Feierabend. Jetzt #24 und später #VfB. — a...   \n",
       "2010-12-01 17:25:56  So, und jetzt Flachmann suchen und dann ab ins...   \n",
       "\n",
       "                     truncated    userId  userLocation  userName  \n",
       "createdAt                                                         \n",
       "2011-07-24 16:03:13        NaN  921241.0           NaN       NaN  \n",
       "2010-02-23 17:47:11        NaN  921241.0           NaN       NaN  \n",
       "2010-12-01 17:25:56        NaN  120433.0           NaN       NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Example hahshtag:')\n",
    "tw6[tw6['hashtag'] == addedHashtagsRowsList[0]['hashtag']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createdAt\n",
       "2010-02-23 09:59:41               fb\n",
       "2010-02-23 11:28:27              yam\n",
       "2010-02-23 17:47:11               24\n",
       "2010-02-23 18:19:03    iphoneography\n",
       "2010-02-23 18:31:46     partnermonth\n",
       "Name: hashtag, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw6.hashtag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping per hashtags per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to provide visualisation to the other team, we needed to give ocation information to all tweets. So we did not process tweets without longitude or latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw6.dropna(subset=['longitude'], inplace=True)\n",
    "tw6.dropna(subset=['latitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw6.latitude = tw6.latitude.apply(float)\n",
    "tw6.longitude = tw6.longitude.apply(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging information of all tweet containing a particular hastag turned out to be really useful to detect an event. For column of String type, we used a join with a delimiter as balow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delimiter = '_$$$_'\n",
    "str_join = lambda x: delimiter.join(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that applies to a dataframe will group each row by day and aggregate all its content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggDate(df):\n",
    "    temp = df.groupby(df.index.map(lambda x: x.date))\n",
    "    groupedDf = temp.agg({  'text' : str_join,\n",
    "                            'longitude' : np.median,\n",
    "                            'latitude' : np.median,\n",
    "                            'hashtag' : lambda x: x.iloc[0], ## the first occurence\n",
    "                            'numberOfTweets' : 'count',\n",
    "                            'userId' : pd.Series.nunique})\n",
    "    # rename userId column to a more representative name\n",
    "    return groupedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:44:32 Making column number of tweets\n",
      "19:44:32 Starting group by hastag...\n",
      "19:44:32 Starting to put hashtag in dictionary... (around 50 min)\n",
      "19:44:44 10%\n",
      "19:44:55 20%\n",
      "19:45:07 30%\n",
      "19:45:18 40%\n",
      "19:45:29 50%\n",
      "19:45:40 60%\n",
      "19:45:50 70%\n",
      "19:46:01 80%\n",
      "19:46:12 90%\n",
      "19:46:23 100%\n",
      "19:46:23 Finished operations! Dictionary with 27040 different hashtags.\n"
     ]
    }
   ],
   "source": [
    "pr('Making column number of tweets')\n",
    "tw6['numberOfTweets'] = 1\n",
    "pr('Starting group by hastag...')\n",
    "gp = tw6.groupby('hashtag')\n",
    "pr('Starting to put hashtag in dictionary... (around 50 min)')\n",
    "\n",
    "count = 0\n",
    "lengp = len(gp)\n",
    "printingValue = int(lengp / 10)\n",
    "dictionary = {}\n",
    "for hashtag, df in gp:\n",
    "    dictionary[hashtag] = aggDate(df)\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/lengp*100))\n",
    "pr('Finished operations! Dictionary with {} different hashtags.'.format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary with hashtags dataframes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numberOfTweets</th>\n",
       "      <th>longitude</th>\n",
       "      <th>text</th>\n",
       "      <th>userId</th>\n",
       "      <th>latitude</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-03-29</th>\n",
       "      <td>1</td>\n",
       "      <td>7.77983</td>\n",
       "      <td>gibts eigentlich auch richtige #news auf @JOIZ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.2102</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-27</th>\n",
       "      <td>1</td>\n",
       "      <td>6.64852</td>\n",
       "      <td>#le12h30 vu de la #régie. #rsr #rts #radio #jo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.5335</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-24</th>\n",
       "      <td>1</td>\n",
       "      <td>9.07426</td>\n",
       "      <td>Si comincia! #percompiereilmiodovere #como #ne...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.8138</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-09</th>\n",
       "      <td>1</td>\n",
       "      <td>7.35289</td>\n",
       "      <td>\"@Gazzetta_it: \"Italiani? Sempre più in sovrap...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.7567</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            numberOfTweets  longitude  \\\n",
       "2011-03-29               1    7.77983   \n",
       "2012-01-27               1    6.64852   \n",
       "2012-04-24               1    9.07426   \n",
       "2012-05-09               1    7.35289   \n",
       "\n",
       "                                                         text  userId  \\\n",
       "2011-03-29  gibts eigentlich auch richtige #news auf @JOIZ...     1.0   \n",
       "2012-01-27  #le12h30 vu de la #régie. #rsr #rts #radio #jo...     1.0   \n",
       "2012-04-24  Si comincia! #percompiereilmiodovere #como #ne...     1.0   \n",
       "2012-05-09  \"@Gazzetta_it: \"Italiani? Sempre più in sovrap...     1.0   \n",
       "\n",
       "            latitude hashtag  \n",
       "2011-03-29   47.2102    news  \n",
       "2012-01-27   46.5335    news  \n",
       "2012-04-24   45.8138    news  \n",
       "2012-05-09   47.7567    news  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creation of a dictionnary {'hashtag' : 'dataframe containing all tweet with the hashtag'}\n",
    "print('Dictionary with hashtags dataframes:')\n",
    "dictionary[list(dictionary.keys())[5]].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event detection (with elimination of recurrent events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that define events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Parameters of an event:\n",
    "MIN_TOT_NB_TWEETS = 20 ## The hashtag must have happened at least this number of times to be considered.\n",
    "MIN_NB_DAYS_WITH_HASHTAGS = 3 ## The hashtags must appear at least this number of different days to be considered.\n",
    "MIN_NB_TWEETS_DURING_EVENT = 7 ## To be considered an event, the hashtag must happen at least this nb of times during the day.\n",
    "THRESHOLD_ANOMALY_FACTOR = 2.5 ## The occurence of a hashtag during a single day must be above the mean by this FACTOR\n",
    "                             ## multiplied by the std to be considered as an anomaly.\n",
    "MAX_DURATION_OF_EVENT = timedelta(days=30) ## The maximum number of days we consider an event can happen\n",
    "MIN_DURATION_BEFORE_NEW_EVENT = timedelta(days=304) ## (= 10 months) The min time that should pass before an event can happen\n",
    "                                                    ## again and still be considered as event (ie. Christmas is an event\n",
    "                                                    ## each year)\n",
    "MIN_NUMBER_DIFFERENT_USER = 2 # To state that an event occured, a minimum number of different users should have tweeted about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to detect recurrent events that should be removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def isSpecificEventListIllegal(detectedEventDateList):\n",
    "#     '''\n",
    "#     Return true if the list of dates contain illegal tupples of events, so if the event is recurrent\n",
    "#     which would mean it is not a real event.\n",
    "#     '''\n",
    "#     def datesAreIllegal(date1, date2, date3):\n",
    "#         '''\n",
    "#         Return true if the 3 dates are not to be considered as regular events.\n",
    "#         '''\n",
    "#         ## Return if the difference is too small to be considered as 2 different events\n",
    "#         def diffIsSmall(timeDiff):  \n",
    "#             return timeDiff < MAX_DURATION_OF_EVENT\n",
    "\n",
    "#         ## Return true if the difference is not big enough to be an annual event.\n",
    "#         def isDiffSuspect(timeDiff):\n",
    "#             return timeDiff < MIN_DURATION_BEFORE_NEW_EVENT   \n",
    "\n",
    "#         diff1 = abs(date1 - date2)\n",
    "#         diff2 = abs(date2 - date3)\n",
    "#         diff3 = abs(date3 - date1)\n",
    "\n",
    "#         ## The difference is too small, it must be the same event\n",
    "#         if diffIsSmall(diff1) or diffIsSmall(diff2) or diffIsSmall(diff3):\n",
    "#             return False\n",
    "\n",
    "#         ## If there are at least 2 out of 3 suspect difference, then the dates are illegal\n",
    "#         if isDiffSuspect(diff1):\n",
    "#             return isDiffSuspect(diff2) or isDiffSuspect(diff3)\n",
    "#         else:\n",
    "#             return isDiffSuspect(diff2) and isDiffSuspect(diff3)\n",
    "    \n",
    "#     ## MAIN FUNCTION : ##\n",
    "#     # Go through the list of events and try all \"triples\" to see if there is any illegal triples. This is a quickly done\n",
    "#     # code to do that. Code complexity bellow is in O(k^3), with k being the size of the list. We will apply this function\n",
    "#     # to n list so we will have an overall complexity in O(n*k^3). We can consider however that each list will\n",
    "#     # be small so k can be considered as constant and therefore the overall complexity will be in O(n).\n",
    "#     for i in range(len(detectedEventDateList) - 2):\n",
    "#         for j in range(i, len(detectedEventDateList) - 1):\n",
    "#             for k in range(j, len(detectedEventDateList)):\n",
    "#                 if datesAreIllegal(detectedEventDateList[i], detectedEventDateList[j], detectedEventDateList[k]):\n",
    "#                     return True\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main method to detect event according to all our criterias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:46:23 Starting to compute 27040 dict items to detect event. (4 min)\n",
      "19:46:24 10%\n",
      "19:46:24 20%\n",
      "19:46:25 30%\n",
      "19:46:26 40%\n",
      "19:46:27 50%\n",
      "19:46:28 60%\n",
      "19:46:28 70%\n",
      "19:46:29 80%\n",
      "19:46:30 90%\n",
      "19:46:31 100%\n",
      "19:46:31 Finished! Number of events detected = 8\n"
     ]
    }
   ],
   "source": [
    "pr('Starting to compute {} dict items to detect event. (4 min)'.format(len(dictionary)))\n",
    "nbOfEventDetected = 0\n",
    "count = 0\n",
    "printingValue = int(len(dictionary) / 10)\n",
    "for [h,df] in dictionary.items():\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "    df['event'] = False\n",
    "    if len(df) > MIN_NB_DAYS_WITH_HASHTAGS:\n",
    "        if df['numberOfTweets'].sum() > MIN_TOT_NB_TWEETS:\n",
    "            if df['userId'][0] >= MIN_NUMBER_DIFFERENT_USER:\n",
    "                threshold = df['numberOfTweets'].mean() + THRESHOLD_ANOMALY_FACTOR * df['numberOfTweets'].std()\n",
    "                df['event'] = df.numberOfTweets.apply(lambda x: x > threshold and x > MIN_NB_TWEETS_DURING_EVENT)\n",
    "            \n",
    "            ## Remove recurrent events:\n",
    "            detectedEventDf = df[df['event']]\n",
    "            if len(detectedEventDf) > 2 and isSpecificEventListIllegal(detectedEventDf.index):\n",
    "                df['event'] = False\n",
    "            nbOfEventDetected += len(df[df['event']])\n",
    "pr('Finished! Number of events detected = {}'.format(nbOfEventDetected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pr('Starting to compute {} dict items to detect event. (4 min)'.format(len(dictionary)))\n",
    "# nbOfEventDetected = 0\n",
    "# count = 0\n",
    "# printingValue = int(len(dictionary) / 10)\n",
    "# for [h,df] in dictionary.items():\n",
    "#     count += 1\n",
    "#     if count % printingValue == 0:\n",
    "#         pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "#     df['event'] = False\n",
    "#     if len(df) > MIN_NB_DAYS_WITH_HASHTAGS:\n",
    "#         if df['numberOfTweets'].sum() > MIN_TOT_NB_TWEETS:\n",
    "#             threshold = df['numberOfTweets'].mean() + THRESHOLD_ANOMALY_FACTOR * df['numberOfTweets'].std()\n",
    "#             df['event'] = df.numberOfTweets.apply(lambda x: x >= threshold and x >= MIN_NB_TWEETS_DURING_EVENT)\n",
    "            \n",
    "#             ## Remove recurrent events:\n",
    "#             detectedEventDf = df[df['event']]\n",
    "#             if len(detectedEventDf) > 2 and isSpecificEventListIllegal(detectedEventDf.index):\n",
    "#                 df['event'] = False\n",
    "#             nbOfEventDetected += len(df[df['event']])\n",
    "# pr('Finished! Number of events detected = {}'.format(nbOfEventDetected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging close events and grouping into single event dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have a function that is going to merge events that are considered as too \"close\" to each other to be considered individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def mergeCloseEvents(rowsList):\n",
    "#     '''\n",
    "#     Take a list of dictionary, where each dictionary is a \"row\" of the event df, which contained detected events.\n",
    "#     It will process the list to detect event that are close and merge them together.\n",
    "#     Return : the processed list of event.\n",
    "#     '''\n",
    "    \n",
    "#     def areCloseEvents(event1, event2):\n",
    "#         '''\n",
    "#         Return true is 2 events dates are defined as \"close\"\n",
    "#         '''\n",
    "#         return abs(event1['date'] - event2['date']) < MAX_DURATION_OF_EVENT\n",
    "        \n",
    "#     def mergeCloseEventsSublist(closeEventList):\n",
    "#         '''\n",
    "#         This will be applied to each close event sublist. It will merge all events into one unique event.\n",
    "#         The event will consist of the total number of tweets, with the concatenation of the tweet texts and the mean\n",
    "#         of longitude/latitude. A meanDate will be defined as a ponderated mean between all dates.\n",
    "#         The final date will be the one that is in the closeEventList and is closest to this mean date.\n",
    "#         We did this to keep the meaning of the date if it had some, and not have some meaningless \"mean-date\".\n",
    "#         '''\n",
    "#         latitude = 0\n",
    "#         longitude = 0\n",
    "#         numberOfTweets = 0\n",
    "#         text = \"\"\n",
    "#         originalDate = closeEventList[0]['date']\n",
    "#         dateDiff = timedelta(days=0)\n",
    "#         first = True        \n",
    "#         for tweet in closeEventList:\n",
    "#             longitude += tweet['longitude']\n",
    "#             latitude += tweet['latitude'] \n",
    "#             numberOfTweets += tweet['numberOfTweets']\n",
    "#             if first:\n",
    "#                 text = tweet['text']\n",
    "#                 first = False\n",
    "#             else:\n",
    "#                 text += delimiter + tweet['text']\n",
    "#                 dateDiff = dateDiff + (tweet['date'] - originalDate) * tweet['numberOfTweets']\n",
    "\n",
    "#         ## It is multiplied by 2 then soustracted to round correctly to the nearest day\n",
    "#         meanDate = originalDate + 2* dateDiff / numberOfTweets - dateDiff / numberOfTweets        \n",
    "#         latitude = latitude / len(closeEventList)\n",
    "#         longitude = longitude / len(closeEventList)\n",
    "        \n",
    "#         ## We are going to detect the event the closest to the mean date\n",
    "#         minSelectedDate = closeEventList[0]['date']\n",
    "#         minDistance = abs(closeEventList[0]['date'] - meanDate)\n",
    "#         for tweet in closeEventList:\n",
    "#             if abs(tweet['date'] - meanDate) < minDistance:\n",
    "#                 minSelectedDate = tweet['date']    \n",
    "        \n",
    "#         return {'date': minSelectedDate, 'hashtag': closeEventList[0]['hashtag'], 'text': text,\n",
    "#                     'longitude': longitude, 'latitude':latitude, 'numberOfTweets': numberOfTweets, }\n",
    "    \n",
    "#     ############ -----  MAIN METHOD  ----- ############\n",
    "    \n",
    "#     ## If the list is big enough, go through the list and form an export list and merge elements that needs to.\n",
    "#     if len(rowsList) < 2:\n",
    "#         return rowsList\n",
    "#     else:\n",
    "#         firstLastPosOfItemsToMerge = []\n",
    "#         sortedRowsList = sorted(rowsList, key=itemgetter('date')) \n",
    "#         exportedEventList = []\n",
    "#         ## This goes through the *sorted* list and add the pair of indices (first indice and last indice) where events \n",
    "#         ## that should be merged appear.\n",
    "#         lastEventWasClose = False\n",
    "#         firstItem = -1\n",
    "#         for i in range(0, len(sortedRowsList)-1):\n",
    "#             if areCloseEvents(sortedRowsList[i], sortedRowsList[i+1]):\n",
    "#                 if not lastEventWasClose: # So it is the first pairs of the sublist of close events in the whole list\n",
    "#                     firstItem = i\n",
    "#                     lastEventWasClose = True\n",
    "#             else:\n",
    "#                 if lastEventWasClose: # So the list has just ended.\n",
    "#                     exportedEventList.append(mergeCloseEventsSublist(sortedRowsList[firstItem:i+1]))\n",
    "#                     lastEventWasClose = False\n",
    "#                 else: # The element is by itself, let's append it\n",
    "#                     exportedEventList.append(sortedRowsList[i])  \n",
    "#         if lastEventWasClose: # If there were events to merge till the last elem of list\n",
    "#             exportedEventList.append(mergeCloseEventsSublist(sortedRowsList[firstItem:len(sortedRowsList)]))\n",
    "#         else:\n",
    "#             exportedEventList.append(sortedRowsList[len(sortedRowsList)-1])\n",
    "    \n",
    "#     return exportedEventList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be applied to each dataframe. If a row is detected as an event, it will be added to the locaRowsList which will be used to make a general dataframe of all the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "localRowsList = []\n",
    "def applyToMakeEventDf(row):\n",
    "    if row.event:\n",
    "        rowToAdd = {'date': row.name, 'hashtag': row.hashtag, 'text': row.text,\n",
    "                    'longitude': row.longitude, 'latitude':row.latitude, 'numberOfTweets': row.numberOfTweets, }\n",
    "        global localRowsList\n",
    "        localRowsList.append(rowToAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:46:31 Starting to make event df with 27040 dataframes. (around 6 min)\n",
      "19:46:32 10%\n",
      "19:46:34 20%\n",
      "19:46:35 30%\n",
      "19:46:37 40%\n",
      "19:46:39 50%\n",
      "19:46:40 60%\n",
      "19:46:42 70%\n",
      "19:46:44 80%\n",
      "19:46:45 90%\n",
      "19:46:47 100%\n",
      "19:46:47 Making new dataframe.\n",
      "19:46:47 Finished! Dataframe with 8 rows\n"
     ]
    }
   ],
   "source": [
    "eventRowsList = []\n",
    "localRowsList = []\n",
    "count = 0\n",
    "printingValue = int(len(dictionary) / 10)\n",
    "\n",
    "pr('Starting to make event df with {} dataframes. (around 6 min)'.format(len(dictionary)))\n",
    "for h, df in dictionary.items():\n",
    "    global localRowsList\n",
    "    localRowsList = []\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "        \n",
    "    df.apply(applyToMakeEventDf, axis=1)\n",
    "    mergedList = mergeCloseEvents(localRowsList) # merging close events\n",
    "    eventRowsList += mergedList\n",
    "\n",
    "pr('Making new dataframe.')\n",
    "new_events = pd.DataFrame(eventRowsList)\n",
    "new_events.set_index(['date'], inplace=True)\n",
    "pr('Finished! Dataframe with {} rows'.format(len(new_events)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>numberOfTweets</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-04-22</th>\n",
       "      <td>mélenchon</td>\n",
       "      <td>46.52430</td>\n",
       "      <td>6.865800</td>\n",
       "      <td>64</td>\n",
       "      <td>JL #Mélenchon votera à 10:00 dans le Xème arro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-16</th>\n",
       "      <td>france2</td>\n",
       "      <td>46.37060</td>\n",
       "      <td>6.491210</td>\n",
       "      <td>9</td>\n",
       "      <td>Et trop beau chien au passage ;) #France2 #CLA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-16</th>\n",
       "      <td>sui</td>\n",
       "      <td>47.37880</td>\n",
       "      <td>8.538030</td>\n",
       "      <td>25</td>\n",
       "      <td>#sui wonderful_$$$_en plus du goal, j'aurai mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-19</th>\n",
       "      <td>lesanges4</td>\n",
       "      <td>46.15730</td>\n",
       "      <td>6.097980</td>\n",
       "      <td>10</td>\n",
       "      <td>JE VAIS MANGER DEVANT LA TELE.., EN REGARDANT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-21</th>\n",
       "      <td>placeaupeuple</td>\n",
       "      <td>46.52430</td>\n",
       "      <td>6.865810</td>\n",
       "      <td>40</td>\n",
       "      <td>@steffie1985 Dis toi que tu auras encore 2 sem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-06</th>\n",
       "      <td>forzajuve</td>\n",
       "      <td>45.83195</td>\n",
       "      <td>9.226965</td>\n",
       "      <td>16</td>\n",
       "      <td>Parola del giorno ANSIA. Per me e altri milion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-06-08</th>\n",
       "      <td>ios5</td>\n",
       "      <td>47.48500</td>\n",
       "      <td>8.297210</td>\n",
       "      <td>9</td>\n",
       "      <td>@AndreBonhote einstellungen gemacht? #ios5  ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-03-25</th>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.37510</td>\n",
       "      <td>8.538960</td>\n",
       "      <td>41</td>\n",
       "      <td>@greezer der @ThBenkoe hat's schon #iPad2   ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  hashtag  latitude  longitude  numberOfTweets  \\\n",
       "date                                                             \n",
       "2012-04-22      mélenchon  46.52430   6.865800              64   \n",
       "2012-05-16        france2  46.37060   6.491210               9   \n",
       "2010-06-16            sui  47.37880   8.538030              25   \n",
       "2012-04-19      lesanges4  46.15730   6.097980              10   \n",
       "2012-04-21  placeaupeuple  46.52430   6.865810              40   \n",
       "2012-05-06      forzajuve  45.83195   9.226965              16   \n",
       "2011-06-08           ios5  47.48500   8.297210               9   \n",
       "2011-03-25          ipad2  47.37510   8.538960              41   \n",
       "\n",
       "                                                         text  \n",
       "date                                                           \n",
       "2012-04-22  JL #Mélenchon votera à 10:00 dans le Xème arro...  \n",
       "2012-05-16  Et trop beau chien au passage ;) #France2 #CLA...  \n",
       "2010-06-16  #sui wonderful_$$$_en plus du goal, j'aurai mi...  \n",
       "2012-04-19  JE VAIS MANGER DEVANT LA TELE.., EN REGARDANT ...  \n",
       "2012-04-21  @steffie1985 Dis toi que tu auras encore 2 sem...  \n",
       "2012-05-06  Parola del giorno ANSIA. Per me e altri milion...  \n",
       "2011-06-08  @AndreBonhote einstellungen gemacht? #ios5  ht...  \n",
       "2011-03-25  @greezer der @ThBenkoe hat's schon #iPad2   ht...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Events dataframe:')\n",
    "new_events.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked dataframe of all days:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numberOfTweets</th>\n",
       "      <th>longitude</th>\n",
       "      <th>text</th>\n",
       "      <th>userId</th>\n",
       "      <th>latitude</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-04-19</th>\n",
       "      <td>17</td>\n",
       "      <td>6.865920</td>\n",
       "      <td>Ce soir #Mélenchon au #19:30. Message aux Suis...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46.52440</td>\n",
       "      <td>mélenchon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-20</th>\n",
       "      <td>54</td>\n",
       "      <td>6.865790</td>\n",
       "      <td>@didierpg26 Pardon, mais g loupé un épisode, e...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.52430</td>\n",
       "      <td>mélenchon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-21</th>\n",
       "      <td>57</td>\n",
       "      <td>6.865800</td>\n",
       "      <td>Pour les indécis, ne vous posez pas la questio...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.52430</td>\n",
       "      <td>mélenchon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-22</th>\n",
       "      <td>64</td>\n",
       "      <td>6.865800</td>\n",
       "      <td>JL #Mélenchon votera à 10:00 dans le Xème arro...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46.52430</td>\n",
       "      <td>mélenchon</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-23</th>\n",
       "      <td>43</td>\n",
       "      <td>6.865810</td>\n",
       "      <td>@pierremoscovici @RMCinfo \"#Mélenchon  nous so...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46.52430</td>\n",
       "      <td>mélenchon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-24</th>\n",
       "      <td>12</td>\n",
       "      <td>6.865810</td>\n",
       "      <td>#RadioBastille Rejoignez nous sur #RéseauFDG f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.52430</td>\n",
       "      <td>mélenchon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-25</th>\n",
       "      <td>1</td>\n",
       "      <td>6.866060</td>\n",
       "      <td>@GerlebGg62 @espritpassion Critiquer la postur...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.52420</td>\n",
       "      <td>mélenchon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-01</th>\n",
       "      <td>2</td>\n",
       "      <td>6.865765</td>\n",
       "      <td>Ce que vient de tisser aujourd'hui #Sarkozy au...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.52425</td>\n",
       "      <td>mélenchon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-02</th>\n",
       "      <td>2</td>\n",
       "      <td>6.865835</td>\n",
       "      <td>#Mélenchon sort du corps de #Hollande _$$$_RT ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.52430</td>\n",
       "      <td>mélenchon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-04</th>\n",
       "      <td>2</td>\n",
       "      <td>6.865680</td>\n",
       "      <td>#Mélenchon à #Stalingrad : sur le pupitre est ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.52430</td>\n",
       "      <td>mélenchon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            numberOfTweets  longitude  \\\n",
       "2012-04-19              17   6.865920   \n",
       "2012-04-20              54   6.865790   \n",
       "2012-04-21              57   6.865800   \n",
       "2012-04-22              64   6.865800   \n",
       "2012-04-23              43   6.865810   \n",
       "2012-04-24              12   6.865810   \n",
       "2012-04-25               1   6.866060   \n",
       "2012-05-01               2   6.865765   \n",
       "2012-05-02               2   6.865835   \n",
       "2012-05-04               2   6.865680   \n",
       "\n",
       "                                                         text  userId  \\\n",
       "2012-04-19  Ce soir #Mélenchon au #19:30. Message aux Suis...     2.0   \n",
       "2012-04-20  @didierpg26 Pardon, mais g loupé un épisode, e...     1.0   \n",
       "2012-04-21  Pour les indécis, ne vous posez pas la questio...     1.0   \n",
       "2012-04-22  JL #Mélenchon votera à 10:00 dans le Xème arro...     2.0   \n",
       "2012-04-23  @pierremoscovici @RMCinfo \"#Mélenchon  nous so...     2.0   \n",
       "2012-04-24  #RadioBastille Rejoignez nous sur #RéseauFDG f...     1.0   \n",
       "2012-04-25  @GerlebGg62 @espritpassion Critiquer la postur...     1.0   \n",
       "2012-05-01  Ce que vient de tisser aujourd'hui #Sarkozy au...     1.0   \n",
       "2012-05-02  #Mélenchon sort du corps de #Hollande _$$$_RT ...     1.0   \n",
       "2012-05-04  #Mélenchon à #Stalingrad : sur le pupitre est ...     1.0   \n",
       "\n",
       "            latitude    hashtag  event  \n",
       "2012-04-19  46.52440  mélenchon  False  \n",
       "2012-04-20  46.52430  mélenchon  False  \n",
       "2012-04-21  46.52430  mélenchon  False  \n",
       "2012-04-22  46.52430  mélenchon   True  \n",
       "2012-04-23  46.52430  mélenchon  False  \n",
       "2012-04-24  46.52430  mélenchon  False  \n",
       "2012-04-25  46.52420  mélenchon  False  \n",
       "2012-05-01  46.52425  mélenchon  False  \n",
       "2012-05-02  46.52430  mélenchon  False  \n",
       "2012-05-04  46.52430  mélenchon  False  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Linked dataframe of all days:')\n",
    "dictionary[new_events.iloc[0].hashtag].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we worked with another team, we needed a way to communicate them our detection. We used a JSON with all the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 events.\n"
     ]
    }
   ],
   "source": [
    "total_number_of_events = len(new_events)\n",
    "print('There are {} events.'.format(total_number_of_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>numberOfTweets</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mélenchon</td>\n",
       "      <td>46.5243</td>\n",
       "      <td>6.8658</td>\n",
       "      <td>64</td>\n",
       "      <td>JL #Mélenchon votera à 10:00 dans le Xème arro...</td>\n",
       "      <td>2012-04-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hashtag  latitude  longitude  numberOfTweets  \\\n",
       "0  mélenchon   46.5243     6.8658              64   \n",
       "\n",
       "                                                text        date  \n",
       "0  JL #Mélenchon votera à 10:00 dans le Xème arro...  2012-04-22  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_df = new_events.copy()\n",
    "e_df['date'] = e_df.index\n",
    "e_df.index = [i for i in range (len(e_df))]\n",
    "e_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to generate the right datetimes for the jsons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# epoch_dt = datetime(1970, 1, 1)\n",
    "# def to_utc(date):\n",
    "#     d_dt = datetime.combine(date, datetime.min.time())\n",
    "#     return int((d_dt - epoch_dt).total_seconds()*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def convert_to_unix_time(record):\n",
    "#     datetime_index = pd.DatetimeIndex([datetime(record['year'], record['month'], 1)])\n",
    "#     unix_time_index = datetime_index.astype(np.int64) // 10**6\n",
    "#     return unix_time_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:46:47 Converting dates...\n",
      "19:46:47 Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>numberOfTweets</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>utc_date</th>\n",
       "      <th>unix_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mélenchon</td>\n",
       "      <td>46.5243</td>\n",
       "      <td>6.8658</td>\n",
       "      <td>64</td>\n",
       "      <td>JL #Mélenchon votera à 10:00 dans le Xème arro...</td>\n",
       "      <td>2012-04-22</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>1335052800000</td>\n",
       "      <td>1333238400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hashtag  latitude  longitude  numberOfTweets  \\\n",
       "0  mélenchon   46.5243     6.8658              64   \n",
       "\n",
       "                                                text        date  year  month  \\\n",
       "0  JL #Mélenchon votera à 10:00 dans le Xème arro...  2012-04-22  2012      4   \n",
       "\n",
       "        utc_date      unix_time  \n",
       "0  1335052800000  1333238400000  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr('Converting dates...')\n",
    "e_df['year'] = e_df['date'].apply(lambda x: x.year)\n",
    "e_df['month'] = e_df['date'].apply(lambda x: x.month)\n",
    "e_df['utc_date'] = e_df['date'].apply(lambda x: to_utc(x))\n",
    "e_df['unix_time'] = e_df.apply(convert_to_unix_time, axis=1)\n",
    "pr('Done.')\n",
    "e_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation of a JSON is easier from a dictionary than from a dataframe. Also, the other team we worked with asked us to group events by months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grouping by months\n",
    "e_gb_month = e_df.groupby(e_df.unix_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:46:47 Making event list...\n",
      "19:46:47 Done.\n"
     ]
    }
   ],
   "source": [
    "# Generation of the dictionary for the final JSON\n",
    "pr('Making event list...')\n",
    "months = []\n",
    "for month, df in e_gb_month:\n",
    "    days = []\n",
    "    for i in range (len(df)):\n",
    "        ht = df.iloc[i]['hashtag']\n",
    "        lat = df.iloc[i]['latitude']\n",
    "        lon = df.iloc[i]['longitude']\n",
    "        t_num = df.iloc[i]['numberOfTweets']\n",
    "        tweets = df.iloc[i]['text'].split(delimiter)\n",
    "        date = df.iloc[i]['utc_date']\n",
    "        \n",
    "        data_unit = { 'name': ht\n",
    "                    , 'latitude' : lat\n",
    "                    , 'longitude' : lon\n",
    "                    , 'tweets' : tweets\n",
    "                    , 'number_of_tweets' : str(t_num)\n",
    "                    , 'date' : int(date)}\n",
    "        days.append(data_unit)\n",
    "    \n",
    "    curr_month = {'date': int(month), 'data' : days}\n",
    "    months.append(curr_month)\n",
    "\n",
    "final_events = {'events' : months}\n",
    "pr('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the final JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:46:47 Exporting to json...\n",
      "19:46:47 Export done. File \"export_twitter_events_2017-02-03_19h46min47_8_events.json\" has been created.\n"
     ]
    }
   ],
   "source": [
    "exportFilename = 'export_twitter_events_' + datetime.now().strftime(\"%Y-%m-%d_%Hh%Mmin%S\") + \\\n",
    "'_' + str(total_number_of_events)+ '_events.json'\n",
    "exportPath =  os.path.join('data', exportFilename)\n",
    "\n",
    "pr('Exporting to json...')\n",
    "with open(exportPath, 'w') as f:\n",
    "     json.dump(final_events, f)\n",
    "pr('Export done. File \"{}\" has been created.'.format(exportFilename))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
