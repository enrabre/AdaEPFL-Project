{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event detection in Twitter dataset\n",
    "The goal of this project is to detect past events in Switzerland, using a dataset of tweets. This dataset contain 28 million tweets coming for most of them from Switzerland. This notebook will get you through our project, explaining our methodology. It is splitted in 5 parts:\n",
    "\n",
    "1. **Data cleaning and hashtags extraction**<br>\n",
    "We split the tweets\n",
    "2. **Event detection**<br>\n",
    "\n",
    "3. **Improving quality of detected event**<br>\n",
    "\n",
    "4. **Exporting data**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import math\n",
    "from collections import Counter\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "import copy\n",
    "import json\n",
    "from datetime import timedelta\n",
    "from operator import itemgetter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of check point (a print plus the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def now():\n",
    "    return str(datetime.now().time())[:8]\n",
    "def pr(strToPrint):\n",
    "    print(now() + ' '+ strToPrint)\n",
    "    \n",
    "def strNb(nb):\n",
    "    return '{0:,}'.format(nb).replace(',', '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_filename = os.path.join('data','head_100k_pickle.pkl')\n",
    "tw = pd.read_pickle(pickle_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:11:10 Starting to read file... (3 min)\n",
      "19:11:10 File is loaded!\n"
     ]
    }
   ],
   "source": [
    "columns_header = ['id', 'userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId',\n",
    "                  'inReplyTo', 'source', 'truncated', 'placeLatitude', 'placeLongitude', 'sourceName', 'sourceUrl',\n",
    "                 'userName', 'screenName', 'followersCount', 'friendsCount', 'statusesCount',\n",
    "                 'userLocation']\n",
    "\n",
    "filename = os.path.join('data','twex.tsv') # 'sample.tsv')\n",
    "pr('Starting to read file... (3 min)')\n",
    "# tw = pd.read_csv(filename, sep='\\t', encoding='utf-8', escapechar='\\\\', names=columns_header,\n",
    "#                       quoting=csv.QUOTE_NONE, na_values='N', header=None)\n",
    "\n",
    "pr('File is loaded!')\n",
    "# Audio(url=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 100.000 tweets.\n"
     ]
    }
   ],
   "source": [
    "print('The dataset contains {} tweets.'.format(strNb(len(tw))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>...</th>\n",
       "      <th>placeLongitude</th>\n",
       "      <th>sourceName</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>userName</th>\n",
       "      <th>screenName</th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>userLocation</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9514097914</td>\n",
       "      <td>1.7341e+07</td>\n",
       "      <td>2010-02-23 05:55:51</td>\n",
       "      <td>Guuuuten Morgen! :-)</td>\n",
       "      <td>7.43926</td>\n",
       "      <td>46.9489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TwitBird</td>\n",
       "      <td>http://www.nibirutech.com</td>\n",
       "      <td>Tilman Jentzsch</td>\n",
       "      <td>blickwechsel</td>\n",
       "      <td>586</td>\n",
       "      <td>508.0</td>\n",
       "      <td>9016.0</td>\n",
       "      <td>Bern, Switzerland</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9514846412</td>\n",
       "      <td>7.19828e+06</td>\n",
       "      <td>2010-02-23 06:22:40</td>\n",
       "      <td>Still the best coffee in town — at La Stanza h...</td>\n",
       "      <td>8.53781</td>\n",
       "      <td>47.3678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gowalla</td>\n",
       "      <td>http://gowalla.com/</td>\n",
       "      <td>Nico Luchsinger</td>\n",
       "      <td>halbluchs</td>\n",
       "      <td>1820</td>\n",
       "      <td>703.0</td>\n",
       "      <td>4687.0</td>\n",
       "      <td>Zurich, Switzerland</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       userId            createdAt  \\\n",
       "0  9514097914   1.7341e+07  2010-02-23 05:55:51   \n",
       "1  9514846412  7.19828e+06  2010-02-23 06:22:40   \n",
       "\n",
       "                                                text  longitude latitude  \\\n",
       "0                               Guuuuten Morgen! :-)    7.43926  46.9489   \n",
       "1  Still the best coffee in town — at La Stanza h...    8.53781  47.3678   \n",
       "\n",
       "  placeId  inReplyTo source truncated   ...   placeLongitude sourceName  \\\n",
       "0     NaN        NaN    197       NaN   ...              NaN   TwitBird   \n",
       "1     NaN        NaN    550       NaN   ...              NaN    Gowalla   \n",
       "\n",
       "                   sourceUrl         userName    screenName followersCount  \\\n",
       "0  http://www.nibirutech.com  Tilman Jentzsch  blickwechsel            586   \n",
       "1        http://gowalla.com/  Nico Luchsinger     halbluchs           1820   \n",
       "\n",
       "  friendsCount  statusesCount         userLocation hashtag  \n",
       "0        508.0         9016.0    Bern, Switzerland      []  \n",
       "1        703.0         4687.0  Zurich, Switzerland      []  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('First rows of dataset:')\n",
    "tw.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data cleaning and extracting hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_hashtags(text):\n",
    "    ht_list = re.findall(r\"#(\\w+)\", text)\n",
    "    non_empty_hts = list(filter((lambda ht: ht != []), ht_list))\n",
    "    lowerCharList = [ht.lower() for ht in non_empty_hts]\n",
    "    return lowerCharList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to extract all the hashtags of the \"text\" cell in each tweet and put them in a new column (in the form of a list of hashtags per tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:23:58 Extracting hashtags... (2 min)\n",
      "19:23:59 We have found 19.719 rows with hashtags.\n"
     ]
    }
   ],
   "source": [
    "pr('Extracting hashtags... (2 min)')\n",
    "tw['hashtag'] = tw.text.apply(lambda x: extract_hashtags(str(x))) # Getting list of hashtag into new column\n",
    "twh = tw.ix[tw.hashtag.apply(lambda x: len(x) != 0)] # droping the rows (tweets) that contains no hashtags.\n",
    "pr('We have found {} rows with hashtags.'.format(strNb(len(twh))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of tweets (with only text and hashtag column):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Magic spells run off after midnight, I guess s...</td>\n",
       "      <td>[fb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Limitas of public transportation! No taxi, rai...</td>\n",
       "      <td>[yam]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text hashtag\n",
       "8   Magic spells run off after midnight, I guess s...    [fb]\n",
       "10  Limitas of public transportation! No taxi, rai...   [yam]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Examples of tweets (with only text and hashtag column):')\n",
    "twh[['text', 'hashtag']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data and making date index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop tweet which not contains values for text or createdAt as this is mandatory information to privide to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data have been reduced from 19719 tweets to 19719 tweets.\n"
     ]
    }
   ],
   "source": [
    "tw1 = twh.dropna(axis=0, how='any', subset=['text', 'createdAt'])\n",
    "print('The data have been reduced from {} tweets to {} tweets.'.format(len(twh), len(tw1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:11:11 Removing bad dates...\n",
      "19:11:11 Finished.\n"
     ]
    }
   ],
   "source": [
    "pr('Removing bad dates...')\n",
    "twhCleanDate = tw1[tw1['createdAt'].str.len() == 19]\n",
    "pr('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:11:11 Starting to examine dates...\n",
      "19:11:11 There are 0 dates that cannot be transformed.\n"
     ]
    }
   ],
   "source": [
    "pr('Starting to examine dates...')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "datetime_serie = twhCleanDate['createdAt'].convert_objects(convert_dates='coerce')\n",
    "dateNotConvertible = datetime_serie[pd.isnull(datetime_serie)]\n",
    "warnings.filterwarnings('default')\n",
    "pr('There are {} dates that cannot be transformed.'.format(len(dateNotConvertible)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:11:11 Starting copy...\n",
      "19:11:11 Converting to datetime...\n",
      "19:11:11 Setting up new indices...\n",
      "19:11:11 Deleting old \"createdAt\" column...\n",
      "19:11:11 Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>placeLatitude</th>\n",
       "      <th>placeLongitude</th>\n",
       "      <th>sourceName</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>userName</th>\n",
       "      <th>screenName</th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>userLocation</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>createdAt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-02-23 09:59:41</th>\n",
       "      <td>9519737890</td>\n",
       "      <td>1.46579e+07</td>\n",
       "      <td>Magic spells run off after midnight, I guess s...</td>\n",
       "      <td>6.13870</td>\n",
       "      <td>46.175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>http://twitter.com/#!/download/iphone</td>\n",
       "      <td>Javier Belmonte</td>\n",
       "      <td>vichango</td>\n",
       "      <td>167</td>\n",
       "      <td>277.0</td>\n",
       "      <td>2885.0</td>\n",
       "      <td>Geneva, Switzerland</td>\n",
       "      <td>[fb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-23 11:28:27</th>\n",
       "      <td>9521789689</td>\n",
       "      <td>9.96202e+06</td>\n",
       "      <td>Limitas of public transportation! No taxi, rai...</td>\n",
       "      <td>6.33641</td>\n",
       "      <td>46.4631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gowalla</td>\n",
       "      <td>http://gowalla.com/</td>\n",
       "      <td>Thomas Winter</td>\n",
       "      <td>thwinter</td>\n",
       "      <td>1070</td>\n",
       "      <td>1359.0</td>\n",
       "      <td>3349.0</td>\n",
       "      <td>Hettlingen CH / SanJose Ca</td>\n",
       "      <td>[yam]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id       userId  \\\n",
       "createdAt                                      \n",
       "2010-02-23 09:59:41  9519737890  1.46579e+07   \n",
       "2010-02-23 11:28:27  9521789689  9.96202e+06   \n",
       "\n",
       "                                                                  text  \\\n",
       "createdAt                                                                \n",
       "2010-02-23 09:59:41  Magic spells run off after midnight, I guess s...   \n",
       "2010-02-23 11:28:27  Limitas of public transportation! No taxi, rai...   \n",
       "\n",
       "                     longitude latitude placeId  inReplyTo source truncated  \\\n",
       "createdAt                                                                     \n",
       "2010-02-23 09:59:41    6.13870   46.175     NaN        NaN      1       NaN   \n",
       "2010-02-23 11:28:27    6.33641  46.4631     NaN        NaN    550       NaN   \n",
       "\n",
       "                    placeLatitude placeLongitude          sourceName  \\\n",
       "createdAt                                                              \n",
       "2010-02-23 09:59:41           NaN            NaN  Twitter for iPhone   \n",
       "2010-02-23 11:28:27           NaN            NaN             Gowalla   \n",
       "\n",
       "                                                 sourceUrl         userName  \\\n",
       "createdAt                                                                     \n",
       "2010-02-23 09:59:41  http://twitter.com/#!/download/iphone  Javier Belmonte   \n",
       "2010-02-23 11:28:27                    http://gowalla.com/    Thomas Winter   \n",
       "\n",
       "                    screenName followersCount  friendsCount  statusesCount  \\\n",
       "createdAt                                                                    \n",
       "2010-02-23 09:59:41   vichango            167         277.0         2885.0   \n",
       "2010-02-23 11:28:27   thwinter           1070        1359.0         3349.0   \n",
       "\n",
       "                                   userLocation hashtag  \n",
       "createdAt                                                \n",
       "2010-02-23 09:59:41         Geneva, Switzerland    [fb]  \n",
       "2010-02-23 11:28:27  Hettlingen CH / SanJose Ca   [yam]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr('Starting copy...') # (to avoid transformation problems)\n",
    "tw5 = twhCleanDate.copy()\n",
    "pr('Converting to datetime...')\n",
    "tw5['createdAt'] = pd.to_datetime(twhCleanDate['createdAt'])\n",
    "pr('Setting up new indices...')\n",
    "tw5.index = tw5['createdAt']\n",
    "pr('Deleting old \"createdAt\" column...')\n",
    "del tw5['createdAt']\n",
    "pr('Done!')\n",
    "tw5.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createdAt\n",
       "2010-02-23 09:59:41               [fb]\n",
       "2010-02-23 11:28:27              [yam]\n",
       "2010-02-23 17:47:11          [24, vfb]\n",
       "2010-02-23 18:19:03    [iphoneography]\n",
       "2010-02-23 18:31:46     [partnermonth]\n",
       "2010-02-24 06:09:23      [insider, fb]\n",
       "Name: hashtag, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw5['hashtag'][:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put one hashtag per row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will make a dataframe with one row = one hashtag. This will be done by going through the dataframe, and making in parallel a list of rows (with 1 hashtag per row) that needs to be added to the old dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addedHashtagsRowsList = []\n",
    "def multiplyHashtagRows(row, columns):\n",
    "    '''\n",
    "    Examine each row. If there are multiple hashtags, it will return the first one.\n",
    "    (so the first one will replace the list of hashtags in the df). Then for all the next ones,\n",
    "    it will make a copy of the row in the addedHashtagsRowsList, (in a dictionary format).\n",
    "    So this dictionary can in the end be transformed in a DF and added to the original DF.\n",
    "    (The speed is increased a lot by doing it this way!)\n",
    "    '''\n",
    "    htList = row.hashtag\n",
    "    if len(htList) > 1:\n",
    "        ## Making the dictionary\n",
    "        addedHashtag = {}\n",
    "        addedHashtag['createdAt'] = row.name #the df index\n",
    "        for col in columns:\n",
    "            addedHashtag[col] = row[col]\n",
    "        ## Copying the dict for each hashtag\n",
    "        i = 1\n",
    "        while i < len(htList) :\n",
    "            deepCopy = copy.deepcopy(addedHashtag)\n",
    "            deepCopy['hashtag'] = htList[i]\n",
    "            addedHashtagsRowsList.append(deepCopy)\n",
    "            i+=1\n",
    "    return htList[0] # return the first hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:11:11 Multiplying the hashtag rows... (around 10 min)\n",
      "19:11:13 Finished! 8887 rows will be added to the dataframe!\n"
     ]
    }
   ],
   "source": [
    "addedHashtagsRowsList = []\n",
    "tw5_1 = tw5.copy()\n",
    "pr('Multiplying the hashtag rows... (around 10 min)')\n",
    "tw5_1['hashtag'] = tw5.apply(multiplyHashtagRows, args=[tw5.columns,], axis=1)\n",
    "pr('Finished! {} rows will be added to the dataframe!'.format(len(addedHashtagsRowsList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:11:13 Starting to make the new dataframe with additionnal rows..\n",
      "19:11:13 Starting to append the two df... Old df size = 19719\n",
      "19:11:13 Done! New df size = 28606\n"
     ]
    }
   ],
   "source": [
    "pr('Starting to make the new dataframe with additionnal rows..')\n",
    "addedHashtagsDf = pd.DataFrame(addedHashtagsRowsList)\n",
    "addedHashtagsDf.set_index(['createdAt'], inplace=True)\n",
    "pr('Starting to append the two df... Old df size = {}'.format(len(tw5_1)))\n",
    "tw6 = tw5_1.append(addedHashtagsDf)\n",
    "pr('Done! New df size = {}'.format(len(tw6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example hahshtag:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>placeLatitude</th>\n",
       "      <th>placeLongitude</th>\n",
       "      <th>screenName</th>\n",
       "      <th>source</th>\n",
       "      <th>sourceName</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>userId</th>\n",
       "      <th>userLocation</th>\n",
       "      <th>userName</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>createdAt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-07-24 16:03:13</th>\n",
       "      <td>1121</td>\n",
       "      <td>763.0</td>\n",
       "      <td>vfb</td>\n",
       "      <td>95162125584039936</td>\n",
       "      <td>9.501397e+16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e401fb8eb4e7595a</td>\n",
       "      <td>47.5356</td>\n",
       "      <td>9.14004</td>\n",
       "      <td>eLd0raDo</td>\n",
       "      <td>14</td>\n",
       "      <td>Tweetbot for iOS</td>\n",
       "      <td>http://tapbots.com/tweetbot</td>\n",
       "      <td>6735.0</td>\n",
       "      <td>@mikstweed Wie lange ist es her, dass der #vfb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>921241</td>\n",
       "      <td>Kreuzlingen, TG, Switzerland</td>\n",
       "      <td>Markus Tressl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-23 17:47:11</th>\n",
       "      <td>1121</td>\n",
       "      <td>763.0</td>\n",
       "      <td>vfb</td>\n",
       "      <td>9535390586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.6463</td>\n",
       "      <td>9.16570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eLd0raDo</td>\n",
       "      <td>550</td>\n",
       "      <td>Gowalla</td>\n",
       "      <td>http://gowalla.com/</td>\n",
       "      <td>6735.0</td>\n",
       "      <td>So, Feierabend. Jetzt #24 und später #VfB. — a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>921241</td>\n",
       "      <td>Kreuzlingen, TG, Switzerland</td>\n",
       "      <td>Markus Tressl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-01 17:25:56</th>\n",
       "      <td>1317</td>\n",
       "      <td>830.0</td>\n",
       "      <td>vfb</td>\n",
       "      <td>10021795398684672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.9499</td>\n",
       "      <td>7.47071</td>\n",
       "      <td>e38a1a641d02f8db</td>\n",
       "      <td>46.9543</td>\n",
       "      <td>7.39491</td>\n",
       "      <td>chm</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>http://twitter.com/#!/download/iphone</td>\n",
       "      <td>13552.0</td>\n",
       "      <td>So, und jetzt Flachmann suchen und dann ab ins...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120433</td>\n",
       "      <td>Bern, Switzerland</td>\n",
       "      <td>chm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    followersCount  friendsCount hashtag                 id  \\\n",
       "createdAt                                                                     \n",
       "2011-07-24 16:03:13           1121         763.0     vfb  95162125584039936   \n",
       "2010-02-23 17:47:11           1121         763.0     vfb         9535390586   \n",
       "2010-12-01 17:25:56           1317         830.0     vfb  10021795398684672   \n",
       "\n",
       "                        inReplyTo latitude  longitude           placeId  \\\n",
       "createdAt                                                                 \n",
       "2011-07-24 16:03:13  9.501397e+16      NaN        NaN  e401fb8eb4e7595a   \n",
       "2010-02-23 17:47:11           NaN  47.6463    9.16570               NaN   \n",
       "2010-12-01 17:25:56           NaN  46.9499    7.47071  e38a1a641d02f8db   \n",
       "\n",
       "                    placeLatitude placeLongitude screenName source  \\\n",
       "createdAt                                                            \n",
       "2011-07-24 16:03:13       47.5356        9.14004   eLd0raDo     14   \n",
       "2010-02-23 17:47:11           NaN            NaN   eLd0raDo    550   \n",
       "2010-12-01 17:25:56       46.9543        7.39491        chm      1   \n",
       "\n",
       "                             sourceName  \\\n",
       "createdAt                                 \n",
       "2011-07-24 16:03:13    Tweetbot for iOS   \n",
       "2010-02-23 17:47:11             Gowalla   \n",
       "2010-12-01 17:25:56  Twitter for iPhone   \n",
       "\n",
       "                                                 sourceUrl  statusesCount  \\\n",
       "createdAt                                                                   \n",
       "2011-07-24 16:03:13            http://tapbots.com/tweetbot         6735.0   \n",
       "2010-02-23 17:47:11                    http://gowalla.com/         6735.0   \n",
       "2010-12-01 17:25:56  http://twitter.com/#!/download/iphone        13552.0   \n",
       "\n",
       "                                                                  text  \\\n",
       "createdAt                                                                \n",
       "2011-07-24 16:03:13  @mikstweed Wie lange ist es her, dass der #vfb...   \n",
       "2010-02-23 17:47:11  So, Feierabend. Jetzt #24 und später #VfB. — a...   \n",
       "2010-12-01 17:25:56  So, und jetzt Flachmann suchen und dann ab ins...   \n",
       "\n",
       "                    truncated  userId                  userLocation  \\\n",
       "createdAt                                                             \n",
       "2011-07-24 16:03:13       NaN  921241  Kreuzlingen, TG, Switzerland   \n",
       "2010-02-23 17:47:11       NaN  921241  Kreuzlingen, TG, Switzerland   \n",
       "2010-12-01 17:25:56       NaN  120433             Bern, Switzerland   \n",
       "\n",
       "                          userName  \n",
       "createdAt                           \n",
       "2011-07-24 16:03:13  Markus Tressl  \n",
       "2010-02-23 17:47:11  Markus Tressl  \n",
       "2010-12-01 17:25:56            chm  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Example hahshtag:')\n",
    "tw6[tw6['hashtag'] == addedHashtagsRowsList[0]['hashtag']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createdAt\n",
       "2010-02-23 09:59:41               fb\n",
       "2010-02-23 11:28:27              yam\n",
       "2010-02-23 17:47:11               24\n",
       "2010-02-23 18:19:03    iphoneography\n",
       "2010-02-23 18:31:46     partnermonth\n",
       "Name: hashtag, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw6.hashtag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping per hashtags per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to provide visualisation to the other team, we needed to give ocation information to all tweets. So we did not process tweets without longitude or latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw6.dropna(subset=['longitude'], inplace=True)\n",
    "tw6.dropna(subset=['latitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw6.latitude = tw6.latitude.apply(float)\n",
    "tw6.longitude = tw6.longitude.apply(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging information of all tweet containing a particular hastag turned out to be really useful to detect an event. For column of String type, we used a join with a delimiter as balow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delimiter = '_$$$_'\n",
    "str_join = lambda x: delimiter.join(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that applies to a dataframe will group each row by day and aggregate all its content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggDate(df):\n",
    "    temp = df.groupby(df.index.map(lambda x: x.date))\n",
    "    groupedDf = temp.agg({  'text' : str_join,\n",
    "                            'longitude' : np.median,\n",
    "                            'latitude' : np.median,\n",
    "                            'hashtag' : lambda x: x.iloc[0], ## the first occurence\n",
    "                            'numberOfTweets' : 'count',\n",
    "                            'userId' : pd.Series.nunique})\n",
    "    # rename userId column to a more representative name\n",
    "    return groupedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:11:13 Making column number of tweets\n",
      "19:11:13 Starting group by hastag...\n",
      "19:11:13 Starting to put hashtag in dictionary... (around 50 min)\n",
      "19:11:18 10%\n",
      "19:11:23 20%\n",
      "19:11:28 30%\n",
      "19:11:33 40%\n",
      "19:11:38 50%\n",
      "19:11:43 60%\n",
      "19:11:48 70%\n",
      "19:11:53 80%\n",
      "19:11:58 90%\n",
      "19:12:03 100%\n",
      "19:12:03 Finished operations! Dictionary with 9410 different hashtags.\n"
     ]
    }
   ],
   "source": [
    "pr('Making column number of tweets')\n",
    "tw6['numberOfTweets'] = 1\n",
    "pr('Starting group by hastag...')\n",
    "gp = tw6.groupby('hashtag')\n",
    "pr('Starting to put hashtag in dictionary... (around 50 min)')\n",
    "\n",
    "count = 0\n",
    "lengp = len(gp)\n",
    "printingValue = int(lengp / 10)\n",
    "dictionary = {}\n",
    "for hashtag, df in gp:\n",
    "    dictionary[hashtag] = aggDate(df)\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/lengp*100))\n",
    "pr('Finished operations! Dictionary with {} different hashtags.'.format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary with hashtags dataframes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>longitude</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>latitude</th>\n",
       "      <th>numberOfTweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-12-23</th>\n",
       "      <td>1</td>\n",
       "      <td>8.31025</td>\n",
       "      <td>schafft starbucks aus! Die reden immer englisc...</td>\n",
       "      <td>wirtschaftsfluechtling</td>\n",
       "      <td>47.056</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            userId  longitude  \\\n",
       "2010-12-23       1    8.31025   \n",
       "\n",
       "                                                         text  \\\n",
       "2010-12-23  schafft starbucks aus! Die reden immer englisc...   \n",
       "\n",
       "                           hashtag  latitude  numberOfTweets  \n",
       "2010-12-23  wirtschaftsfluechtling    47.056               1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creation of a dictionnary {'hashtag' : 'dataframe containing all tweet with the hashtag'}\n",
    "print('Dictionary with hashtags dataframes:')\n",
    "dictionary[list(dictionary.keys())[5]].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event detection (with elimination of recurrent events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that define events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Parameters of an event:\n",
    "MIN_TOT_NB_TWEETS = 20 ## The hashtag must have happened at least this number of times to be considered.\n",
    "MIN_NB_DAYS_WITH_HASHTAGS = 3 ## The hashtags must appear at least this number of different days to be considered.\n",
    "MIN_NB_TWEETS_DURING_EVENT = 7 ## To be considered an event, the hashtag must happen at least this nb of times during the day.\n",
    "THRESHOLD_ANOMALY_FACTOR = 2.5 ## The occurence of a hashtag during a single day must be above the mean by this FACTOR\n",
    "                             ## multiplied by the std to be considered as an anomaly.\n",
    "MAX_DURATION_OF_EVENT = timedelta(days=30) ## The maximum number of days we consider an event can happen\n",
    "MIN_DURATION_BEFORE_NEW_EVENT = timedelta(days=304) ## (= 10 months) The min time that should pass before an event can happen\n",
    "                                                    ## again and still be considered as event (ie. Christmas is an event\n",
    "                                                    ## each year)\n",
    "MIN_NUMBER_DIFFERENT_USER = 2 # To state that an event occured, a minimum number of different users should have tweeted about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to detect recurrent events that should be removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isSpecificEventListIllegal(detectedEventDateList):\n",
    "    '''\n",
    "    Return true if the list of dates contain illegal tupples of events, so if the event is recurrent\n",
    "    which would mean it is not a real event.\n",
    "    '''\n",
    "    def datesAreIllegal(date1, date2, date3):\n",
    "        '''\n",
    "        Return true if the 3 dates are not to be considered as regular events.\n",
    "        '''\n",
    "        ## Return if the difference is too small to be considered as 2 different events\n",
    "        def diffIsSmall(timeDiff):  \n",
    "            return timeDiff < MAX_DURATION_OF_EVENT\n",
    "\n",
    "        ## Return true if the difference is not big enough to be an annual event.\n",
    "        def isDiffSuspect(timeDiff):\n",
    "            return timeDiff < MIN_DURATION_BEFORE_NEW_EVENT   \n",
    "\n",
    "        diff1 = abs(date1 - date2)\n",
    "        diff2 = abs(date2 - date3)\n",
    "        diff3 = abs(date3 - date1)\n",
    "\n",
    "        ## The difference is too small, it must be the same event\n",
    "        if diffIsSmall(diff1) or diffIsSmall(diff2) or diffIsSmall(diff3):\n",
    "            return False\n",
    "\n",
    "        ## If there are at least 2 out of 3 suspect difference, then the dates are illegal\n",
    "        if isDiffSuspect(diff1):\n",
    "            return isDiffSuspect(diff2) or isDiffSuspect(diff3)\n",
    "        else:\n",
    "            return isDiffSuspect(diff2) and isDiffSuspect(diff3)\n",
    "    \n",
    "    ## MAIN FUNCTION : ##\n",
    "    # Go through the list of events and try all \"triples\" to see if there is any illegal triples. This is a quickly done\n",
    "    # code to do that. Code complexity bellow is in O(k^3), with k being the size of the list. We will apply this function\n",
    "    # to n list so we will have an overall complexity in O(n*k^3). We can consider however that each list will\n",
    "    # be small so k can be considered as constant and therefore the overall complexity will be in O(n).\n",
    "    for i in range(len(detectedEventDateList) - 2):\n",
    "        for j in range(i, len(detectedEventDateList) - 1):\n",
    "            for k in range(j, len(detectedEventDateList)):\n",
    "                if datesAreIllegal(detectedEventDateList[i], detectedEventDateList[j], detectedEventDateList[k]):\n",
    "                    return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main method to detect event according to all our criterias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:12:03 Starting to compute 9410 dict items to detect event. (4 min)\n",
      "19:12:03 10%\n",
      "19:12:04 20%\n",
      "19:12:04 30%\n",
      "19:12:04 40%\n",
      "19:12:05 50%\n",
      "19:12:05 60%\n",
      "19:12:05 70%\n",
      "19:12:06 80%\n",
      "19:12:06 90%\n",
      "19:12:06 100%\n",
      "19:12:06 Finished! Number of events detected = 3\n"
     ]
    }
   ],
   "source": [
    "pr('Starting to compute {} dict items to detect event. (4 min)'.format(len(dictionary)))\n",
    "nbOfEventDetected = 0\n",
    "count = 0\n",
    "printingValue = int(len(dictionary) / 10)\n",
    "for [h,df] in dictionary.items():\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "    df['event'] = False\n",
    "    if len(df) > MIN_NB_DAYS_WITH_HASHTAGS:\n",
    "        if df['numberOfTweets'].sum() > MIN_TOT_NB_TWEETS:\n",
    "            if df['userId'][0] >= MIN_NUMBER_DIFFERENT_USER:\n",
    "                threshold = df['numberOfTweets'].mean() + THRESHOLD_ANOMALY_FACTOR * df['numberOfTweets'].std()\n",
    "                df['event'] = df.numberOfTweets.apply(lambda x: x > threshold and x > MIN_NB_TWEETS_DURING_EVENT)\n",
    "            \n",
    "            ## Remove recurrent events:\n",
    "            detectedEventDf = df[df['event']]\n",
    "            if len(detectedEventDf) > 2 and isSpecificEventListIllegal(detectedEventDf.index):\n",
    "                df['event'] = False\n",
    "            nbOfEventDetected += len(df[df['event']])\n",
    "pr('Finished! Number of events detected = {}'.format(nbOfEventDetected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pr('Starting to compute {} dict items to detect event. (4 min)'.format(len(dictionary)))\n",
    "# nbOfEventDetected = 0\n",
    "# count = 0\n",
    "# printingValue = int(len(dictionary) / 10)\n",
    "# for [h,df] in dictionary.items():\n",
    "#     count += 1\n",
    "#     if count % printingValue == 0:\n",
    "#         pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "#     df['event'] = False\n",
    "#     if len(df) > MIN_NB_DAYS_WITH_HASHTAGS:\n",
    "#         if df['numberOfTweets'].sum() > MIN_TOT_NB_TWEETS:\n",
    "#             threshold = df['numberOfTweets'].mean() + THRESHOLD_ANOMALY_FACTOR * df['numberOfTweets'].std()\n",
    "#             df['event'] = df.numberOfTweets.apply(lambda x: x >= threshold and x >= MIN_NB_TWEETS_DURING_EVENT)\n",
    "            \n",
    "#             ## Remove recurrent events:\n",
    "#             detectedEventDf = df[df['event']]\n",
    "#             if len(detectedEventDf) > 2 and isSpecificEventListIllegal(detectedEventDf.index):\n",
    "#                 df['event'] = False\n",
    "#             nbOfEventDetected += len(df[df['event']])\n",
    "# pr('Finished! Number of events detected = {}'.format(nbOfEventDetected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging close events and grouping into single event dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have a function that is going to merge events that are considered as too \"close\" to each other to be considered individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mergeCloseEvents(rowsList):\n",
    "    '''\n",
    "    Take a list of dictionary, where each dictionary is a \"row\" of the event df, which contained detected events.\n",
    "    It will process the list to detect event that are close and merge them together.\n",
    "    Return : the processed list of event.\n",
    "    '''\n",
    "    \n",
    "    def areCloseEvents(event1, event2):\n",
    "        '''\n",
    "        Return true is 2 events dates are defined as \"close\"\n",
    "        '''\n",
    "        return abs(event1['date'] - event2['date']) < MAX_DURATION_OF_EVENT\n",
    "        \n",
    "    def mergeCloseEventsSublist(closeEventList):\n",
    "        '''\n",
    "        This will be applied to each close event sublist. It will merge all events into one unique event.\n",
    "        The event will consist of the total number of tweets, with the concatenation of the tweet texts and the mean\n",
    "        of longitude/latitude. A meanDate will be defined as a ponderated mean between all dates.\n",
    "        The final date will be the one that is in the closeEventList and is closest to this mean date.\n",
    "        We did this to keep the meaning of the date if it had some, and not have some meaningless \"mean-date\".\n",
    "        '''\n",
    "        latitude = 0\n",
    "        longitude = 0\n",
    "        numberOfTweets = 0\n",
    "        text = \"\"\n",
    "        originalDate = closeEventList[0]['date']\n",
    "        dateDiff = timedelta(days=0)\n",
    "        first = True        \n",
    "        for tweet in closeEventList:\n",
    "            longitude += tweet['longitude']\n",
    "            latitude += tweet['latitude'] \n",
    "            numberOfTweets += tweet['numberOfTweets']\n",
    "            if first:\n",
    "                text = tweet['text']\n",
    "                first = False\n",
    "            else:\n",
    "                text += delimiter + tweet['text']\n",
    "                dateDiff = dateDiff + (tweet['date'] - originalDate) * tweet['numberOfTweets']\n",
    "\n",
    "        ## It is multiplied by 2 then soustracted to round correctly to the nearest day\n",
    "        meanDate = originalDate + 2* dateDiff / numberOfTweets - dateDiff / numberOfTweets        \n",
    "        latitude = latitude / len(closeEventList)\n",
    "        longitude = longitude / len(closeEventList)\n",
    "        \n",
    "        ## We are going to detect the event the closest to the mean date\n",
    "        minSelectedDate = closeEventList[0]['date']\n",
    "        minDistance = abs(closeEventList[0]['date'] - meanDate)\n",
    "        for tweet in closeEventList:\n",
    "            if abs(tweet['date'] - meanDate) < minDistance:\n",
    "                minSelectedDate = tweet['date']    \n",
    "        \n",
    "        return {'date': minSelectedDate, 'hashtag': closeEventList[0]['hashtag'], 'text': text,\n",
    "                    'longitude': longitude, 'latitude':latitude, 'numberOfTweets': numberOfTweets, }\n",
    "    \n",
    "    ############ -----  MAIN METHOD  ----- ############\n",
    "    \n",
    "    ## If the list is big enough, go through the list and form an export list and merge elements that needs to.\n",
    "    if len(rowsList) < 2:\n",
    "        return rowsList\n",
    "    else:\n",
    "        firstLastPosOfItemsToMerge = []\n",
    "        sortedRowsList = sorted(rowsList, key=itemgetter('date')) \n",
    "        exportedEventList = []\n",
    "        ## This goes through the *sorted* list and add the pair of indices (first indice and last indice) where events \n",
    "        ## that should be merged appear.\n",
    "        lastEventWasClose = False\n",
    "        firstItem = -1\n",
    "        for i in range(0, len(sortedRowsList)-1):\n",
    "            if areCloseEvents(sortedRowsList[i], sortedRowsList[i+1]):\n",
    "                if not lastEventWasClose: # So it is the first pairs of the sublist of close events in the whole list\n",
    "                    firstItem = i\n",
    "                    lastEventWasClose = True\n",
    "            else:\n",
    "                if lastEventWasClose: # So the list has just ended.\n",
    "                    exportedEventList.append(mergeCloseEventsSublist(sortedRowsList[firstItem:i+1]))\n",
    "                    lastEventWasClose = False\n",
    "                else: # The element is by itself, let's append it\n",
    "                    exportedEventList.append(sortedRowsList[i])  \n",
    "        if lastEventWasClose: # If there were events to merge till the last elem of list\n",
    "            exportedEventList.append(mergeCloseEventsSublist(sortedRowsList[firstItem:len(sortedRowsList)]))\n",
    "        else:\n",
    "            exportedEventList.append(sortedRowsList[len(sortedRowsList)-1])\n",
    "    \n",
    "    return exportedEventList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be applied to each dataframe. If a row is detected as an event, it will be added to the locaRowsList which will be used to make a general dataframe of all the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "localRowsList = []\n",
    "def applyToMakeEventDf(row):\n",
    "    if row.event:\n",
    "        rowToAdd = {'date': row.name, 'hashtag': row.hashtag, 'text': row.text,\n",
    "                    'longitude': row.longitude, 'latitude':row.latitude, 'numberOfTweets': row.numberOfTweets, }\n",
    "        global localRowsList\n",
    "        localRowsList.append(rowToAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:12:07 Starting to make event df with 9410 dataframes. (around 6 min)\n",
      "19:12:07 10%\n",
      "19:12:08 20%\n",
      "19:12:08 30%\n",
      "19:12:09 40%\n",
      "19:12:09 50%\n",
      "19:12:10 60%\n",
      "19:12:10 70%\n",
      "19:12:11 80%\n",
      "19:12:11 90%\n",
      "19:12:12 100%\n",
      "19:12:12 Making new dataframe.\n",
      "19:12:12 Finished! Dataframe with 3 rows\n"
     ]
    }
   ],
   "source": [
    "eventRowsList = []\n",
    "localRowsList = []\n",
    "count = 0\n",
    "printingValue = int(len(dictionary) / 10)\n",
    "\n",
    "pr('Starting to make event df with {} dataframes. (around 6 min)'.format(len(dictionary)))\n",
    "for h, df in dictionary.items():\n",
    "    global localRowsList\n",
    "    localRowsList = []\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "        \n",
    "    df.apply(applyToMakeEventDf, axis=1)\n",
    "    mergedList = mergeCloseEvents(localRowsList) # merging close events\n",
    "    eventRowsList += mergedList\n",
    "\n",
    "pr('Making new dataframe.')\n",
    "new_events = pd.DataFrame(eventRowsList)\n",
    "new_events.set_index(['date'], inplace=True)\n",
    "pr('Finished! Dataframe with {} rows'.format(len(new_events)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>numberOfTweets</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-03-25</th>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.3751</td>\n",
       "      <td>8.53896</td>\n",
       "      <td>41</td>\n",
       "      <td>@greezer der @ThBenkoe hat's schon #iPad2   ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-16</th>\n",
       "      <td>sui</td>\n",
       "      <td>47.3788</td>\n",
       "      <td>8.53803</td>\n",
       "      <td>25</td>\n",
       "      <td>#sui wonderful_$$$_en plus du goal, j'aurai mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-06-08</th>\n",
       "      <td>ios5</td>\n",
       "      <td>47.4850</td>\n",
       "      <td>8.29721</td>\n",
       "      <td>9</td>\n",
       "      <td>@AndreBonhote einstellungen gemacht? #ios5  ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           hashtag  latitude  longitude  numberOfTweets  \\\n",
       "date                                                      \n",
       "2011-03-25   ipad2   47.3751    8.53896              41   \n",
       "2010-06-16     sui   47.3788    8.53803              25   \n",
       "2011-06-08    ios5   47.4850    8.29721               9   \n",
       "\n",
       "                                                         text  \n",
       "date                                                           \n",
       "2011-03-25  @greezer der @ThBenkoe hat's schon #iPad2   ht...  \n",
       "2010-06-16  #sui wonderful_$$$_en plus du goal, j'aurai mi...  \n",
       "2011-06-08  @AndreBonhote einstellungen gemacht? #ios5  ht...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Events dataframe:')\n",
    "new_events.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked dataframe of all days:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>longitude</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>latitude</th>\n",
       "      <th>numberOfTweets</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-03-02</th>\n",
       "      <td>3</td>\n",
       "      <td>8.42875</td>\n",
       "      <td>http://bit.ly/hpQGAV #ipad2 #leaked photos_$$$...</td>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.0333</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-03-04</th>\n",
       "      <td>1</td>\n",
       "      <td>9.60857</td>\n",
       "      <td>Conan takes on the #iPad2 http://t.co/I97hmyJ</td>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.2488</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-03-15</th>\n",
       "      <td>1</td>\n",
       "      <td>8.51569</td>\n",
       "      <td>@pokipsie oh! #ipad2 schon bestellt?</td>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.3601</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-03-25</th>\n",
       "      <td>12</td>\n",
       "      <td>8.53896</td>\n",
       "      <td>@greezer der @ThBenkoe hat's schon #iPad2   ht...</td>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.3751</td>\n",
       "      <td>41</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-03-26</th>\n",
       "      <td>1</td>\n",
       "      <td>8.50014</td>\n",
       "      <td>Maaaaannnn after a Day like this I'm to pumped...</td>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.3689</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-06</th>\n",
       "      <td>1</td>\n",
       "      <td>8.54340</td>\n",
       "      <td>ach wie schön, nur ein paar Kilometer von Züri...</td>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.2872</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-08</th>\n",
       "      <td>1</td>\n",
       "      <td>8.51900</td>\n",
       "      <td>@rkroebl Fehlplanung. Habs verkauft um dann di...</td>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.1827</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-06-08</th>\n",
       "      <td>1</td>\n",
       "      <td>8.29843</td>\n",
       "      <td>Ich suche ein #ipad2 64Gb 3G. Wer verkauft ein...</td>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.4842</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-06-15</th>\n",
       "      <td>1</td>\n",
       "      <td>8.29995</td>\n",
       "      <td>Wär hätte Interesse an neuem #iPad2 64GB 3G ? ...</td>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.4693</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-09-24</th>\n",
       "      <td>1</td>\n",
       "      <td>8.26805</td>\n",
       "      <td>@greezer mh, 4.3.3 lässt sich noch jailbreaken...</td>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.2089</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            userId  longitude  \\\n",
       "2011-03-02       3    8.42875   \n",
       "2011-03-04       1    9.60857   \n",
       "2011-03-15       1    8.51569   \n",
       "2011-03-25      12    8.53896   \n",
       "2011-03-26       1    8.50014   \n",
       "2011-04-06       1    8.54340   \n",
       "2011-04-08       1    8.51900   \n",
       "2011-06-08       1    8.29843   \n",
       "2011-06-15       1    8.29995   \n",
       "2011-09-24       1    8.26805   \n",
       "\n",
       "                                                         text hashtag  \\\n",
       "2011-03-02  http://bit.ly/hpQGAV #ipad2 #leaked photos_$$$...   ipad2   \n",
       "2011-03-04      Conan takes on the #iPad2 http://t.co/I97hmyJ   ipad2   \n",
       "2011-03-15               @pokipsie oh! #ipad2 schon bestellt?   ipad2   \n",
       "2011-03-25  @greezer der @ThBenkoe hat's schon #iPad2   ht...   ipad2   \n",
       "2011-03-26  Maaaaannnn after a Day like this I'm to pumped...   ipad2   \n",
       "2011-04-06  ach wie schön, nur ein paar Kilometer von Züri...   ipad2   \n",
       "2011-04-08  @rkroebl Fehlplanung. Habs verkauft um dann di...   ipad2   \n",
       "2011-06-08  Ich suche ein #ipad2 64Gb 3G. Wer verkauft ein...   ipad2   \n",
       "2011-06-15  Wär hätte Interesse an neuem #iPad2 64GB 3G ? ...   ipad2   \n",
       "2011-09-24  @greezer mh, 4.3.3 lässt sich noch jailbreaken...   ipad2   \n",
       "\n",
       "            latitude  numberOfTweets  event  \n",
       "2011-03-02   47.0333               3  False  \n",
       "2011-03-04   47.2488               1  False  \n",
       "2011-03-15   47.3601               1  False  \n",
       "2011-03-25   47.3751              41   True  \n",
       "2011-03-26   47.3689               1  False  \n",
       "2011-04-06   47.2872               1  False  \n",
       "2011-04-08   47.1827               1  False  \n",
       "2011-06-08   47.4842               1  False  \n",
       "2011-06-15   47.4693               1  False  \n",
       "2011-09-24   47.2089               1  False  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Linked dataframe of all days:')\n",
    "dictionary[new_events.iloc[0].hashtag].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we worked with another team, we needed a way to communicate them our detection. We used a JSON with all the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 events.\n"
     ]
    }
   ],
   "source": [
    "total_number_of_events = len(new_events)\n",
    "print('There are {} events.'.format(total_number_of_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>numberOfTweets</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.3751</td>\n",
       "      <td>8.53896</td>\n",
       "      <td>41</td>\n",
       "      <td>@greezer der @ThBenkoe hat's schon #iPad2   ht...</td>\n",
       "      <td>2011-03-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hashtag  latitude  longitude  numberOfTweets  \\\n",
       "0   ipad2   47.3751    8.53896              41   \n",
       "\n",
       "                                                text        date  \n",
       "0  @greezer der @ThBenkoe hat's schon #iPad2   ht...  2011-03-25  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_df = new_events.copy()\n",
    "e_df['date'] = e_df.index\n",
    "e_df.index = [i for i in range (len(e_df))]\n",
    "e_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to generate the right datetimes for the jsons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_dt = datetime(1970, 1, 1)\n",
    "def to_utc(date):\n",
    "    d_dt = datetime.combine(date, datetime.min.time())\n",
    "    return int((d_dt - epoch_dt).total_seconds()*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_unix_time(record):\n",
    "    datetime_index = pd.DatetimeIndex([datetime(record['year'], record['month'], 1)])\n",
    "    unix_time_index = datetime_index.astype(np.int64) // 10**6\n",
    "    return unix_time_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:12:12 Converting dates...\n",
      "19:12:12 Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>numberOfTweets</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>utc_date</th>\n",
       "      <th>unix_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ipad2</td>\n",
       "      <td>47.3751</td>\n",
       "      <td>8.53896</td>\n",
       "      <td>41</td>\n",
       "      <td>@greezer der @ThBenkoe hat's schon #iPad2   ht...</td>\n",
       "      <td>2011-03-25</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>1301011200000</td>\n",
       "      <td>1298937600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hashtag  latitude  longitude  numberOfTweets  \\\n",
       "0   ipad2   47.3751    8.53896              41   \n",
       "\n",
       "                                                text        date  year  month  \\\n",
       "0  @greezer der @ThBenkoe hat's schon #iPad2   ht...  2011-03-25  2011      3   \n",
       "\n",
       "        utc_date      unix_time  \n",
       "0  1301011200000  1298937600000  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr('Converting dates...')\n",
    "e_df['year'] = e_df['date'].apply(lambda x: x.year)\n",
    "e_df['month'] = e_df['date'].apply(lambda x: x.month)\n",
    "e_df['utc_date'] = e_df['date'].apply(lambda x: to_utc(x))\n",
    "e_df['unix_time'] = e_df.apply(convert_to_unix_time, axis=1)\n",
    "pr('Done.')\n",
    "e_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation of a JSON is easier from a dictionary than from a dataframe. Also, the other team we worked with asked us to group events by months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grouping by months\n",
    "e_gb_month = e_df.groupby(e_df.unix_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:12:12 Making event list...\n",
      "19:12:12 Done.\n"
     ]
    }
   ],
   "source": [
    "# Generation of the dictionary for the final JSON\n",
    "pr('Making event list...')\n",
    "months = []\n",
    "for month, df in e_gb_month:\n",
    "    days = []\n",
    "    for i in range (len(df)):\n",
    "        ht = df.iloc[i]['hashtag']\n",
    "        lat = df.iloc[i]['latitude']\n",
    "        lon = df.iloc[i]['longitude']\n",
    "        t_num = df.iloc[i]['numberOfTweets']\n",
    "        tweets = df.iloc[i]['text'].split(delimiter)\n",
    "        date = df.iloc[i]['utc_date']\n",
    "        \n",
    "        data_unit = { 'name': ht\n",
    "                    , 'latitude' : lat\n",
    "                    , 'longitude' : lon\n",
    "                    , 'tweets' : tweets\n",
    "                    , 'number_of_tweets' : str(t_num)\n",
    "                    , 'date' : int(date)}\n",
    "        days.append(data_unit)\n",
    "    \n",
    "    curr_month = {'date': int(month), 'data' : days}\n",
    "    months.append(curr_month)\n",
    "\n",
    "final_events = {'events' : months}\n",
    "pr('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the final JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:12:12 Exporting to json...\n",
      "19:12:12 Export done. File \"export_twitter_events_2017-02-03_19h12min12_3_events.json\" has been created.\n"
     ]
    }
   ],
   "source": [
    "exportFilename = 'export_twitter_events_' + datetime.now().strftime(\"%Y-%m-%d_%Hh%Mmin%S\") + \\\n",
    "'_' + str(total_number_of_events)+ '_events.json'\n",
    "exportPath =  os.path.join('data', exportFilename)\n",
    "\n",
    "pr('Exporting to json...')\n",
    "with open(exportPath, 'w') as f:\n",
    "     json.dump(final_events, f)\n",
    "pr('Export done. File \"{}\" has been created.'.format(exportFilename))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ADA-kernel]",
   "language": "python",
   "name": "conda-env-ADA-kernel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
