{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import math\n",
    "from collections import Counter\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "import copy\n",
    "import json\n",
    "from datetime import timedelta\n",
    "from operator import itemgetter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of check point (a print plus the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def now():\n",
    "    return str(datetime.now().time())[:8]\n",
    "def pr(strToPrint):\n",
    "    print(now() + ' '+ strToPrint)\n",
    "\n",
    "    \n",
    "# from IPython.display import Audio\n",
    "# sound_file = 'beep.wav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_filename = os.path.join('data','head_100k_pickle.pkl')\n",
    "tw = pd.read_pickle(pickle_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_header = ['id', 'userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId',\n",
    "                  'inReplyTo', 'source', 'truncated', 'placeLatitude', 'placeLongitude', 'sourceName', 'sourceUrl',\n",
    "                 'userName', 'screenName', 'followersCount', 'friendsCount', 'statusesCount',\n",
    "                 'userLocation']\n",
    "\n",
    "filename = os.path.join('data','twex.tsv') # 'sample.tsv')\n",
    "pr('Starting to read file... (3 min)')\n",
    "tw = pd.read_csv(filename, sep='\\t', encoding='utf-8', escapechar='\\\\', names=columns_header,\n",
    "                      quoting=csv.QUOTE_NONE, na_values='N', header=None)\n",
    "\n",
    "pr('File is loaded!')\n",
    "# Audio(url=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrating hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_hashtags(text):\n",
    "    ht_list = re.findall(r\"#(\\w+)\", text)\n",
    "    non_empty_hts = list(filter((lambda ht: ht != []), ht_list))\n",
    "    lowerCharList = [ht.lower() for ht in non_empty_hts]\n",
    "    return lowerCharList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr('Making hashtags... (2 min)')\n",
    "tw['hashtag'] = np.nan\n",
    "tw.hashtag = tw.text.apply(lambda x: extract_hashtags(str(x)))\n",
    "twh = tw.ix[tw.hashtag.apply(lambda x: len(x) != 0)]\n",
    "pr('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twh.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data and making date index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop tweet which not contains values for text or createdAt as this is mandatory information to privide to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw1 = twh.dropna(axis=0, how='any', subset=['text', 'createdAt'])\n",
    "print('The data have been reduced from {} tweets to {} tweets.'.format(len(twh), len(tw1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr('Removing bad dates...')\n",
    "twhCleanDate = tw1[tw1['createdAt'].str.len() == 19]\n",
    "pr('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr('Starting to examine dates...')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "datetime_serie = twhCleanDate['createdAt'].convert_objects(convert_dates='coerce')\n",
    "dateNotConvertible = datetime_serie[pd.isnull(datetime_serie)]\n",
    "warnings.filterwarnings('default')\n",
    "pr('There are {} dates that cannot be transformed.'.format(len(dateNotConvertible)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pr('Starting copy...') # (to avoid transformation problems)\n",
    "tw5 = twhCleanDate.copy()\n",
    "pr('Converting to datetime...')\n",
    "tw5['createdAt'] = pd.to_datetime(twhCleanDate['createdAt'])\n",
    "pr('Setting up new indices...')\n",
    "tw5.index = tw5['createdAt']\n",
    "pr('Deleting old \"createdAt\" column...')\n",
    "del tw5['createdAt']\n",
    "pr('Done!')\n",
    "tw5.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw5['hashtag'][:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put one hashtag per row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will make a dataframe with one row = one hashtag. This will be done by going through the dataframe, and making in parallel a list of rows (with 1 hashtag per row) that needs to be added to the old dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addedHashtagsRowsList = []\n",
    "def multiplyHashtagRows(row, columns):\n",
    "    '''\n",
    "    Examine each row. If there are multiple hashtags, it will return the first one.\n",
    "    (so the first one will replace the list of hashtags in the df). Then for all the next ones,\n",
    "    it will make a copy of the row in the addedHashtagsRowsList, (in a dictionary format).\n",
    "    So this dictionary can in the end be transformed in a DF and added to the original DF.\n",
    "    (The speed is increased a lot by doing it this way!)\n",
    "    '''\n",
    "    htList = row.hashtag\n",
    "    if len(htList) > 1:\n",
    "        ## Making the dictionary\n",
    "        addedHashtag = {}\n",
    "        addedHashtag['createdAt'] = row.name #the df index\n",
    "        for col in columns:\n",
    "            addedHashtag[col] = row[col]\n",
    "        ## Copying the dict for each hashtag\n",
    "        i = 1\n",
    "        while i < len(htList) :\n",
    "            deepCopy = copy.deepcopy(addedHashtag)\n",
    "            deepCopy['hashtag'] = htList[i]\n",
    "            addedHashtagsRowsList.append(deepCopy)\n",
    "            i+=1\n",
    "    return htList[0] # return the first hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addedHashtagsRowsList = []\n",
    "tw5_1 = tw5.copy()\n",
    "pr('Multiplying the hashtag rows... (around 10 min)')\n",
    "tw5_1['hashtag'] = tw5.apply(multiplyHashtagRows, args=[tw5.columns,], axis=1)\n",
    "pr('Finished! {} rows will be added to the dataframe!'.format(len(addedHashtagsRowsList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr('Starting to make the new dataframe with additionnal rows..')\n",
    "addedHashtagsDf = pd.DataFrame(addedHashtagsRowsList)\n",
    "addedHashtagsDf.set_index(['createdAt'], inplace=True)\n",
    "pr('Starting to append the two df... Old df size = {}'.format(len(tw5_1)))\n",
    "tw6 = tw5_1.append(addedHashtagsDf)\n",
    "pr('Done! New df size = {}'.format(len(tw6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Example hahshtag:')\n",
    "tw6[tw6['hashtag'] == addedHashtagsRowsList[0]['hashtag']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw6.hashtag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping per hashtags per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to provide visualisation to the other team, we needed to give ocation information to all tweets. So we did not process tweets without longitude or latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw6.dropna(subset=['longitude'], inplace=True)\n",
    "tw6.dropna(subset=['latitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw6.latitude = tw6.latitude.apply(float)\n",
    "tw6.longitude = tw6.longitude.apply(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging information of all tweet containing a particular hastag turned out to be really useful to detect an event. For column of String type, we used a join with a delimiter as balow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delimiter = '_$$$_'\n",
    "str_join = lambda x: delimiter.join(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that applies to a dataframe will group each row by day and aggregate all its content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggDate(df):\n",
    "    temp = df.groupby(df.index.map(lambda x: x.date))\n",
    "    groupedDf = temp.agg({  'text' : str_join,\n",
    "                            'longitude' : np.median,\n",
    "                            'latitude' : np.median,\n",
    "                            'hashtag' : lambda x: x.iloc[0], ## the first occurence\n",
    "                            'numberOfTweets' : 'count',\n",
    "                            'userId' : pd.Series.nunique})\n",
    "    # rename userId column to a more representative name\n",
    "    return groupedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creation of a dictionnary {'hashtag' : 'dataframe containing all tweet with the hashtag'}\n",
    "print('Dictionary with hashtags dataframes:')\n",
    "dictionary[list(dictionary.keys())[5]].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event detection (with elimination of recurrent events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that define events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Parameters of an event:\n",
    "MIN_TOT_NB_TWEETS = 20 ## The hashtag must have happened at least this number of times to be considered.\n",
    "MIN_NB_DAYS_WITH_HASHTAGS = 3 ## The hashtags must appear at least this number of different days to be considered.\n",
    "MIN_NB_TWEETS_DURING_EVENT = 7 ## To be considered an event, the hashtag must happen at least this nb of times during the day.\n",
    "THRESHOLD_ANOMALY_FACTOR = 2.5 ## The occurence of a hashtag during a single day must be above the mean by this FACTOR\n",
    "                             ## multiplied by the std to be considered as an anomaly.\n",
    "MAX_DURATION_OF_EVENT = timedelta(days=30) ## The maximum number of days we consider an event can happen\n",
    "MIN_DURATION_BEFORE_NEW_EVENT = timedelta(days=304) ## (= 10 months) The min time that should pass before an event can happen\n",
    "                                                    ## again and still be considered as event (ie. Christmas is an event\n",
    "                                                    ## each year)\n",
    "MIN_NUMBER_DIFFERENT_USER = 2 # To state that an event occured, a minimum number of different users should have tweeted about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to detect recurrent events that should be removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isSpecificEventListIllegal(detectedEventDateList):\n",
    "    '''\n",
    "    Return true if the list of dates contain illegal tupples of events, so if the event is recurrent\n",
    "    which would mean it is not a real event.\n",
    "    '''\n",
    "    def datesAreIllegal(date1, date2, date3):\n",
    "        '''\n",
    "        Return true if the 3 dates are not to be considered as regular events.\n",
    "        '''\n",
    "        ## Return if the difference is too small to be considered as 2 different events\n",
    "        def diffIsSmall(timeDiff):  \n",
    "            return timeDiff < MAX_DURATION_OF_EVENT\n",
    "\n",
    "        ## Return true if the difference is not big enough to be an annual event.\n",
    "        def isDiffSuspect(timeDiff):\n",
    "            return timeDiff < MIN_DURATION_BEFORE_NEW_EVENT   \n",
    "\n",
    "        diff1 = abs(date1 - date2)\n",
    "        diff2 = abs(date2 - date3)\n",
    "        diff3 = abs(date3 - date1)\n",
    "\n",
    "        ## The difference is too small, it must be the same event\n",
    "        if diffIsSmall(diff1) or diffIsSmall(diff2) or diffIsSmall(diff3):\n",
    "            return False\n",
    "\n",
    "        ## If there are at least 2 out of 3 suspect difference, then the dates are illegal\n",
    "        if isDiffSuspect(diff1):\n",
    "            return isDiffSuspect(diff2) or isDiffSuspect(diff3)\n",
    "        else:\n",
    "            return isDiffSuspect(diff2) and isDiffSuspect(diff3)\n",
    "    \n",
    "    ## MAIN FUNCTION : ##\n",
    "    # Go through the list of events and try all \"triples\" to see if there is any illegal triples. This is a quickly done\n",
    "    # code to do that. Code complexity bellow is in O(k^3), with k being the size of the list. We will apply this function\n",
    "    # to n list so we will have an overall complexity in O(n*k^3). We can consider however that each list will\n",
    "    # be small so k can be considered as constant and therefore the overall complexity will be in O(n).\n",
    "    for i in range(len(detectedEventDateList) - 2):\n",
    "        for j in range(i, len(detectedEventDateList) - 1):\n",
    "            for k in range(j, len(detectedEventDateList)):\n",
    "                if datesAreIllegal(detectedEventDateList[i], detectedEventDateList[j], detectedEventDateList[k]):\n",
    "                    return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main method to detect event according to all our criterias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr('Starting to compute {} dict items to detect event. (4 min)'.format(len(dictionary)))\n",
    "nbOfEventDetected = 0\n",
    "count = 0\n",
    "printingValue = int(len(dictionary) / 10)\n",
    "for [h,df] in dictionary.items():\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "    df['event'] = False\n",
    "    if len(df) > MIN_NB_DAYS_WITH_HASHTAGS:\n",
    "        if df['numberOfTweets'].sum() > MIN_TOT_NB_TWEETS:\n",
    "            if df['userId'][0] >= MIN_NUMBER_DIFFERENT_USER:\n",
    "                threshold = df['numberOfTweets'].mean() + THRESHOLD_ANOMALY_FACTOR * df['numberOfTweets'].std()\n",
    "                df['event'] = df.numberOfTweets.apply(lambda x: x > threshold and x > MIN_NB_TWEETS_DURING_EVENT)\n",
    "            \n",
    "            ## Remove recurrent events:\n",
    "            detectedEventDf = df[df['event']]\n",
    "            if len(detectedEventDf) > 2 and isSpecificEventListIllegal(detectedEventDf.index):\n",
    "                df['event'] = False\n",
    "            nbOfEventDetected += len(df[df['event']])\n",
    "pr('Finished! Number of events detected = {}'.format(nbOfEventDetected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pr('Starting to compute {} dict items to detect event. (4 min)'.format(len(dictionary)))\n",
    "# nbOfEventDetected = 0\n",
    "# count = 0\n",
    "# printingValue = int(len(dictionary) / 10)\n",
    "# for [h,df] in dictionary.items():\n",
    "#     count += 1\n",
    "#     if count % printingValue == 0:\n",
    "#         pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "#     df['event'] = False\n",
    "#     if len(df) > MIN_NB_DAYS_WITH_HASHTAGS:\n",
    "#         if df['numberOfTweets'].sum() > MIN_TOT_NB_TWEETS:\n",
    "#             threshold = df['numberOfTweets'].mean() + THRESHOLD_ANOMALY_FACTOR * df['numberOfTweets'].std()\n",
    "#             df['event'] = df.numberOfTweets.apply(lambda x: x >= threshold and x >= MIN_NB_TWEETS_DURING_EVENT)\n",
    "            \n",
    "#             ## Remove recurrent events:\n",
    "#             detectedEventDf = df[df['event']]\n",
    "#             if len(detectedEventDf) > 2 and isSpecificEventListIllegal(detectedEventDf.index):\n",
    "#                 df['event'] = False\n",
    "#             nbOfEventDetected += len(df[df['event']])\n",
    "# pr('Finished! Number of events detected = {}'.format(nbOfEventDetected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging close events and grouping into single event dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have a function that is going to merge events that are considered as too \"close\" to each other to be considered individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mergeCloseEvents(rowsList):\n",
    "    '''\n",
    "    Take a list of dictionary, where each dictionary is a \"row\" of the event df, which contained detected events.\n",
    "    It will process the list to detect event that are close and merge them together.\n",
    "    Return : the processed list of event.\n",
    "    '''\n",
    "    \n",
    "    def areCloseEvents(event1, event2):\n",
    "        '''\n",
    "        Return true is 2 events dates are defined as \"close\"\n",
    "        '''\n",
    "        return abs(event1['date'] - event2['date']) < MAX_DURATION_OF_EVENT\n",
    "        \n",
    "    def mergeCloseEventsSublist(closeEventList):\n",
    "        '''\n",
    "        This will be applied to each close event sublist. It will merge all events into one unique event.\n",
    "        The event will consist of the total number of tweets, with the concatenation of the tweet texts and the mean\n",
    "        of longitude/latitude. A meanDate will be defined as a ponderated mean between all dates.\n",
    "        The final date will be the one that is in the closeEventList and is closest to this mean date.\n",
    "        We did this to keep the meaning of the date if it had some, and not have some meaningless \"mean-date\".\n",
    "        '''\n",
    "        latitude = 0\n",
    "        longitude = 0\n",
    "        numberOfTweets = 0\n",
    "        text = \"\"\n",
    "        originalDate = closeEventList[0]['date']\n",
    "        dateDiff = timedelta(days=0)\n",
    "        first = True        \n",
    "        for tweet in closeEventList:\n",
    "            longitude += tweet['longitude']\n",
    "            latitude += tweet['latitude'] \n",
    "            numberOfTweets += tweet['numberOfTweets']\n",
    "            if first:\n",
    "                text = tweet['text']\n",
    "                first = False\n",
    "            else:\n",
    "                text += delimiter + tweet['text']\n",
    "                dateDiff = dateDiff + (tweet['date'] - originalDate) * tweet['numberOfTweets']\n",
    "\n",
    "        ## It is multiplied by 2 then soustracted to round correctly to the nearest day\n",
    "        meanDate = originalDate + 2* dateDiff / numberOfTweets - dateDiff / numberOfTweets        \n",
    "        latitude = latitude / len(closeEventList)\n",
    "        longitude = longitude / len(closeEventList)\n",
    "        \n",
    "        ## We are going to detect the event the closest to the mean date\n",
    "        minSelectedDate = closeEventList[0]['date']\n",
    "        minDistance = abs(closeEventList[0]['date'] - meanDate)\n",
    "        for tweet in closeEventList:\n",
    "            if abs(tweet['date'] - meanDate) < minDistance:\n",
    "                minSelectedDate = tweet['date']    \n",
    "        \n",
    "        return {'date': minSelectedDate, 'hashtag': closeEventList[0]['hashtag'], 'text': text,\n",
    "                    'longitude': longitude, 'latitude':latitude, 'numberOfTweets': numberOfTweets, }\n",
    "    \n",
    "    ############ -----  MAIN METHOD  ----- ############\n",
    "    \n",
    "    ## If the list is big enough, go through the list and form an export list and merge elements that needs to.\n",
    "    if len(rowsList) < 2:\n",
    "        return rowsList\n",
    "    else:\n",
    "        firstLastPosOfItemsToMerge = []\n",
    "        sortedRowsList = sorted(rowsList, key=itemgetter('date')) \n",
    "        exportedEventList = []\n",
    "        ## This goes through the *sorted* list and add the pair of indices (first indice and last indice) where events \n",
    "        ## that should be merged appear.\n",
    "        lastEventWasClose = False\n",
    "        firstItem = -1\n",
    "        for i in range(0, len(sortedRowsList)-1):\n",
    "            if areCloseEvents(sortedRowsList[i], sortedRowsList[i+1]):\n",
    "                if not lastEventWasClose: # So it is the first pairs of the sublist of close events in the whole list\n",
    "                    firstItem = i\n",
    "                    lastEventWasClose = True\n",
    "            else:\n",
    "                if lastEventWasClose: # So the list has just ended.\n",
    "                    exportedEventList.append(mergeCloseEventsSublist(sortedRowsList[firstItem:i+1]))\n",
    "                    lastEventWasClose = False\n",
    "                else: # The element is by itself, let's append it\n",
    "                    exportedEventList.append(sortedRowsList[i])  \n",
    "        if lastEventWasClose: # If there were events to merge till the last elem of list\n",
    "            exportedEventList.append(mergeCloseEventsSublist(sortedRowsList[firstItem:len(sortedRowsList)]))\n",
    "        else:\n",
    "            exportedEventList.append(sortedRowsList[len(sortedRowsList)-1])\n",
    "    \n",
    "    return exportedEventList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be applied to each dataframe. If a row is detected as an event, it will be added to the locaRowsList which will be used to make a general dataframe of all the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "localRowsList = []\n",
    "def applyToMakeEventDf(row):\n",
    "    if row.event:\n",
    "        rowToAdd = {'date': row.name, 'hashtag': row.hashtag, 'text': row.text,\n",
    "                    'longitude': row.longitude, 'latitude':row.latitude, 'numberOfTweets': row.numberOfTweets, }\n",
    "        global localRowsList\n",
    "        localRowsList.append(rowToAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eventRowsList = []\n",
    "localRowsList = []\n",
    "count = 0\n",
    "printingValue = int(len(dictionary) / 10)\n",
    "\n",
    "pr('Starting to make event df with {} dataframes. (around 6 min)'.format(len(dictionary)))\n",
    "for h, df in dictionary.items():\n",
    "    global localRowsList\n",
    "    localRowsList = []\n",
    "    count += 1\n",
    "    if count % printingValue == 0:\n",
    "        pr(\"{:.0f}%\".format(count/len(dictionary)*100))\n",
    "        \n",
    "    df.apply(applyToMakeEventDf, axis=1)\n",
    "    mergedList = mergeCloseEvents(localRowsList) # merging close events\n",
    "    eventRowsList += mergedList\n",
    "\n",
    "pr('Making new dataframe.')\n",
    "new_events = pd.DataFrame(eventRowsList)\n",
    "new_events.set_index(['date'], inplace=True)\n",
    "pr('Finished! Dataframe with {} rows'.format(len(new_events)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Events dataframe:')\n",
    "new_events.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Linked dataframe of all days:')\n",
    "dictionary[new_events.iloc[0].hashtag].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we worked with another team, we needed a way to communicate them our detection. We used a JSON with all the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_number_of_events = len(new_events)\n",
    "print('There are {} events.'.format(total_number_of_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_df = new_events.copy()\n",
    "e_df['date'] = e_df.index\n",
    "e_df.index = [i for i in range (len(e_df))]\n",
    "e_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to generate the right datetimes for the jsons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_dt = datetime(1970, 1, 1)\n",
    "def to_utc(date):\n",
    "    d_dt = datetime.combine(date, datetime.min.time())\n",
    "    return int((d_dt - epoch_dt).total_seconds()*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_unix_time(record):\n",
    "    datetime_index = pd.DatetimeIndex([datetime(record['year'], record['month'], 1)])\n",
    "    unix_time_index = datetime_index.astype(np.int64) // 10**6\n",
    "    return unix_time_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr('Converting dates...')\n",
    "e_df['year'] = e_df['date'].apply(lambda x: x.year)\n",
    "e_df['month'] = e_df['date'].apply(lambda x: x.month)\n",
    "e_df['utc_date'] = e_df['date'].apply(lambda x: to_utc(x))\n",
    "e_df['unix_time'] = e_df.apply(convert_to_unix_time, axis=1)\n",
    "pr('Done.')\n",
    "e_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation of a JSON is easier from a dictionary than from a dataframe. Also, the other team we worked with asked us to group events by months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grouping by months\n",
    "e_gb_month = e_df.groupby(e_df.unix_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generation of the dictionary for the final JSON\n",
    "pr('Making event list...')\n",
    "months = []\n",
    "for month, df in e_gb_month:\n",
    "    days = []\n",
    "    for i in range (len(df)):\n",
    "        ht = df.iloc[i]['hashtag']\n",
    "        lat = df.iloc[i]['latitude']\n",
    "        lon = df.iloc[i]['longitude']\n",
    "        t_num = df.iloc[i]['numberOfTweets']\n",
    "        tweets = df.iloc[i]['text'].split(delimiter)\n",
    "        date = df.iloc[i]['utc_date']\n",
    "        \n",
    "        data_unit = { 'name': ht\n",
    "                    , 'latitude' : lat\n",
    "                    , 'longitude' : lon\n",
    "                    , 'tweets' : tweets\n",
    "                    , 'number_of_tweets' : str(t_num)\n",
    "                    , 'date' : int(date)}\n",
    "        days.append(data_unit)\n",
    "    \n",
    "    curr_month = {'date': int(month), 'data' : days}\n",
    "    months.append(curr_month)\n",
    "\n",
    "final_events = {'events' : months}\n",
    "pr('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the final JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exportFilename = 'export_twitter_events_' + datetime.now().strftime(\"%Y-%m-%d_%Hh%Mmin%S\") + \\\n",
    "'_' + str(total_number_of_events)+ '_events.json'\n",
    "exportPath =  os.path.join('data', exportFilename)\n",
    "\n",
    "pr('Exporting to json...')\n",
    "with open(exportPath, 'w') as f:\n",
    "     json.dump(final_events, f)\n",
    "pr('Export done. File \"{}\" has been created.'.format(exportFilename))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
